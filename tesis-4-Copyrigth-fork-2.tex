% Alessio Botta 2008 - alebotta@gmail.com
%
% phdimt class file and demo
% A LaTex implementation of IMT Lucca PhD Thesis style
% Please send an email to let me know you used this style.

% The first option is final or draft. 
% Use draft for a quick compile (no figures, no links).

\documentclass[final,10pt,a5paper]{phdimt}
% PDF settings and metadata

\usepackage{lipsum}
%\usepackage[linedheaders,parts,pdfspacing]{classicthesis}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{acronym}


\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[utf8]{inputenc}
\usepackage[mathscr]{euscript}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
%\usepackage{pstricks}
\usepackage{caption}
\usepackage{rotating}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{cleveref}
\usepackage{nomencl}
\makenomenclature


%\usepackage{subcaption}
%\usepackage{subcaption}


%\pdfinfo{ 
%	/Title  (Time Series Forecasting  Based on Classification of Dynamic Patterns)
%	/Creator (TeXShop - Made on a Mac)
%	/Producer (TeX)
%	/Author (Rodrigo L\'{o}pez Far\'{i}as)
%	/CreationDate (T:20080214193000)
%	/ModDate (D:20080214193000)
%	/Subject (Engienerng)
%	/Keywords ()
%}
%\pdfcatalog{
%	/PageMode (/UseOutlines)
%      /OpenAction (fitbh)
%}

% LaTex metadata (used in the frontmatter)



\title{Time Series Forecasting  Based on Classification of Dynamic Patterns}
\author{Rodrigo L\'{o}pez Far\'{i}as}
\mail{rodrigo.lopez@imtlucca.it}
\program{Computer Science and Engineering}
\coordinator{Prof. Alberto Bemporad}
\coordinatorinst{Institute of Advanced Studies Lucca}
\cycle{XXVII}
\year{2015}

% If you have a more/less than two supervisors, you need to edit phdimt.cls a bit

\supervisor{Dr. Alberto Bemporad}
\supervisorinst{Institute of Advanced Studies Lucca}
\cosupervisor{Dr. Pantelis Sopasakis}
\cosupervisorinst{Institute of Advanced Studies Lucca}
\tutor{Dr. Alberto Bemporad}
\tutorinst{Institute of Advanced Studies Lucca}

% The same about the reviewers

\firstreviewer{Dr. Carlos Ocampo Mart\'{i}nez}
\firstreviewerinst{Universitat Polit\`{e}cnica de Catalunya}
\secondreviewer{Dr. Andrea Emilio Rizzoli}
\secondreviewerinst{Istituto Dalle Molle di Studi sull'Intelligenza Artificiale}
\thirdreviewer{}
\thirdreviewerinst{}

\begin{document}

% My vectors are bold, no arrows
\def\vec#1{{\bf #1}}

% Let's do not cheat on line spacing
\renewcommand\baselinestretch{1}
\baselineskip=14pt

% Some hyphenation hints
\hyphenation{Ein-ste-in}
\hyphenation{a-ch-ie-ved}
\hyphenation{ap-pro-ach-es}
\hyphenation{mo-del}
\hyphenation{al-go-rit-h-ms}
\hyphenation{flow-meters}
\hyphenation{functions}
\hyphenation{me-tho-do-lo-gies}
\hyphenation{A-RI-MA}
\hyphenation{The-re-fore}
\hyphenation{Sa-lu-des}
\hyphenation{ti-me}
\hyphenation{in-de-pen-dent}
\hyphenation{s-mo-o-thing}
\hyphenation{sour-ces}
\hyphenation{app-roach}
\hyphenation{pi-ece-wise}
\hyphenation{wea-ther}
\hyphenation{Spa-in}
\hyphenation{to-pic}
\hyphenation{co-mand}
\hyphenation{ti-me-in---de-pen-dent}
\hyphenation{ba-sed}


\frontmatter

\pagestyle{empty} % following pages do not require page numbering
\maketitle

\makereviewerspage

\include{frontmatter/dedication}
\pagestyle{plain} % numbering is shown from now on

\tableofcontents
\listoffigures
\listoftables
\include{frontmatter/acknowledgement}
\include{frontmatter/vita}
\include{frontmatter/abstract}

\mainmatter

\long\def\symbolfootnote[#1]#2 {\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

\chapter{Introduction}



%\emph{This paper proposes a general Multiple-Model Predictor framework for forecasting time series that presents multiple patterns (behaviours). The proposed approach allows off-line identifying the different time series behaviours, training a model for each behaviour and identifing on-line which is the pattern and associated model to use at each time instant. }



%\section{Related Work}

%Research in water demand forecasting area is not something new since the water is one of the most basic non renewable natural resources for sustain life and ecosystems. 
In areas like natural sciences, economics or engineering it is necessary, for specific purposes, to monitor or observe the dynamics of certain phenomena that is related to the field of study.  The  weather dynamics, industrial process, the fluctuation of the stock market are just some examples where the understanding of the dynamics is relevant. For example, the study of the environment dynamics in natural sciences is useful for the implementation of policies to preserve ecosystems, optimise the use of natural resources and improve the quality of life by modelling the dynamics of the urban sprawl. 

The prediction of the stock market in economics is vital for making better decisions about the actions that can be taken by the investors, and the study of model identification with the objective of constructing a model that behaves similarly to the real process for control. There are examples where the observation and modelling of the dynamics of the system is relevant, and a subset of them are related directly to the study of time series. 

Time series are presented explicitly or implicitly in every day life. A time series is defined as a sequence of data measurements chronologically ordered with certain frequency. This data  might come from different sources related to the studied discipline; these data may come from human activity, wind dynamics, and mathematics, among others. These disciplines follow different ends but they share in common the problem: the modelling of the dynamical system that fits better with the observed data able to produce or simulate such information. One of the most active research for these purposes is system modelling for prediction.

The study of the analysis of time series was born with the need to understand the dynamics of the data fluctuation generated by an unknown system. Dynamics are seen as changes of values along time of certain variable of study. These fluctuations might represent different kinds of data, depending on the application field.% as such as the variation of stock market index in finance an economy, temperature, speed and humidity in meteorology, prediction for control engineering.  


%http://cienciaeconomica.blogspot.mx/2011/10/analisis-de-series-de-tiempo-un-poco-de.html

%Para los años de 1960 y en adelante, los investigadores habían desarrollado modelos donde era posible descomponer una serie de datos en sus partes como la tendencia, el componente cíclico y una componente aleatoria e irregular, basándose en la aplicación de medias móviles y en las desviaciones de la variable, esto se completa con el procedimiento estadístico denominado X-11 (Ahora se usa un modelo  X-12),




The observed data is generated by a known or unknown model. When the model is unknown, a general dynamical model is constructed from previous analysis of the data.

%Sometimes is possible to have an insight about the model should be used just observing the data. 


A general classification of the models used for forecasting can be done according to the linear nature of their structure. The classification according to this criterion is:



\begin{itemize}
\item \textbf{Linear models}: Explain the relation between the variables by means of linear correlation.
\item \textbf{Nonlinear models}: The relation between the variables are not explained by means of linear correlations. The modelling deals with a nonlinear structure, present in piece-wise linear and nonlinear models. The characteristics of these models are:
	\begin{itemize}
		\item \textbf{Piece-wise linear models}: Is a set of linear models that are activated when certain conditions are satisfied.
		\item \textbf{Piece-wise nonlinear models}: Is a set of nonlinear models that are activated when certain conditions are satisfied.
	\end{itemize}
\end{itemize}
%Linear models
%Non linear models
%piece wise Linear
%piece wise non linear
%Pattern based models
%Stochastic based model


\section{Problem Definition}



Time series forecasting is performed by a regression function, which is a model that receives a sequence of observations and returns a scalar or a vector of real numbers. The regression function that predicts the value in the next instant of time $t+1$, given a sequence of values in $\mathbf{Y}_t$ is expressed by Equation \ref{reggressor}.


\begin{equation}\label{reggressor}
\hat{\mathbf{Y}}_{t+1} = \mathbf{F}( \mathbf{A},  \mathbf{Y}_t^{'})
\end{equation} where $\mathbf{F}$ is the regression function, $\mathbf{A}=\{{a_1},\dots,{a}_k\}$ are the parameters of the model, $\mathbf{Y}_t^{'}$ is the input vector with $m$ number of elements $\mathbf{Y}_t^{'}=\{Y_{t-m},\dots,Y_{t}\}$ and $\hat{\mathbf{Y}}_{t+1}$ is the the prediction given by the regressor $\mathbf{F}$ returning $h$ steps ahead $ \hat{\mathbf{Y}}_{t+1} =\{  \hat{Y}_{t+1} , \dots,\hat{Y}_{t+h} \}$. The general objective function for fitting time series with a regressor model is given by Equation \ref{fittingfunction}.

\begin{equation}\label{fittingfunction}
\underset{ \{ \mathbf{A} \} }{\text{min}} \sum_{t=m}^{n-h} ||  \hat{\mathbf{Y}}_{t+1} - {\mathbf{Y}}_{t+1}||^2
\end{equation} where the regression function minimises the squared errors between the output of the regressor $\mathbf{F}$ and the original data $\mathbf{Y}_{t+1}=\{  {Y}_{t+1} , \dots,{Y}_{t+h} \}$. 


% adjusting the parameters $\alpha$ from the index $t$ starting from the number of observations required $m$ to the total number of of data $N$ minus the size of the prediction horizon $h$.

Time series are generated by dynamical systems from different sources, such as energy sources as wind \cite{Jiang2015}, energy prices \cite{Wang2011770}, \cite{EnergyInformationAdministration2014}, human water demand, and water precipitation \cite{aquastat2003} often difficult to model and forecast with precision. According to the stationarity and sampling theory  usually systems present an intermittent  change of behavior \cite{a} that implies a mismatch with the regression model. This problem motivates the use of alternative regression methods and combinations of them, depending on the nature of the system. 

The problem to solve in this thesis is the design of a general framework that incorporates multiple regression models that are selected to be activated according to predefined rules. For this purpose, the data is analysed and clustered in classes according to their common characteristics to fit local models. The proposed model to study is given by the piecewise Equation \ref{multimodel}



\begin{eqnarray}\label{multimodel}
  \mathbf{F}(\mathbf{A},\mathbf{Y}_{t}^{'}) = \begin{cases}
        f_1(a_1,\mathbf{Y}_{t}) & \text{ }\text{ }  \text{ if $mode=1$} 
        \\
         f_2(a_2,\mathbf{Y}_t^{'}) &\text{ }\text{ }   \text{ if $mode=2$} \\
		\vdots \\
       f_k (a_k,\mathbf{Y}_t^{'}) &\text{ } \text{ } \text{if $mode =k$}
        \end{cases}
\end{eqnarray} where $ \mathbf{F}(\mathbf{A},\mathbf{Y}_{t}^{'})$ is the multi-model that contains $k$ independent local models $f_1,\dots,f_k$. $\mathbf{A}$ is the set of the selector parameters for the models and it is defined as $\mathbf{A}  = \{a_1,a_2,\dots,a_k \}$ and $\mathbf{Y}_t^{'}$ is a vector with recent measurements at time $t$. The objective of study is to build a multi-model that fits complex dynamics of time series. The objective function for the multi-model is described by Equation \ref{fittingfunction2}.


\begin{equation}\label{fittingfunction2}
\underset{ \{ \mathbf{A} \} }{\text{arg min}} \sum_{t=m}^{n-h} ||   \mathbf{F}(\mathbf{A},\mathbf{Y}_t^{'})  - \mathbf{Y}_{t+1}||
\end{equation} The selection of the modes of global modelling is a design problem for the activation of different modes according to knowledge collected from the observed data and a priori information. The developing of this global modelling is related to the construction and exploitation of probabilistic or deterministic rules that should be explored for finding a suitable model of this kind that estimates accurately  the next operation mode.






 

\section{Main Objective}

The main objective is to find high performance drinking water demand prediction models that provide accurate predictions in the short term. The availability of an accurate and detailed prediction is a very important part for making accurate decisions regarding the operation, control and management of drinking water networks. An accurate model allows minimising operational costs and wastewater without sacrificing quality of service, delivering drinking water to the population. 
The problem is addressed studying the identification and classification of different dynamic patterns found in drinking water demand time series for the design of local predictors that are integrated later in a global model.



%Time series extracted from real environments, like those produced by flow meters installed in drinking water network, demands and the design of a multi-model that is able to mimic the dynamic of the observed data sequence to produce predictions in the short term.

\subsection{Particular Objectives}

\begin{itemize}
\item To explore the integration of machine learning, data mining, and statistical models such as neural networks, k-means clustering, and Box-Jenkins models in Multi-Model predictors (\textbf{MMP}) and compare their performance with classical forecasting methods such as exponential smoothing and traditional neural networks for data regression.

\item To explore and validate with standard metrics the clustering of time series to identify different behaviours and its decomposition to simplify and improve the accuracy of the forecasting models.

%\item Study and validate the drinking water demand behaviour with machine learning, statistical and clustering tools. 

\item The exploration and design of global methodologies for the detection and activation of forecasting operation modes.

\item Test and validate in the short term the Multi-Model Predictor architecture with drinking water demand time series.
\end{itemize}




\section{Justification}
Time series analysis is an important discipline useful in the optimisation of the exploitation of natural resources and renewable energy. The performance of model forecasters impacts directly the operation costs since an estimate of future information can be used to take optimal decisions in the management of drinking water. For example, in the case of the drinking water delivery, where special attention is required in the maximisation of the water availability and minimisation of the operational costs for bringing safe drinking water to the population, it is important to have certainty about the future requirement of this resource in different terms (short and long term) to optimise its use.


%The economical costs associated to drinking water production are due to: chemicals, legal canons, and electricity costs. Moreover, the transportation of drinking water through the overall water network plays an important role as for electricity costs in pumping stations [3]. Improving the water supply management means the reduction of operating costs avoiding the development of new supplies and unnecessary expansion of infrastructure [4]. It also reduces withdrawals from limited freshwater supplies, reducing at the same time the negative e?ect on the natural environment.


%[3] D. Barcelli, C. Rocchi, G. Ripaccioli, C. Pascucci, Decentralized wireless control of large-scale systems.
%[4] D. Stephenson, et al., Water supply management., Kluwer Academic Pub- lishers, 1998.

 The Primary Health Care in Alma-Ata declared in 1978 the safe water as the most important resource for human health \cite{world1993guidelines}. The extraction, treatment, storage, and distribution of drinking water is a costly and complex task usually bringing it from faraway places using pipe networks connected to the urban population to distribute the consumption of this element \cite{Alperovits1977}. In the operational cost of the water production are implied chemicals, legal canons, electricity costs. The transportation of the drinking water also contributes to the electricity cost since the water pumping stations require energy to operate, such as the Barcelona drinking water delivery \cite{barcellidecentralized}.


Optimising the management of the water network supply also avoids the unnecessary expansion of the water network infrastructure and new supplies \cite{stephenson1998water}. It also reduces withdrawals from limited freshwater supplies, reducing at the same time the negative effect that produces the exploitation of this resources on the natural environment.


To manage the water supply network efficiently several strategies have been developed. One of them is Model Predictive Control (MPC) \cite{Pascual2013}, an optimisation-based control strategy applicable to a wide range of industrial applications \cite{ma2012model,ormsbee1994optimal,van2006model}. MPC provides suitable techniques to compute optimal control strategies ahead in real time for all the control elements of a water supply system. The accuracy of MPC depends on the water distribution model and the accuracy of the short term forecasting water demand. MPC solves the control problem each time step finding the best input control sequence several steps ahead, applying just the first action of the sequence. Since the MPC uses the prediction as reference for the optimal control, inaccurate predictions increase statistically the operation cost. According to the study of Hippert et al. in \cite{hippert2001neural}  an increase of 1\% in the error would imply to \pounds 10 million increase of operational cost.


Usually MPC is extended with a feedback mechanism that deals with the system disturbances. This extension consists of solving the best input sequence for a certain forecast horizon $t+1,\dots,t+h$, and apply just the solution of the next step $t+1$. It is desired to apply this process each time $t$ in an environment with disturbances. The longer the horizon, the better control performance is achieved. MPC applied to drinking water networks has as main objective to reduce operational costs, related to production, transportation and the maximisation of the quality of service, delivering the water properly to the population. For this reason it is important to make accurate water consumption predictions which will be used by MPC.



\section{Thesis Organisation}
The thesis is organised as follows: Chapter 2 addresses related work to the Multi-Model forecasting framework. Chapter 3 introduces time series forecasting and system identification, considering the linear and nonlinear approaches for the analysis also including an introduction of classification and feature extraction important for data treatment that can be implemented straightforwardly in the proposed framework. Chapter 4 addresses the proposed Multi-Model Predictor architecture and three proposed implementations. Chapter 5 presents the results of the different proposed forecasting methodologies. Finally Chapter 6 presents conclusions and future work suggested by the author. 

 
\chapter{Related Work}


%Por otro lado, y analogo al proceso de experimentaci ? on llevado a cabo en laboratorios con el ?
%objetivo de aumentar la comprension de alguna teor ? ??a para su validacion y empleo posterior, ?
%la simulacion, considerada como un m ? etodo de experimentaci ? on controlada, es el proceso ?
%de imitacion de aspectos importantes del comportamiento de un sistema, mediante la cons- ?
%truccion de un modelo implementado en un computador de tal forma que permita generar ?
%observaciones dadas ciertas entradas.


In the early successful stage, during the 70's decade, important discoveries appeared in time series modelling and forecasting with the first applications in econometrics. George E.P. Box and G.M. Jenkins \cite{BoxJen} used the \textit{divide and conquer} strategy decomposing and characterising the basic components of series trying to explain in a certain way the characteristics of the general dynamics of time series, such as trend, seasonal, cyclic, and random components. All these components were integrated for the first time in the Autoregressive Integrated Moving Average methodology creating the Box-Jenkins or ARIMA methodology.

The first reference regarding the study of the combination of forecast produced by different models is found in 1976. It asserts that although the single Box-Jenkins forecasting is better that other methodologies of that time, like exponential smoothing, a simple average forecast from a set of models can be better under some circumstances. This implies the suggestion of using a combination of several models instead of one. This discovery by Casta\~no et. al \cite{CastanoVelez2000} stimulated the study of the linear combination of forecasts models to improve the prediction performance. 

The study affirms that after proving under the assumption of having different unbiased forecasts, the optimum linear combination of forecasts produces another unbiased forecast. In order to optimise the weights  of the linear combination it is necessary to have as much evidence as possible for constructing the forecast. 

This idea was accepted with the development of expert systems, which reinforced the idea of combining different forecasters belonging to different information sources. Casta\~no \cite{CastanoVelez2000}  also statistically proved that the forecast produced by a combination of models is always better that the use of a single model.

Nowadays, the development of a new generation of forecasting models and strategies is strongly related to multidisciplinary novel research mainly from mathematics, computing, and statistics. Regarding the time series literature, there is a strong effort on finding the best way to decompose time series in several but simpler time series to fit better simpler models that improve the forecasting performance. This is not an easy task since in real cases arise several challenges, one is about the unavailability of a full model that describes the dynamic fluctuation of the data. Often the time series information is insufficient, noisy, or corrupted. For this reason the data should be analysed and processed to be fixed. In other situations the data is so large and complex that it is computationally infeasible to optimise parameters of statistical models or training the machine learning models.

Fortunately, despite all the problems that may occur, thanks to the growing of computational resources and the development of machine learning and pattern recognition algorithms, it is possible to analyse time series with a higher complexity in their dynamics (e.g. Nonlinear dynamical systems and hybrid dynamical systems). A good review about the representation, indexing, similarity measure, segmentation and visualisation of time series analysis from the data mining point of view is found in \cite{Fu2011164}. Also the book of Multiple Model Approaches to Modelling and Control \cite{MurJoh97} a comprehensive survey about modelling using multiple models is presented. A collection of practical examples and approaches are discussed where the single modelling approach is not enough for systems with complex and hybrid motion.


% allowing to take better solutions and decisions to problems. Applications are found in medicine and health. \cite{Fink1984} and meteorology



%the take of difficult decisions or prediction of very complex phenomena with high non-linearity dynamics such as meteorological phenomena. The objective as 


%The different local dynamics might be produced in terms of changes of dynamical regimes that are governed by internal or external events involved in the global or long term behavior. 




Although the multi-modelling approach was born with the analysis of partially known systems (grey box modelling), the same ideas  can be adopted for time series which there is no knowledge about the system or mathematical model that produces its dynamics. Real cases are presented regularly in the water demand, solar radiation, wind speed and stock market fluctuations.

%Forecast Multi-Modelling approaches has been used in different fields such as in engineering renewable energy, finance, sociology. 





As a historical reference, one of the earliest works related to water time series forecasting, presents a multi-model application to water demand forecasting shown in the work of Shvartser in \cite{Shvartser1993}. This work proposes a methodology based on pattern recognition and time series analysis; the daily consumption cycle is divided in three segments; rising, oscillating, and falling. These segments are modelled separately and are seen as dynamical states. The sequence of the activation of each pattern associated with one state is modelled with a Markov process capturing the transition probabilities between states. Each segment of the time series with a specific pattern is associated with each state which is represented with a low order  ARIMA model.

Figure \ref{shvartser} is an example of the decomposition of the time series in segments. Segments number 1, 2, and 3 are classified as rising, oscillating, and falling respectively where the limits of the segments are defined by the dense dotted lines. Each segment may be occur during the day transaction indicated by the pointy line. The pattern is observed throughout the information of two  days.


 \begin{figure}[h!]
				\centering
				\includegraphics[width=60mm]{figures/STATESARIMA2.pdf}
				\caption{Rising, oscillating, and falling patterns classification}
				\label{shvartser}
\end{figure} 


S. Alvisi and M. Franchini \cite{Alvisi2007} developed a short-term, pattern-based model for water-demand forecasting. The model captures the periodic patterns presented at annual, weekly and daily levels. 

%Prior to fine-tuning the estimated values of future demands through the inclusion of persistence effects.

 The model structure is based on the observed patterns at different abstraction levels of the water demand time series. The forecasting structure is hierarchically organised in two levels: The high level module that captures the low frequency patterns, like the seasonal and weekly patterns of the time series observed in Figure 1 from \cite{Alvisi2007}. The low level model describes and predicts the daily consumption (Figure 3 presented in \cite{Alvisi2007}). In order to get the hourly forecasts over the next 24 hours period, a short term forecasting mechanism based on the combination of both models is implemented.
 

The work of J. Quevedo and V. Puig in \cite{quevedo2014} addresses a similar approach, where a Seasonal ARIMA (SARIMA) is used for predicting the daily water demand consumption combined with a descriptor class that distributes the amount of the predicted water consumption along one day. Mainly two validated descriptors are used. These descriptor classes are shown in Figure \ref{qualitativeDescriptors1} and Figure \ref{qualitativeDescriptors2}, describing working and resting days patterns. These descriptors are validated using the LAMDA clustering method found in SALSA  software package \cite{Kempowsky2004B,Kempowsky2004}. The patterns given by the descriptors are a priori assigned to each calendar day according to the human activity calendar.





\begin{figure}[h!]
\centering
\parbox{5cm}{
\includegraphics[trim = 0mm 100mm 0mm 100mm, clip,width=5.2cm]{figures/quevedoDescriptors01B.pdf}
%
\caption{Class 1 descriptor associated with Weekend and holiday consumption patterns. (\textcopyright 2014, IEEE).}
\label{qualitativeDescriptors1}}
%\qquad
\begin{minipage}{5cm}
\includegraphics[trim = 0mm 90mm 0mm 90mm, clip,width=5cm]{figures/quevedoDescriptors02.pdf}
\caption{Class 2 descriptor associated with weekdays consumption pattern. (\textcopyright 2014, IEEE).}
\label{qualitativeDescriptors2}
\end{minipage}
\end{figure}



%In the work of [khalil bermuiza] although the application of the work is regarding to the forecasting of solar radiation is possible to use it for water demand forecast as well. It is developed a hourly global horizontal solar radiation forecaster based on the implementation of unsupervised k-means clustering algorithm and artificial neural networks (ANN). K-Means algorithm is focused on grouping data according to its characteristics in the phase space. Once the the 	

 %information from the data in the phase space with the aim of modelling the time series behavior and find patterns of the input space by clustering the data in its phase space form. On the other hand, nonlinear autoregressive (NAR) neural networks are powerful computational models for modelling and forecasting nonlinear time series. Taking the advantage of both methods, a new method is proposed combining k-means algorithm and NAR network to provide better forecasting results. In the Figure \ref{solarRadiationFcst} Is shown the basic principle of the proposed approach. The time series of the hourly solar radiation 	


Benmouiza developed \cite{Benmouiza2013} a methodology based on the combination of clustering methods and artificial neural networks to predict in the short term the solar horizontal radiation. The predictor model is a composition of several independent local models. Each model is trained with clustered data of one class containing similar dynamical patterns. The architecture has a Global Nonlinear Autoregressive Neural Network (NAR) used for predicting the local model to forecast. Once the local model is selected, a local NAR model associated with each cluster is used to forecast the hourly radiation. Although the application of the work aims to forecast the solar radiation, it might be possible to implement it for water demand forecast, since the dynamics of the water demand may obey also to global and local structure in its dynamics where the local dynamic patterns can be identified and clustered.

%The algorithm proposed has three stages:  First, phase space reconstitution of the hourly global solar radiation time series was reached by finding the appropriate time delay using mutual information method, and the minimum embedding dimension is defined using false nearest neighbour method. Secondly, the unsupervised k-means clustering algorithm was then applied for grouping the input data into k clusters, which have similar charac- teristics. 

%For choosing of the right number of clusters, the silhou- ette plot which represents a graphical representation of the separation of the heads of each cluster from another one was then used. Subsequently, a different NAR neural network was prepared on each cluster to act as a local predictor for the corresponding subspace of the input space. In addition, another NAR network was used to act as a global predictor for the solar radiation time series. The methodology was applied to generate multi-step ahead forecasts for the hourly global horizontal solar radiation time series. 

%solarRadiationTraining.pdf

%modificar la figura para poderla explicar
Figure \ref{solarRadiationFcst} shows how the data in phase space is classified using a clustering algorithm finding three kind (or regions) of solar radiations. Each region represents low, medium and high solar radiation levels. A global NAR (Neural Auto Regressive) predicts the next region. Once the next region is estimated the local model forecaster related to the region is used to predict the radiation in an hourly basis. The dynamics of the hourly basis radiation is shown in Figure \ref{radiationHourlyBasis}.


%incluir trabajo relacionado de el Dr Rico y Flores




\begin{figure}[h!]
\centering

\includegraphics[width=10cm]{figures/solarForecasting.pdf}
\caption{Phase space representation and clustered in regions of the hourly global horizontal solar radiation $^0$.}
\label{solarRadiationFcst}
\end{figure}

%"Reprinted from Publication title, Vol /edition number, Author(s), Title of article / title of chapter, Pages No., Copyright (Year), with permission from Elsevier [OR APPLICABLE SOCIETY COPYRIGHT OWNER]." 


%Energy Conversion and Management, Vol 75, K. Benmouiza,A. Cheknane, Reprinted from Forecasting hourly global solar radiation using hybrid k­-means and nonlinear autoregressive neural network models, Pages No 561-569, Copyright (2013), with permission from Elsevier

\begin{figure}[h!]
\centering
\includegraphics[width=9cm]{figures/solarRadiationTraining.pdf}
%
\caption{Measured hourly global horizontal solar radiation time series of July 1996 $^0$.}
\label{radiationHourlyBasis}

\end{figure}


\footnotetext{Energy Conversion and Management, Vol 75, K. Benmouiza,A. Cheknane, Reprinted from Forecasting hourly global solar radiation using hybrid k­-means and nonlinear autoregressive neural network models, Pages No 561-569, Copyright (2013), with permission from Elsevier}\label{fn}



{\color{red} M. Bakker et al. \cite{Bakker2013} propose a fully adaptive forecast scheme using a static calendar to compute in real time model weight coefficients (named day factors), and demand patterns used by the model. The model assumes the existence of four kinds of different water demand patterns (reported in \cite{Zhou2002}) of which those associated with holidays, weekdays, and holidays variation are known in advance but not for variants of season water demand patterns which should be detected on-line.
}


 Another interesting approach based on the combination of multiple models, are the consensus and ensemble methods. {\color{red} The consensus models are tools to create structured prediction maps which consider a limited set of future forecasts based on expert information. This set of forecast are provided by the human knowledge \cite{Mcnees87}.} On the other hand, an ensemble forecast is a collection of two or more forecasts performed at the same time. These methods focus on generating scenarios that describe probabilistically the predicted states of a dynamical system \cite{Leutbecher2008}. Ensemble forecasting is considered a Monte Carlo analysis method where multiple numerical predictions are produced from generating different possible initial conditions given a past sequence and current set of observations. {\color{red} Applications of these models are found in medicine \cite{Koestler2013}, health \cite{Fink1984}, economics, meteorology and water management. In economics, this approach is so relevant at the point that exists a specialised firm named Consensus Economics$^\text{TM}$ group \cite{mon16} that collects the state-of-the-art forecasters with their predictions for a big number of variables (more than 1000) from 85 industrialised countries in Eastern Europe, Asia Pacific and Latin America. The group has a significant community of researchers that confirm better accuracy of Consensus Forecasts$^\text{TM}$ than most of the individual forecasters \cite{Bat00, BWWA01, Jon14, NR11}.} 
 
 %with an influent collection of macroeconomic forecasts and analyses by topic for each country from a total of 115 including the most powerful economies with high impact in academic research. Novotny et al. \cite{novotny2011}, validated the accuracy of the Consensus Forecasts estimates using the database of the International Monetary Fund and the Organisation for Economic Co-operation and Development.  
 

%A 2007 paper by Batchelor used Consensus Forecasts data to consider various theories of bias in macroeconomic forecasts, and concluded: "In all countries there is evidence that individual forecasters converge on the consensus forecast too slowly. However, the persistent optimism of some forecasters, and the persistent pessimism of others, is not consistent with the predictions of models of ?rational bias? that have become popular in the finance and economics literature.
 
In meteorology, consensus forecasts are implemented to predict weather and meteorological phenomena. As an example, Figure \ref{TPC}, a picture taken from the National Whether Service \cite{noaa}, describes a category 4 Hurricane, Debby, that appeared in 1994. Each colored trajectory line, is a forecast simulation considering a randomised scenario. The set of forecast trajectories are used to produce a nominal prediction. This forecast
is the most likely trajectory that might be taken as official to predict the phenomenon dynamics.
 
 
 
 
 %Regarding to water management, Tiwari et al. \cite{tiwari2015} propose an interesting work which considers a bootstrap method to learn a wavelet based machine learning in order to consider the prediction bounds.
  
 
 
 
 
 % {\color{blue} A graphical example is shown in Figure \ref{TPC} about a category 4 Hurricane, Debby, that appeared in 1994 (taken from the National Whether Service \cite{noaa}) where each colored trajectory line, is a forecast simulation considering a randomised scenario. The set of forecast trajectories are used to produce a nominal prediction. This forecast is the most likely trajectory that might be taken as official to predict the meteorological phenomenon dynamics.}


 \begin{figure}[h!]
				\centering
				\includegraphics[width=80mm]{figures/TPC.pdf}
				\caption{Example of an application of ensemble forecasting applied to the prediction of hurricane trajectories.}
				\label{TPC}
\end{figure} 


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}


{\color{red}Considering the survey of novel application in water management by Donkor et al. \cite{Donkor2014}, the general concern of forecast methodologies is to fit the time series reducing the difference between the real value and the estimated forecast. Even though model fitting by minimising the gap is crucial to select a good forecasting model. The uncertainty prediction is also an important component for the forecast that should be considered in order to define the prediction bounds to give confidence to the prediction. With regard to water management, Tiwari et al. \cite{TA15} propose a bootstrap method to learn a wavelet based machine learning considering also the minimization of the prediction bounds. Other articles studying the uncertainty of stochastic models are found in \cite{Christopher2015}, \cite{Cutore2008} and \cite{Alvisi2014}.}  


Based on the multi-modelling approach and the references collected regarding the design of multi-model forecasting, this thesis proposes the next contributions to multi-model forecasting.


\begin{itemize}
\item  The design of a module based framework that is able to exploit the empirical information using machine learning algorithms rather than the seasonal structure.  

\item Algorithms based on the exploitation of the historical information are useful as alternative to the existing modelling global methods.

\item An implementation based on machine learning algorithms such as neural networks and clustering that identifies on-line different kinds of behavior patterns.


\item Two implementations based on the qualitative and quantitative decomposition of the time series. Where the predicted quantitative information is distributed along of an unitary pattern that describes a kind of activity (e.g, working or resting days). 
\begin{itemize}
\item The first variation does not assume any a priory human information about the water consumption modes and proposes the use of a qualitative k-Nearest Neighbor  (kNN) for mode prediction.
\item The second implementation assumes the existence of an activity calendar used as predictor, but with the contribution of extending the method using a simple nonlinear filter to detect the qualitative pattern mismatches to readjust the pattern forecast along time.
\end{itemize}
\end{itemize}


{\color{blue}This Chapter presented a brief historical introduction where George E.P. Box and G.M. Jenkins explored the decomposition of the data to analyse and understand time series dynamics to construct a stochastic model capable to predict. After some research Casta\~no et al. proved that a combination of unbiased forecasts gives regularly a better unbiased forecast. Nowadays computational power gives the possibility to analyse and construct complex dynamics presented in time series. We addressed recent research in multi-modelling applied to time series from real sources as water demand, solar radiation, stock market fluctuations and meteorology. Next Chapter \ref{TSFSI} addresses the common forecasting methods and clustering algorithms oriented to system identification for our proposal.

}

%a framework that has some advantages over the research found related to local and global modelling, that is  Some kind of data does not present strictly this kind of multiple periodicity else it is ruled by human activity. The detection and prediction of the next local model or prediction mode is obtained in three ways. 

%First, a  human made calendar that contains the working days and holidays-weekend days is often used for determine the behavior of certain human activity like water or energy consumption. 


%Other way to perform the local model to use is to detect automatically the kind of behavior without having any assumption of the future. This model than can distinguish the kind of human activity. 


%The third method is to model a global forecaster that given the historical information is able to estimate the next local model to use. The prediction of the next mode or the detection is independent of the local behavior of the data.

\chapter{Time Series Forecasting and System Identification}\label{TSFSI}


A time series is defined as a sequence of chronologically ordered observations recorded at regular time intervals. The observations are sequential data that might represent qualitative or quantitative information depending on the source and application field.  When the data are quantitative, the measurements that compose the time series express magnitudes or scalar information. An univariate time series notation used in the literature is defined as follows in Equation \ref{tsnotation}:

\begin{equation}\label{tsnotation}
	\mathbf{Y}_{full} =\{Y_1, Y_2,\dots,Y_t,\dots, Y_n\}
\end{equation} where $\mathbf{Y}_{full}$ is the full time series, $t$ is the index that indicates the time when the value was taken, $n$ is the length of the time series. Usually time series $\mathbf{Y}_{full}$, deals with scalar numbers that might be integers, reals, but sometimes they store qualitative information, where each element takes on a symbol from a defined set of objects. $Y_t \in \mathbf{A} $, where $\mathbf{A}$ is a set of symbols included in the alphabet.



These measurements are taken via observations and then recorded somehow; for example manually or using computers that interact with electronic devices that record the information in data bases. The use of high speed computers and big storage allows to implement powerful statistics and machine learning algorithms. 

Time series forecasting is strongly related to system identification and modelling. {\color{blue} System identification integrates statistical and mathematical tools to estimate and exploit the available information, and also studies the optimal design of experiments to generate informative data to fit dynamical models. This is achieved by removing redundant or erroneous data keeping useful and descriptive information to model the object of study. This data treatment is useful for reducing the training or the parameter optimisation time, and the model simplification.

The selection of information is important to control the detail level of the model according to the power and capacity of the implementation device: e.g., for small devices with limited memory and power processing a simple version of the model generated from the data should consider the most important characteristics of the object of study (as models implemented in microcontrollers for embedded control).

%Although nowadays the computational power and data storage devices are increasing, 

%computers are becoming cheaper with high power of processing would require and manage more complete models that deal with very complex data with the objective of producing the best forecast with the best accuracy possible.

%\textit{The field of system identification [Notes 1] uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction.
%The field of system identification [Notes 1] uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction. } 

There are three levels of modelling abstraction in system identification. According to the scope of the and knowledge availability of the study object, the modelling levels can be classified as white, grey, and black-box modelling. 

In white-box modelling, the dynamical processes are modelled using differential equations to describe the motion of the system. Under this approach there is detailed and enough information to build an accurate model describing the nominal motion of the system of interest. This kind of modelling is useful for solving the implementation problems of control, simulation and synthesis of the control law, especially in the space state approach. Typically, system identification is done by a human expert following laws and rules, for example the physical law of motion or chemical reaction laws. Although white-box is desirable in many cases due to the parsimony of the models, this approach has limitations when the systems are more complex or when it is not possible to have the precise parameter values for the system. In these cases the white-box methodology is not enough. 
 
The next abstraction level of modelling is grey-box modelling. It addresses those problems where the dynamical system is partially known and the model is completed using empirical information. A typical case of grey-box modelling is when the general dynamic of the system is modelled but there are parameters that still need to be tuned. For example, the parabolic shot modelled with newton gravitational laws or hydrologic model behaviours such as Nash-Sutcliffe and coefficient of determination model efficiency \cite{Krause2005} (used to assess the predictive power of hydrological models). In this case, these parameters should be estimated using statistical methods with the available observations so far.

Black-box modelling is the highest level of modelling abstraction of the system identification approaches where there is little, no information, or unclear insight about the model behind that generates the sequence of observed data. This kind of modelling uses general purpose models tuned or trained using just the observed data. Examples of this kind of models are Auto Regressive (AR) models, Artificial Neural Networks (ANN) and Support Vector Machines (SVM) for discrete time modelling of continuous dynamical systems, or deterministic state machine for piecewise models or Markov Chains Models for piecewise stochastic models.

Once a model is obtained, it can be applied to simulation, control or forecasting of dynamical systems among others. The concept of black-box modelling abstraction is linked to the time series analysis that is the main focus of this thesis.



%http://en.wikipedia.org/wiki/System_identification



The main purpose of the time series analysis, is the design of dynamical models based on the empirical observations of different phenomena. For the study of these observations, time series analysis provides the useful theory for the construction of methodologies and algorithms that are provided as tools for understanding the information for the correct modelling of the behaviour of the observed data. 
%The time series modelling is addressed as black - box modelling 


During the analysis of time series two general approaches are taken into account depending on the linearity of the data, like the linear and nonlinear approaches. Linear methods are useful when the interpretation of the observed data is regular presenting a dominant frequency and sequential information that can be measured with linear correlations. It is assumed that the systems are governed and explained with linear algebra theory. Using this approach the linear equations are limited to model systems that present a decaying, growing or damped periodically oscillating behaviour. The remaining irregularities are assumed to be random external inputs or disturbances to the system that can be described statistically by the normal distribution. Some examples of systems studied under this assumption are shown in Table \ref{sysclaff} taken from the book Nonlinear Dynamical Systems in \cite{strogatz} of Strogatz and Steven H., where the dynamics of the linear systems is determined and classified by the number of variables or differential equations. The basic systems belonging to this classification, present a simple growth, decay, or equilibrium dynamics when the dynamical systems contain one variable. When oscillations are present in the systems, they can be modelled with linear systems of two variables, for example, a simple mass-spring system. With more than three variables, applications are found in engineering e.g., coupled systems modelled with several linear systems. For more complex systems like coupled oscillators require a greater number of equations or variables to model them.  }

On the other hand, nonlinear methods address a more general family of dynamical models. This approach must be considered when the time series presents irregularities from the point of view of linear dynamical systems theory. The main drawback of linear theory is the impossibility to distinguish the random noise from the nonlinear structure of the data, therefore, it cannot model appropriately this kind of behaviour since the nonlinearity produces false residuals that might be confused with the noise where in reality it can be still modelled somehow. The systems in this category are natural processes. The simplest nonlinear models that contain just one variable are used to study fixed points, bifurcations, over damped systems or the equilibrium in ecosystems. For two variables natural oscillations, like limit cycles, biological oscillations, and nonlinear electronics are studied. For three or more variables, systems that might present strange attractors, chemical kinetics, iterated maps or fractals are studied. For many variables the nonlinear optics, non equilibrium statistical mechanics, heart cell synchronisation, biological neural networks response, complete ecosystems and economics behavior are studied. For the continuum domain, nonlinear waves, plasma, earthquakes, general relativity, quantum field theory, reaction diffusion, biological and chemical waves among others systems are studied.


{


\begin{table}[h!]
\centering
\caption{Classification of the dynamical systems according to their number of variables}
\label{my-label}
\tiny{\begin{tabular}{|c|l|l|l|}
\hline
$n$                & \multicolumn{1}{c|}{$n=1$}                                                                                                                                 & \multicolumn{1}{c|}{$n=2$}                                                                                                                  & \multicolumn{1}{c|}{$n\textgreater3$}                                                                                                                                                 \\ \hline
General approach & \multicolumn{1}{c|}{Growth, decay, or equilibrium}                                                                                                       & \multicolumn{1}{c|}{Oscillations}                                                                                                         & \multicolumn{1}{c|}{Applications in engineering}                                                                                                                                    \\ \hline
Linear           & \begin{tabular}[c]{@{}l@{}}Exponential growth\\ RC circuit\end{tabular}                                                                                  &                                                                                                                                           & \begin{tabular}[c]{@{}l@{}}Civil Engineering\\ Electrical Engineering\end{tabular}                                                                                                  \\ \hline
Nonlinear        & \begin{tabular}[c]{@{}l@{}}Fixed Points\\ Bifurcations\\ Over damped systems\\ relaxational dynamics\\ Logistic Equation for single species\end{tabular} & \begin{tabular}[c]{@{}l@{}}Pendulum\\ Limit cycles\\ Biological Oscillators\\ (Heart cells, neurons)\\ Nonlinear Electronics\end{tabular} & \begin{tabular}[c]{@{}l@{}}Strange Attractors\\ 3-Body problem\\ Chemical kinetics\\ Iterated maps\\ Fractals\\ Forced nonlinear oscillators\\ Practical uses of chaos\end{tabular} \\ \hline
\end{tabular}
\begin{tabular}{|c|l|l|}

\hline
$n   $             & \multicolumn{1}{c|}{$n \gg 1$}                                                                                                                                                                                     & \multicolumn{1}{c|}{Continuum}                                                                                                                                                                                                         \\ \hline
General approach & \multicolumn{1}{c|}{Collective phenomena}                                                                                                                                                                                           & \multicolumn{1}{c|}{Waves and patterns}                                                                                                                                                                                                \\ \hline
Linear           & \begin{tabular}[c]{@{}l@{}}Coupled harmonic oscillators\\ Equilibrium Statistical mechanics\end{tabular}                                                                                                                            & \begin{tabular}[c]{@{}l@{}}Elasticity\\ Wave equations\\ Electromagnetism\\ Acoustics\end{tabular}                                                                                                                                    \\ \hline
Nonlinear        & \begin{tabular}[c]{@{}l@{}}Lasers, non linear optics\\ Non-equilibrium statistical\\ mechanics \\ Nonlinear solid state physics\\ Heart cell synchronisation\\ Neural networks\\ Immune systems\\ Ecosystems\\ Economics\end{tabular} & \begin{tabular}[c]{@{}l@{}}Nonlinear waves\\ Plasma\\ Earthquakes\\ General relativity \\ Quantum field theory \\ Reaction Diffusion\\ Biological and chemical waves\\ Fibrillation \\ Epilepsy\\ Turbulent fluids\\ Life\end{tabular} \\ \hline
\end{tabular}\label{sysclaff}

}
\end{table}

}


All the mentioned systems are able to generate a time series since it is possible to measure the states of certain variables along time. For this reason the study of time series is also strongly related to the study of nonlinear dynamical systems when they come from natural or artificial systems. 

The present thesis is related to dynamical systems difficult to model analytically belonging to nonlinear systems family. The models are obtained using different technics with the final objective of producing accurate forecasts.

 %The Table \ref{sysclaff} taken from the book \cite{strogatz} of Strogatz and Steven H. shows a general classification of different kinds of dynamical systems related to some disciplines, their examples and the usual approaches to model them. 



A special case of nonlinear systems are the piecewise linear or nonlinear dynamical systems, where the system is modelled by a set of local models that are activated by certain rules. Each local model is associated to one operation mode. The activation of these local models depends on the model of the finite state machine in the deterministic case \cite{Black2008}, or by a stochastic state machine that activates the different local models by probabilistic rules such as those performed by Markov Models Chains. Figure \ref{stateMachine} shows an example of a finite state machine model that captures the dynamics of behavior changes. The submodels or states are activated when an event occurs. In contrast, the Markov Model \cite{Grinstead2010} in Figure \ref{stateMachineMarkov} models the change of states by producing probabilities events to jump from one state to another.
  
 
 

\begin{figure}[h!]
\centering
\includegraphics[width=4.5cm]{figures/stateMachine.pdf}
\caption{In a finite state machine transition states are triggered by events.}
\label{stateMachine}
\end{figure}



 

\begin{figure}[h!]
\centering
\includegraphics[width=4.5cm]{figures/statemarkov.pdf}
\caption{In Markov Model transition states are triggered with certain probability.}
\label{stateMachineMarkov}
\end{figure}
 
 
 
% in the analysis of nonlinear time series another considerations should be taken into account. For example in nonlinear time series is assumed that the random disturbances might be not only produced by random inputs with certain distributions else by deterministic equations of motion in an autonomous way. Another more complicated and common scenario is when a system that is nonlinear presents random inputs or disturbance noise as well that will most likely to produce an irregular data set. This situation makes the task of identifying the model from a noisy environment more difficult.





%The motivation of this paper starts with the assumption that the time series presents different dynamic behaviors that might be seen as the change of different unknown regimes. For example, in the case of a water distribution network, the water demand in holidays is different than in working days, or between summer and winter, but regular during these periods. This fact motivates the use of several models for characterising each regime.

%Time series can be studied under the linear and non linear scope. The linear analysis belongs to the classical analysis of time series that assumes the existence of a correlation between the observed variable and the joint actuation of four components giving the mean value. The linear analysis 

%https://stat.duke.edu/courses/Fall99/sta290/Notes/AR/ar_notes.book.pdf



The next subsection addresses the basics of linear time series analysis introducing its basic components, a general methodology to extract them, and some notes about stationarity and sampling that remarks the importance of selecting the correct data to the study and modelling of time series.


\section{Linear Time Series Analysis}




Linear time series analysis studies time series that can be modelled by analysing and identifying the secular trend, seasonality, and noise components. A popular and simple way to analyse the dynamics is by using auto-regressive time series models. These models are a standard in modern stationary time series data analysis \cite{montgomery2008}. The advantage of these models is that they are seen as a combination of components of larger models that leads to generalised forms. 

Although it has limitations for nonlinear time series, the concepts and structure of linear models provide a background for the analysis of nonlinear models. Univariate time series also can be classified according to their domain in frequency based methods and Time domain based methods.

%compositional structure to build forecasting models.

%http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics



%\begin{enumerate}
%	\item Frequency based methods.
%	\item Time domain based methods.
%\end{enumerate}


Regarding time domain based methods the time series are studied from the stochastic process point of view. A stochastic process is a sequence of random variables $Y_t$ taking any value from $[-\infty$,$\infty]$ where $t$ is interpreted as the time in the discrete domain. Given a sequence of values each of the $Y_t$ variables have their own function that captures their distribution within of their corresponding moment. Each pair of these variables will have their corresponding joint distribution and the marginal distribution functions. 


These time series can be decomposed to be analysed by its three components: the trend (long term direction), the seasonal (systematic, calendar related movements) and the irregular (unsystematic, short term fluctuations) components.

%There are other methods that use combination of decomposition for modelling forecasters, as the well known Box-Jenkings methodology.


In order to proceed with the description of some of the most popular methods used in the literature as Box-Jenkins and Holt-Winters, basic definitions are defined next about the structure of the linear time series.

\begin{enumerate}[i]
\item Secular trend is the persistent component of the observed phenomena; it describes the long-term trend. The appearance of this component is exemplified in Figure \ref{secularTrend} where the red line shows the long-term trend. Although for simplification a linear fitting is used, it is also common to observe a long-term nonlinear trend (e.g., exponential or geometric growth). Some examples of secular trend cases can be global warming, inflation in economy, increase of energy and consumption and decrement of the availability of the water resources along the time.
	%\begin{itemize}
%		\item Global warming.
%		\item Inflation in economy.
%		\item The increase of energy consumption.
%		\item The decrement of the availability of the water resources along the time.
%	\end{itemize} 
	
	\begin{figure}[!htb]
	\centering
	  \includegraphics[width=80mm]{figures/secularTrendThin.eps}
	   \caption{Secular trend}\label{secularTrend}
\end{figure} 
	
	
	
\item Seasonality variation describes the short-term periodic movement. An example of how this component looks is presented in Figure \ref{seasonality}. Some examples where this component is present are daily variation of the temperature, daily sea level and water demand in a short term.
	%\begin{itemize}
%		\item Daily variation of the temperature.
%		\item Daily sea level .
%		\item Water demand in a short term.
%	\end{itemize}
	\begin{figure}[!htb]
	\centering
	  \includegraphics[width=80mm]{figures/seasonalThin.eps}
	   \caption{Seasonal variation}\label{seasonality}
\end{figure} 
	
%\item Ciclic Variation: This component refers to the oscillations that presents a higher amplitude (presented in a superior order) this is the periodic variation in a long term. This can be yearly variations 
%	\begin{itemize}
%%		\item Monthly variation of the temperature.
%		\item 
%		\item
%	\end{itemize}


\item White noise is modelled by a normal distribution function that captures the random variation of variables. This component also has the property of having zero mean, constant and independent variance for different values along time. An example of what the noise component looks is shown in the Figure \ref{noise}. White noise can be presented as external disturbances, error measurements due to technical limitations and interference.
	%\begin{itemize}
	%	\item External disturbances.
	%	\item Error measurements due technical limitations.
	%	\item Interference.
	%\end{itemize}
	
\begin{figure}[!htb]
	\centering
	  \includegraphics[width=80mm]{figures/noiseThin.eps}
	   \caption{Noise}\label{noise}
\end{figure} 

\item Transient variation captures the accidental dynamic presented regularly as isolated perturbations or aperiodic fluctuations that affect the regular behaviour over the time. An example of the appearance of this component is shown in Figure \ref{transcientVariation}, where aperiodic oscillations are present. It is possible to confuse this component with noise. When it appears, a nonlinear model might be used for describing the dynamics of this component. %but at first instance to discard the possibility of being noise this component is analysed if has a zero mean and independent variance.
\begin{figure}[!htb]
	\centering
	  \includegraphics[width=80mm]{figures/transientVariationThin.eps}
	   \caption{Transient variation}\label{transcientVariation}
\end{figure} 

\end{enumerate}

%## dinamics
%transientVariation.pdf


 

In the time series classical analysis literature, decomposition is used to describe separately the trend and seasonal factors. Linear modelling can be classified according to the way of combining these components in additive, multiplicative and pseudo additive compositions \cite{a}.  Another kind of decompositions focuses on describing long-run cycles, like the weekends and holiday effects \cite{quevedo2014}. The classification of time series according to their decomposition is:

%More extensive decompositions might also include long-run cycles, holiday effects, day of week effects and so on.  Here, we?ll only consider trend and seasonal decompositions.

%WHAT ARE THE UNDERLYING MODELS USED TO DECOMPOSE THE OBSERVED TIME SERIES?
%Additive Decomposition

%Multiplicative Decomposition

%Pseudo-Additive Decomposition
%transientVariation.pdf

\begin{enumerate}[i]
\item Additive models consider the sum of the trend, seasonal, and random variation. It is used  when the seasonal variation is relatively constant over time. It is expressed by Equation \ref{additiveCompositionEq} \begin{equation}\label{additiveCompositionEq}
Y_t = \mathrm{T} + \mathrm{S} + \mathrm{R}
\end{equation} where the times series is generated by adding $\mathrm{T}$, $\mathrm{S}$, and $\mathrm{R}$, the trend, seasonal, and random components respectively. An example of the general structure of this model is shown in Figure \ref{additiveComposition}.



 \begin{figure}[h!]
				\centering
				\includegraphics[width=80mm]{figures/additiveCompositionExampleThin.eps}
				\caption{Additive composition example}
				\label{additiveComposition}
\end{figure} In this figure the red line is the trend approximation of the time series, and the black line is the real data with seasonal components and noise.

\item Multiplicative models are composed by the multiplication of the different components like trend, seasonal and random components. This kind of model is used when an increasing seasonal variation is observed along the time. It is expressed by Equation \ref{multiplicativeEq}.

\begin{equation}\label{multiplicativeEq}
Y_t = \mathrm{T} \times \mathrm{S} \times \mathrm{R}
\end{equation} where the components $ \mathrm{R}$, $ \mathrm{S}$ and $ \mathrm{T}$ are the same described previously, with the difference that the time series $Y_t$ is the result of the product of such components. An example of the structure using this kind of decomposition is shown in the Figure \ref{multiplicativeComposition}.



 \begin{figure}[h!]
				\centering
				\includegraphics[width=80mm]{figures/multiplicativeCompositionExampleThin.eps}
				\caption{Multiplicative composition example}
				\label{multiplicativeComposition}
\end{figure} The red dotted lines of the figure shows the multiplicative trend of variance of the time series $Y_t$.


\item The pseudo-additive decomposition (Mixed) presents a more complex dynamic, adding and multiplying the different components. It combines features of both the additive and the multiplicative models.  The general structure of this kind of model is expressed by Equation \ref{pseudoAdd}:  

\begin{equation}\label{pseudoAdd}
Y_t = \mathrm{TC_t} \times \mathrm{(S+I)-1}
\end{equation} where the addition of the seasonal effects, and irregular fluctuations $\mathrm{(S+I)}-1$ produces a trend in the mean, and the trend cycle component $\mathrm{TC}_t$ produces a multiplicative effect that increases the variance along time. Figure \ref{pseudoadditive} shows an example of how a time series of this type looks like. The low and high red dotted line bound the time series according to an increasing variance. The continuous red line is the mean increasing along the time.

%\textit{This model type is supported exclusively by the X-12-ARIMA method. TRAMO/SEATS does not support this kind of the model. This model is recommended for time series that display strongly fluctuating seasonal effects.} An example is shown in the figure \ref{pseudoadditive}
 \begin{figure}[h!]
				\centering
				\includegraphics[width=80mm]{figures/mixedCompositionExampleThin.eps}
				\caption{Pseudo-additive time series example}
				\label{pseudoadditive}
\end{figure} 

	 
\end{enumerate}



%Besides the structural components presented in linear modelling, it is also important to emphasise the importance of the concept of stochastic process for the linear regression methods.
 
 %In order to characterise a stochastic process the joint distributions functions for any set of variables.
 
\subsection{Decomposition Methodology in Classical Analysis}

In time series classical analysis, it is important the study of different components present in time series separately. Although time series analysis may be considered a sort of art, it requires statistical knowledge, theory and good practices for generating a valid and correct model. Box and Jenkins proposed a methodology to understand systematically how a time series behaves following a \textit{divide and conquer} strategy \cite{BoxJen}. Decomposing the data in simpler time series is crucial to identify and estimate the different components involved in the time series. An important assumption to consider is the generation of the data by a linear dynamical system. Considering the different components the general steps that should be taken into account, according to Box and Jenkins are:

\begin{enumerate}[i]
\item Observation: This step is the most important because with visual analysis is possible to have the intuition and information about general aspects of the data useful for selecting the proper general model to use. % [Examples of different kind of time series with different characteristics that are visible]
\item Trend estimation: To detect the estimation in the long term in time series can be used mainly two different kind of methods belonging to different approaches:
	\begin{itemize}
		\item Filtering: It is a function capable of neglecting the short term dynamics and keep the general dynamics. An example of this kind of algorithms is the smoothing effect of a moving average.
		\item Estimating a regression equation: A (linear or nonlinear) function might be used to describe the trend using least squares optimisation \cite{Sandra2013}. (Like the solid red line in Figures \ref{secularTrend}, \ref{additiveComposition} and \ref{pseudoadditive}). 
		%simple linear regression using least squares optimisation or another function like a quadratic function
	\end{itemize}
\item De-trending: When the trend is already known, for the additive composition the trend is removed subtracting it from the original time series. For the multiplicative composition the trend is removed dividing the time series by the trend.
\item Detection of seasonal factors: If the seasonality is known, the simplest method for estimating these effects is to average the de-trended values for a specific season using the Seasonal Sub-series Plot, otherwise a correlation plot helps finding the seasonality \cite{Cleveland1993} . 
\item Determine the random (irregular) component: The random component is analysed last. After obtaining the forecasting model, the variance of the noise by means of gaussian random distribution is analysed.
\end{enumerate}

 series.  Matlab does this (and estimates the trend with a straight line in the iteration.

\subsection{Stationarity and Sampling}


In order to study a phenomena it is important to reproduce it many times under the same conditions and be sure that the measurements taken effectively correspond to the same scientific study, object, or process. The concept of reproducibility in time series is strongly related to invariability and availability meanings.

\begin{enumerate}
\item Invariability of parameters during the analysis of the system: These parameters must not depend on time and they change once they are noticed or desired to produce intentionally an output.
\item Availability of the data addresses the problem of the incorrect modelling due the lack of information which impedes to detect and consider the non-stationarity of the process. Non-stationarity is difficult to detect and model since it is also difficult to know if the acquired information is enough and reliable providing a good description of the general dynamic of the data.

\end{enumerate}


%The second stationarity condition is related to the availability of the data.


When the phenomena is studied a finite number of times and behaves differently we can also say that it represents a non-stationarity process, and presents an intermittency effect. The intermittence is observed only if we can replicate the process enough times. Conversely, the analyst will not have enough data to produce a reliable model. Therefore, the length of the series must provide enough information to describe and take into account intermittency.

%Time series should provide minimum requirement as how long and precise the information should be.
\subsection{Testing for Stationarity}

The stationarity test is performed over stochastic processes. The simplest non-periodic stochastic process, is composed by a set random variables $\{Y_t\}_{t \in \mathcal{O}}$, where $Y_t \in \mathbb{R}$, $t$ is the time index included in the set $\mathcal{O}$, with infinite values in the interval $(0,\infty]$ in a continuous-time. For discrete-time process the random variable takes a finite number of $\mathbb{N}$ values $\mathcal{O}=(0,\mathbb{N}]$, restricting the length of the discrete time series. A stationarity process is present when the probabilities related to the random variables are constant over time. This is expressed in Equation \ref{statproc}.




\begin{equation}\label{statproc}
	Pr(Y_{t+\tau}) = Pr(Y_{t})
\end{equation} where $Pr$ is the probability of observing certain value, and $\{Y_{t},Y_{t+\tau},$  $\dots,Y_{t+p\tau}\}$ are the observations belonging to the same probability distribution $Pr(Y_t)$.




Stationarity is confirmed when there is no violation of the basic properties of the stochastic system like variance, mean, transition probabilities, and correlations. Considering the difficulty for detecting stationarity for periodic time series, it is recommended to get considerably longer data length than the period length for modelling to capture as much cyclic samples as possible. As soon as we get large data, we will have more chances to determine and distinguish the global trend, patterns and intermittence effects. 
%presented  This occurs when the sample data does not capture the cyclic phenomena with pattern repetitions every certain period $\tau$. %(containing a repetitive characteristic inside a period of time $\tau$)  
This means that the sample data size must be greater than the period, $\tau$. 

In other words we need a large data set of size $n$, such that $n \gg p \tau$, where $\tau$ is the size of the period and $p$ is the number of periods that guarantee $p$ characteristic samples, e.g., Figure \ref{incomsample} gives the impression of non-stationarity only if the data we have is limited, therefore it is not possible to determine periodicity. On the other hand, if we have more data, like in Figure \ref{fullsamples} a pattern is revealed. Hence local stationarity is detected. Figure \ref{fullsamples} shows complete cycles with a relative stationarity (since we do not know what will happen in the future). 

\begin{figure}[h!]
\centering
\parbox{5cm}{
\includegraphics[width=5cm]{figures/plotDemandsCompletePatternsDotted.pdf}
%
\caption{Incomplete pattern}
\label{incomsample}}
\qquad
\begin{minipage}{5cm}
\includegraphics[width=5cm]{figures/plotDemandsCompletePatternsThin2.pdf}
\caption{Four complete patterns}
\label{fullsamples}
\end{minipage}
\end{figure}

Assuming we have sufficient information, we can take two approaches for testing stationarity: the parametric and nonparametric approach. The parametric is usually used by researchers considering statistical assumption about the distribution of the data in the time field domain, such as economists and statesmen. Such assumptions can be tested according to the 1st or 2nd order stationarity criteria.

%http://www.maths.bris.ac.uk/~guy/Research/LSTS/TOS.html

\begin{itemize}
	\item 1st order stationarity is related to strict stationarity criteria. It assumes a time-independent joint statistical distributions and variations of the time series, therefore, the mean and variance at any moment must be the same. Strict stationarity satisfies Equation 
	
	$$
	[Pr(Y_{t_1+\tau'}),\dots,Pr(Y_{t_{n_s}+\tau'})] =[Pr(Y_{t_1}),\dots,Pr(Y_{t_{n_s}})]
	$$ where the probability of the shifted vectors satisfies the equality independently of the chosen value $n_s$, $t_1,\dots,t_{n_s} \in \mathcal{O} $, or lag $\tau'$ \cite{lindgren14}.
	
	
	In real time series this definition is very tight and time series presenting some irregularities almost never satisfy this condition.	
	\item 2nd order stationarity is about relaxed stationarity condition. It is more flexible and considers a stationary process  only if the mean (first moment) is constant and covariance (second moment) is finite and depends just on the time difference $\tau'=t-t'$ along time series.% this is that the mentioned basic statistical measures do not dependent of time.
\end{itemize}


The nonparametric approach does not require any assumption of the data, therefore it is more general. One of the most popular stationarity test is the Runs Test. The test defines a run as ``succession of one or more identical symbols or patterns'', which are followed and preceded by a different symbol or no symbol at all \cite{gibons83}. The idea behind is similarly to a series (or runs) generated by identical flips of a coin, where the symbol $O$ and $I$ represents heads and tails, respectively. An example of such run is the sequence $\mathbf{C}=\{OIIIIIIOOIOO\}$. If the coin is balanced, the sequence will present stationarity approximating the same number of $O$ and $I$ as long as more observations are available, otherwise a trend might be detected.

The stationarity of data can be determined by using the algorithm of the runs test proposed by Bendat and Piersol in \cite{Bendat1986} as follows:


\begin{enumerate}
\item  Divide into intervals $T$ of length $\tau$.
\begin{equation}
	Z_T' = \{Y_t\}_{t=\tau(T-1)+1}^{T \tau}
\end{equation}
\item Compute a mean value for each interval.
\begin{equation}
	\overline{Z}_T' = \frac{1}{{\tau}}\sum_{t=1}^\tau(Z_T')
\end{equation}
\item Collect the occurrences of values above ($I$) and below ($O$) the median value $\tilde{{Z}'}$ of the series using Equation \ref{collect}.
\begin{eqnarray}\label{collect}
  {C_T} = \begin{cases}
        I & \text{ }\text{ }  \text{ if $ \overline{Z}_T'> \tilde{{Z}'}) $} 
        \\
         O &\text{ }\text{ }   \text{ otherwise} \\
        \end{cases}
\end{eqnarray}
\item Count the number of runs with $I$ and $O$ following Equation \ref{count}


\begin{eqnarray}\label{count}
{C}^O = |\{i,\forall i \in \{1,2,\dots,T-1,T\}, | {C}_i = O \}| \\
{C}^I = |\{i,\forall i \in \{1,2,\dots,T-1,T\}, | {C}_i = I \}| \nonumber
\end{eqnarray} 

\item Compute the probability of having a run ${Pr}(\mathbf{C}^O)$ and ${Pr}(\mathbf{C}^I)$ and if ${Pr}(\mathbf{C}^O) \approx {Pr}(\mathbf{C}^I)$, the the stationarity test is passed.


%\item Compute the rate of counts found to known probabilities of runs for random data. If the the $\#I/\#O=1$ the time series is stationary, if $\#I/\#O>1$ it presents a positive trend, and negative trend other wise. 
\end{enumerate}


%\begin{equation}
%\text{if } |\mathbf{C}^O| \approx |\mathbf{C}^I| 
%\end{equation}


%The most recent index occurrence of $\hat{C}_{T+1}$ in the past is taken.


%Note that the runs test works equally well on mean values, mean square values, variance, standard deviation, or any other parameter estimate (Bendat and Piersol 1986). Known probabilities of runs distributions can be found in (Bendat and Piersol 1986), (Bethea and Rhinehart 1991), and (Gibbons 1985). 









%In this the stationarity is locally tested, 






%One way to estimate the longest relevant time scale is computing the inverse of the lowest frequency containing a fraction of the total power of the signal.
%The mean and variance are the most stable parameter in time series.



\subsection{Linear Autocorrelations}
%http://www.ptolomeo.unam.mx:8080/xmlui/bitstream/handle/132.248.52.100/363/A5.pdf?sequence=5


We have seen so far how the general characteristics of the time series are analysed discarding dynamic details at local level. In order to analyse what is going on with the observed dynamic fluctuations at local level of the time series, a relation analysis between variables is performed.

The relation analysis is an important part of the study of linear time series, it focuses on the existence of a linear temporal relation between the the variable of interest and another one is named autocorrelation. 

%The autocorrelation is defined as the mutual relation among the values of a time series in different time periods. It describes the changes of a value presented from one value to another.

There are two kinds of autocorrelation metrics used as tools for measuring the existence and intensity of such correlation among the data, the simple and partial autocorrelation.

\begin{itemize}
\item The Simple Autocorrelation Function (ACF), measures the lineal relation between the observation $Y_t$ from a time series and the delayed value $Y_{t-\tau}$.
\item The Partial Autocorrelation (PACF), is the estimation of the simple correlation but removing the effect produced by the autocorrelations for delays shorter than $\tau$.
\end{itemize}


The degree of relation for the simple and partial autocorrelation is measured using the autocorrelation coefficient $\rho_k$. The coefficient takes values in the interval $[-1,1]$.  This coefficient provides information about the existence and the type of correlation. A coefficient with value of $+1$ means a strong positive relation between two observations separated by $k$ units of time. The positive sign means that both variables respond similarly. A value of $-1$ expresses a strong inverse relation.  When a value of $0$ is obtained, the two variables do not exhibit any relation. Equation \ref{autocorr} computes the autocorrelation function.

%\begin{equation}\label{autocorr}
%\rho_\tau = corr_{\tau_\tau}(Y_t,Y_{t+\tau})= \frac{\sum_{t=1}^{n-k}(Y_t- \overline{Y})(Y_{t+\tau}-\overline{Y}) }{\sum_{t=1}^n (Y_t-\overline{Y})^2}
%\end{equation} 

\begin{equation}\label{autocorr}
\rho_\tau = corr_{\tau_\tau}(Y_t,Y_{t+\tau})= \frac{\gamma_\tau}{\gamma_0}
\end{equation}

where $corr$ is the autocorrelation function at lag $\tau$, $\rho_\tau$ is the coefficient of the simple autocorrelation for a delay of $\tau$, and $\gamma$ is the covariance function defined in Equation \ref{covariancef}

\begin{equation}\label{covariancef}
\gamma_\tau = cov(Y_t,Y_{t-\tau})= E[(Y_t-\overline{Y})(Y_{t-\tau}-\overline{Y})]
\end{equation} where $cov$ is the covariance function, $\overline{Y}$ is the mean of the time series, $Y_{t-\tau}$ is the observation at lag $\tau$, $E$ is the expectation and $\gamma_0$ is equal to 1 by definition following Equation \ref{gammazero}

\begin{equation}\label{gammazero}
\gamma_0 = var(Y_t)=\rho_0 = 1
\end{equation} where $var$ is the variance. This property is found in \cite{Fra85}.

%$\overline{Y}$ is the mean of the time series values, $Y_{t+\tau}$ is the observation in the period at delay $\tau$, and $n$ is the length of the series. The autocorrelation gives information about the Moving Average (MA) polynomial order.


%http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx?fileticket=4_BxecUaZmg%3D
The Partial Autocorrelation Function (PACF) of $\tau$ order is similar to the ACF that measures the correlation between two variables $Y_t$ and $Y_{t-\tau}$, but with the difference of discarding the influence of the dependency of the intermediate lags between both of them. The partial autocorrelation gives information about the AR order. The formula proposed by Levinson and Durbin \cite{Fra85} is described in Equation \ref{parcorr}


\begin{eqnarray}\label{parcorr}
\pi_{11} &=& \rho_1 \\
\pi_{\tau, \tau} &=&  \frac{\rho_\tau - \sum_{j=1}^{\tau-1} \varphi_{\tau-1,j}*\rho_{\tau-j} }{1-\sum_{j=1}^{k-1}\varphi_{\tau-1,j}, \rho_{\tau-j}} \nonumber
\end{eqnarray} where $ \varphi_{\tau,j} = \varphi_{\tau-1,j}-\varphi_{k,k} \varphi_{\tau-1}$ is the covariance at different lags $j=1,2,\dots,\tau-1$.  %$Cov$ is the covariance, and $Var$ is the variance function. In Chapter \ref{BoxJenkins} are addressed the AR and MA polynomial models as part of the Box-Jenkins ARIMA methodology.


Equation ACF \ref{autocorr} and PACF \ref{parcorr} are used to produce plots named autocorrelograms. Each autocorrelogram is composed plotting the coefficients $\rho_\tau$ and $\pi_{\tau,\tau}$ respectively varying $\tau$ at different lags. Each autocorrelogram provides information about the Moving-Average (MA) and Auto-Regressive (AR) polynomial order and structure \cite{BoxJen}. The description of the Box-Jenkins ARIMA methodology is addressed in Chapter \ref{BoxJenkins}.

The Ljung-Box test is an auxiliary test born in 1970 \cite{Box1970} and implemented to quantify the model fitting \cite{LJUNG1978} measuring the independency of the values of time series. This test gives information about the possible existence of linear relations in the data or if the data sequence is merely random noise.  The Ljung-Box test is given by Equation \ref{lbtest}

\begin{equation}\label{lbtest}
	LB_m = n(n+2) \sum_{t=\tau}^m \frac{{\pi}_{t,t}}{n-\tau} 
\end{equation} where $n$ is the sample size, $m$ is the lag length and ${\pi}_{t,t}$ is the autocorrelation of the sample at lag $t$.

The $LB_m$ value is evaluated using the $\chi^2$ test. If $LB_m$ exceeds the critic value of the $\chi^2$ to the significance level selected (usually at $95\% $). The test assumes the mull correlation hypothesis $H_0$ among the values until the opposite is demonstrated. If the alternative is demonstrated, then $H_0$ would be rejected and the existence of significant correlation hypothesis $H_1$ is taken. This is expressed in Equation \ref{lbqthypotesys}.




\begin{eqnarray}\label{lbqthypotesys}
 \begin{cases}
        H_0: \text{No Rejection of null hypothesis} & \text{ }\text{ }  \text{ if $LB_m < \chi_{(\mathbf{m})}^2$} 
        \\
        H_1: \text{Rejection of null hypothesis} &\text{ }\text{ }   \text{ otherwise   } \\
        \end{cases}
\end{eqnarray}



% Linear systems need irregular inputs to produce irregular signals.

%A stochastic process is stationary if these probabilities are constant over the time 


%\begin{displaymath}
%   S_{rk} = \left\{
%     \begin{array}{lr}
%       \frac{1}{\sqrt{n}} & : k = 1\\
%        \sqrt{\frac{1+2 \sum_{i=1}^{k-1}r_i^2 }{n}} & : k = 1,2,3\dots
%     \end{array}
%   \right.
%\end{displaymath} 


%\begin{displaymath}
%   r_{kk} = \left\{
%     \begin{array}{lr}
%       r_1 & : k = 1\\
%       \frac{r_k - \sum_{j=1}^{k-1} r_{k-1 j} r_{k-j}}{1-\sum_{j=1}^{k-1} r_{k-1} j r_j} & : k = 2,3\dots
%     \end{array}
%   \right.
%\end{displaymath} where 

%\begin{equation}
%r_{kj} = r_{k-1 j}  -r_{kk}r_{k-1 j} r_j  \text{ for } j = 1,2,\dots,k-1
%\end{equation} 


%The mean of the probability distribution $p(Y)$ is inferred from the time series itself and is described by 

%\begin{equation}
%		\int_{\infty}^{-\infty} dY'Y' p(Y')
%\end{equation}
% and for the finite time series is represented by the mean itself
 
 
% \begin{equation}
%		\langle \hat{Y}_n \rangle = \frac{1}{N} \sum^{N}_{n=1} Y_n
%\end{equation} where $ \langle \hat{s}_n \rangle$ is the average over the time and $N$ is the total number of measurements from the time series.  The variance of the probability distribution will be estimated by the variance of the time series. The covariance for the continuous time time series is shown in Equation \ref{variancecon} and for the discrete time in Equation \ref{variancedis}. 


%\begin{equation}\label{variancecon}
%	\sigma^2 := \langle(s-\langle Y \rangle)^2 \rangle = \int_{-\infty}^{\infty} ds'(Y'\langle Y \rangle)^2 p (Y')
%\end{equation}

%\begin{equation}\label{variancedis}
%	\hat{\sigma}^2 = \frac{1}{N-1} \sum_{n=1}^{N} \left(Y_n - \hat{\langle s \rangle } \right) ^2 = \frac{1}{N-1} \left(\sum_{n=1}^{N} Y^2_n - N \hat{\langle Y \rangle}^2 \right)
%\end{equation} where $\sigma$ is the standard deviation or the RMS (Root Mean Squared Error) as opposed to the variance at $\sigma^2$. And finally the autocorrelation at lag $\tau$ is given by the Equation \ref{autocorrc}.

%\begin{equation}\label{autocorrc}
%	c_{\tau} = \frac{1}{\sigma^2} \langle (Y_n - \langle Y \rangle ) (s_{n-\tau} - \langle s \rangle ) \rangle = \frac{ \langle s_n s_{n-\tau } \rangle -  {\langle s \rangle}^2 } { \sigma}
%t\end{equation}

%\subsection{Testing for Autocorrelations}

%\subsubsection{Q test}


%https://explorable.com/partial-correlation-analysis
%

\section{Box-Jenkins Auto-Regressive Forecasting}\label{BoxJenkins}

%https://www.ucm.es/data/cont/docs/518-2013-11-11-JAM-IAST-Libro.pdf
%http://faculty.smu.edu/tfomby/eco6375/BJ%20Notes/BJ_notation.pdf

%En 1970, Box y Jenkins desarrollaron un cuerpo metodológico destinado a identificar, estimar y diagnosticar modelos dinámicos de series temporales en los que la variable tiempo juega un papel fundamental.

A very useful framework for time series analysis is the Box-Jenkins Auto-Regressive method. In 1970 Box and Jenkins proposed this framework to identify, estimate, and diagnoses dynamic models of time series, where the variable time is important. The objective of Box-Jenkins is to provide an algorithmic way to construct forecasting models \cite{Econometr2010}.

%En 1970, Box y Jenkins desarrollaron un cuerpo metodológico destinado a identificar, estimar y diagnosticar modelos dinámicos de series temporales en los que la variable tiempo juega un papel fundamental. Una parte importante de esta metodología está pensada para liberar al investigador económetra de la tarea de especificación de los modelos dejando que los propios datos temporales de la variable a estudiar nos indiquen las características de la estructura probabilística subyacente. En parte, los procedimientos que vamos a analizar se contraponen a la "forma tradicional" de identificar y especificar un modelo apoyándonos en las teorías subyacentes al fenómeno analizado aunque, convenientemente utilizados, los conceptos y procedimientos que examinaremos constituyen una herramienta útil para ampliar y complementar los conocimientos econométricos básicos.




\subsection{Auto-Regressive (AR) Models }

The first set of models to capture the dynamic of linear time series is the Auto-Regressive model. Auto-Regressive models explain an endogenous variable of a period $t$ by the previous observations adding an error term (under previous conditions, $Y_t$ can be expressed as a linear combination of its past values plus an error term). The notation of the  auto-regressive models are abbreviated as AR models, a $p$-order model is written as $\text{AR}(p)$. Where $p$ is the number of observations in the past that are taking into account by the model. The basic Auto-Regressive model is expressed as AR(1) and is represented by Equation \ref{ARM}.

\begin{equation}\label{ARM}
\hat{Y}_t = \Phi_0 + \Phi_1 Y_{t-1} + \epsilon_t
\end{equation} where $\hat{Y}_t$ is the forecast value, $\epsilon_t$ is the error term with the property of having a zero mean, constant variance, and null covariances among errors associated with different observations. The general $\text{AR}(p)$ model is expressed by Equation \ref{armodel}

\begin{equation}\label{armodel}
\hat{Y}_t = \sum_{i=1}^{p}\Phi_i Y_{t-i} + \Phi_0 +\epsilon_t
\end{equation} where the $\Phi_i$ components are the auto-regressive coefficients, $p$ is the order of the model and $\epsilon$ is assumed to be a random variable with a normal distribution with zero mean $N(0,\sigma)$.

This can be simplified using the lag polynomial operator. Therefore Equation \ref{lag} defines the lag polynomial operator $\Phi_p(L)$
%In order to simplify the notation is used the lag polynomial operator

\begin{equation}\label{lag}
	\Phi_p(L) = 1 -\Phi_1 (L) -\Phi_2 (L)^2 ,\dots,\Phi_p (L)^p
\end{equation} where $\Phi_0=1$ and $\Phi_p(L)$ is defined in Equation \ref{arsim}.

\begin{equation}\label{arsim}
%\Phi_p (L) Y_t = \Phi_0 +\epsilon_t
\Phi (L) Y_t =\epsilon_t
\end{equation} More details about the lag notation and manipulation is found in \cite{a}. %[https://www.otexts.org/fpp/8/2]




\subsection{Moving-Average (MA) Models }

The Moving-Average (MA) model explains the value of a variable at time instant $t$, as a weigh function of the previous independent error terms. The model is expressed as $\text{MA}(q)$ where $q$ is the number of error terms. The basic moving-average model is expressed as $\text{MA}(1)$ and is written as Equation \ref{masim}:


\begin{equation}\label{masim}
Y_t = \mu + \epsilon_t+ \Theta_1 \epsilon_{t-1} 
\end{equation} where $\mu$ is the mean and $\Theta$ is the coefficient of the model and $\epsilon_{t-1} $is the previous prediction error. Equation \ref{masim} can be generalised as shown in Equation \ref{magen}.

\begin{equation}\label{magen}
Y_t = \sum_{i=1}^{q} \Theta_{(t-1)} \epsilon_{(t+1-i)} +\mu 
\end{equation} and the lag polinomial notation is used as well to express any $\text{MA}(q)$ model as shown in Equation \ref{MAlag}.



\begin{equation}\label{MAlag}
\Theta_q(L) = 1 -\Theta_1 (L)^1-\Theta_2 (L)^2 ,\dots,\Theta_q (L)^q
\end{equation}

Letting $\Theta_0 = 1$ the \text{MA} process can be written as in Equation \ref{compactMA}

\begin{equation}\label{compactMA}
\hat{Y}_t= \Theta(L)\epsilon_t
\end{equation}




\subsection{ARIMA(p,d,q)}

The \text{ARMA} model is a case of the ARIMA model that includes the auto-regressive and moving-average components. It is written as ARMA$(p,q)$ where $p$ is the order of the auto-regressive component and $q$ of the moving average component (Equation \ref{ARMA})

\begin{equation}\label{ARMA}
	\hat{Y}_t = \Phi_0 +  \epsilon_t + \sum_{i=1}^p (\Phi_i Y_{t-i}) + \sum_{i=1}^q \Theta_i \epsilon_{t-i}
\end{equation} the lag polinomium notation is used to represent the model


%\begin{equation}
%	\left( 1- \sum_{i=1}^p \Phi_i L^i \right) Y_t = \left( 1 + \sum_{i=1}^q \Theta_i L^i \right) \epsilon_t
%\end{equation}
\begin{equation}
	\Phi(L)(\hat{Y}_t-\mu) = \theta(L) \epsilon_t
\end{equation}

%\subsection{ARIMA}
The nonseasonal \text{ARIMA} is written as $\text{ARIMA}(p,d,q)$ where $d$ is the degree of differencing associated with the component of integration $(1-L)^d Y_t$. This component discards any non stationary trend from the time series and is present in Equation \ref{arimad}. % and can also get rid of a seasonal random walk type of nonstationarity.

\begin{equation}\label{arimad}
	\left( 1 - \sum_{i=1}^p \Phi_i (L)^i  \right) (1-L)^d Y_t = \left( 1 + \sum_{i=1}^q \Theta_i (L)^i\right)
\end{equation}
%http://people.duke.edu/~rnau/Notes_on_nonseasonal_ARIMA_models--Robert_Nau.pdf



\subsection{Seasonal ARIMA}

The general version of the Box-Jenkins models is the Seasonal ARIMA (SARIMA) model. This model also considers the modelling of the seasonal components of the time series. This model uses two polynomials expressed by ARIMA $ (p,d,q)$ $ (P,D,Q)_{\check{s}}$. Where the notation component  $(p,d,q)$ is the nonseasonal part of the model and  $(P,D,Q)_{\check{s}}$ is the seasonal component where $m$ is the number of measurements per season or cycle. The order of the different polynomials are expressed by $p, d, q, P, D,$ and $Q$. In order to illustrate how the seasonal component extends the nonseasonal model, we analyse a SARIMA model $(1,1,1)(1,1,1)_{\check{s}}$ presented in Equation \ref{SARIMAEX}:

\begin{equation}\label{SARIMAEX}
	(1 - \Phi_1 (L)) (1-\phi (L)^s ) (1-L)(1-(L)^{\check{s}}){Y_t} = (1+\Phi_1 (L))(1+\phi_1 (L)^{\check{s}})\epsilon_t	
\end{equation}  where the seasonal AR, MA, and the difference components consider a seasonal lag at time $\check{s}$ in the component $(L)^{\check{s}}$ that multiplies their respective nonseasonal components.


%http://goo.gl/2nIIjX
The Box-Jenkins model optimisation parameters is a well studied topic, therefore it is possible to find statistical optimisation methodologies and their implementation in different software packages. Regarding the optimisation methods, one of the most popular methodologies are the minimisation of the exact maximum likelihood  \cite{1988}, and least squares fitting methods. The software packages that provide these functionalities are found in \textit{statsmodels} (\textit{Python}), Econometrics Toolbox (\textit{MATLAB}), \textit{ARIMAProcess} Function (\textit{MATHEMATICA}), and the \textit{arima} command (\textit{STATA}), among others.

\section{Holt-Winters: Exponential Smoothing}


An alternative to the Box-Jenkins forecasting is the exponential smoothing method. The basic framework of this method was proposed in the late 50's by C.C. Holt in 1957 and his student Peter Winters \cite{Goodwin2010}. Basically, the forecast produced by this method is the result of weighing the average of the past observations with weights decaying exponentially as the observations get older. The advantages related to this framework is to generate forecasts easily with very low storage requirements, ease of automation, reliable and adaptable to new changes in trends and seasonal patterns. These characteristics make these methods suitable to be applied widely in the industry due to their simplicity \cite{Kalekar2004}. 


%file:///Users/rodrigolopez/Downloads/0046351dc5a91a08de000000.pdf


\subsection{Single Exponential Smoothing}

The basic Holt-Winters comes from a family of exponential smoothing methods based on the continuously updated forecast giving more relevance to the recent experience. Exponential smoothing  is named in this way because the weights decrease exponentially as the observations gets older. The single exponential smoothing is used for short term forecasting. It assumes that the dynamics of the data fluctuate around a reasonably stable mean. That is, the assumption of lack of trend or seasonal components of the time series under analysis. Given the information available in time $t$, the exponential smoothing is given by Equation \ref{expsm}.

\begin{equation}\label{expsm}
\hat{Y}_{t+1} =\sum_{i=0}^{\infty}  w_t Y_{t-i} 
%\\S_t=\alpha *X_t + (1-\alpha) *S_{(t-1)}
\end{equation} where $W =\{w_0,w_1,\dots, w_{\infty}\}$ are the weights given to the past values of the series with the property of summing $1$. The recent values have a higher weight that the old ones. The weighs are set using Equation \ref{setexpvals}

\begin{equation}\label{setexpvals}
w_i=\alpha(1-\alpha)^i
\end{equation} where $i=\{1,2,\dots,\infty\}$ and $\alpha$ is a constant real value $0<\alpha<1$ that regulates the relevance of the previous values to produce the forecast. Each successive observation in the series is a new smoothed value. This value is computed as the weighted average of the current observation and the previous smoothed observations. Substituting Equation \ref{setexpvals} in \ref{expsm} are obtained the forecasting equations \ref{expsmootfff}-\ref{iterativesmoothing}.

\begin{equation}\label{expsmootfff}
\hat{Y}_{t+1} = \alpha Y_t + \alpha(1-\alpha)Y_{t-1} + \alpha(1-\alpha)^2 Y_{t-2} + \dots + \alpha(1-\alpha)^m Y_{t-m}
\end{equation} where $m$ is the number of considered lags. Factoring Equation \ref{expsmootfff} we obtain:
\begin{equation}\label{expsmootf2}
\hat{Y}_{t+1} =  \alpha Y_t + (1-\alpha)(\alpha Y_{t-1} + \alpha (1-\alpha) Y_{t-2} + \dots +  \alpha (1-\alpha)^{m-1} Y_{t-m})
\end{equation} which is expressed iteratively in Equation \ref{iterativesmoothing}
\begin{equation}\label{iterativesmoothing}
\hat{Y}_{t+1} = \alpha Y_t +(1-\alpha)\hat{Y}_t
\end{equation} where $\hat{Y}_t$ is the previous estimate. An alternative notation of the smoothing forecasting is given by Equation \ref{erroriterative}

\begin{equation}\label{erroriterative}
\hat{Y}_{t+1} = \alpha \epsilon_n + \hat{Y} _t
\end{equation} where $\epsilon_n = Y_n- \hat{Y}_n$. It works as the correction for Equation \ref{iterativesmoothing}.



%The initialisation of the value $S_t$ is important to the accuracy of the forecast. One way to  initialize it is setting $S_t=y_t$. Another way to initialize it is to take the average values of the first observations. Between the $\alpha$ and $S_t$ there exists a trade off: the smaller the value of $\alpha$ more relevant $S_t$ becomes.



\subsection{Double Exponential Smoothing}

Double exponential smoothing is an extension of the single exponential smoothing that considers the trend in time series. The exponential smoothing component that capture the linear trend is described by the linear Equation \ref{eq}

\begin{equation}\label{eq}
	\hat{Y}_{t} = \ell_{t-1} + b_{t-1} 
\end{equation} where the terms $\ell_t$ and $b_t$ are the level and slope (trend) at time $t$. This model is used to predict stable long-term time series. The $h$-step ahead prediction, also named Holt's method, is performed by Equation \ref{holtsmethod}

\begin{equation}\label{holtsmethod}
	\hat{Y}_{t+h}=\ell_{t-1} +h b_{t-1}
\end{equation} where $h$ is positive and defines the prediction at time $h$. A deeper analysis of this component is found in \cite{tho80}. The level and trend components are smoothed according to Equation \ref{smoothing}-\ref{smoothing-1}


\begin{eqnarray}\label{smoothing}
\ell_{t-1} &=& Y_{t-1} + (1-\alpha)^2 \epsilon_{t-1} \\
b_{t-1} &=& b_{t-2}+\alpha^2 \epsilon_{t-1} \label{smoothing-1}
\end{eqnarray} where $\epsilon_{t-1}$ is the previous forecasting error defined by Equation \ref{errorequation}

\begin{eqnarray}\label{errorequation}
\epsilon_{t-1} = \hat{Y}_{t-1} -Y_{t-1}
\end{eqnarray}

The Holt-Winters linear trend estimates the trend using a weighting function which considers more relevant the most recent observations. This estimation uses a local trend equation where the linear trend $\ell$ and slope $b$ components of the forecasting Equation \ref{holtsmethod} are updated each period with Equation \ref{errorhwmode} and \ref{errorhwmode-1}.



\begin{eqnarray}\label{errorhwmode}
\ell_t &=& \alpha Y_t + (1-\alpha_1)(\ell_{t-1}+b_{t-1}) \\
b_t &=& \alpha_2 (\ell_t-\ell_{t-1})+ (1-\alpha_2)b_{t-1} \label{errorhwmode-1}
\end{eqnarray}

Similar to single exponential smoothing, the slope and level components also can be expressed as a function of their errors using the correction form presented in Equation \ref{errorhwmode2} - \ref{errorhwmode2-1}.


\begin{eqnarray}\label{errorhwmode2}
\ell_t &=& \ell_{t-1}+b_{t-1}+\alpha_1 \epsilon \\
b_t &=& b+{t-1}+\alpha_1 \alpha_2 \epsilon \label{errorhwmode2-1}
\end{eqnarray}


The problem of the initialisation of parameters $\ell_1$ and $b_1$ has been addressed before \cite{Kalekar2004}. Different methods have been proposed for the initialisation of these values. For $b_1$, it is possible to use the alternatives given by Equation \ref{eq2} - \ref{eq2-1}, for $\ell_1$ it can be initialised to $Y_1$.


\begin{eqnarray}\label{eq2}
b_1 & = & Y_2 \\
b_1 & = & [(Y_2-Y_1)+(Y_3-Y_2)+(Y_4-Y_3)] /3 \\
b_1 & = & (Y_n-Y_1)/(n-1) \label{eq2-1}
\end{eqnarray}

The three initialisation options approximate a trend in different ways; the first ones is by taking just the second element of the time series, the second is computing the average of the three first differences of the time series, and as third option is considered the slope produced by the division of the subtraction of the fist element $Y_1$ of the last element $Y_n$.

%and for $m$ empirically is found that a good estimation for the initial value is $m_1 = Y_1$


%Analog to the single exponential smoothing, the slope and level components can be written in function of their errors using the correction form presented in Equation \ref{slopelevelcorr}.

%\begin{eqnarray}\label{slopelevelcorr}
%s_t & = & s_{t-1} + b_{t-1} +\alpha_{1} \epsilon_t \\
%b_t & = & b_{t-1} + \alpha_1 \alpha_2 \epsilon_t \nonumber
%\end{eqnarray}



\subsection{Seasonal Holt-Winters}
%https://www.otexts.org/fpp/7/5

The Holt-Winters extension for seasonal components in time series is composed basically by three equations: the level equation $\ell_t$, the trend equation $b_t$ and the seasonal component equation $s_t^1$. With smoothing parameters $\alpha_1,\alpha_2$, and $\alpha_3$, and the seasonal parameter $\tau$ that expresses the period of the seasonality. These components are described in Equation \ref{SHW}-\ref{SHW-1}

\begin{eqnarray}\label{SHW}
\hat{Y}_{t+h} & =& \ell_t + h b_t + s^1_{t-\tau+h^+_\tau} \\
\ell_t &=& \alpha_1 (Y_t-s^1_{t-\tau})+(1-\alpha)(\ell_{t-1}+b_{t-1}) \\
b_t &=& \alpha_2 (\ell_t-l_{t-1})+(1-\alpha_2)b_{t-1} \\
s^1_t &=& \alpha_3(Y_t-\ell_{t-1}-b_{t-1}) +(1-\alpha_3) s^1_{t-\tau} \label{SHW-1}
\end{eqnarray} where $h_\tau^+ = [(h-1) \text{ mod } \tau]+1$.

The level equation computes an average of the weights between the seasonal measurement $(Y_t - s^1_{t-\tau})$ and the nonseasonal prediction $(\ell_{t-1}+b_{t-1})$ in time $t$. The trend equation, is similar to Equation \ref{errorhwmode}. The seasonal equation is an average of the weights between the current seasonal time, $(Y_t-\ell_{t-1}-b_{t-1})$, and the same seasonal time of the previous year. The corresponding error correction alternative is given by equations \ref{errorseasonal} - \ref{errorseasonal1}.



\begin{eqnarray}\label{errorseasonal}
\ell_t &=& \ell_{t-1} + b_{t-1} +\alpha_1 \epsilon_t \\ 
b_t &=& b_{t-1} + \alpha_1 \alpha_2 \epsilon_t \\ 
s^1_t &=& s^1_{t-m} + \alpha_3 \epsilon_t  \label{errorseasonal1}
\end{eqnarray} 



where $\epsilon_t = (\ell_{t-1}+b_{t-1} +s^1_{t-\tau}) = Y_t - \hat{Y}_{t|(t-1)}$ are the one-step training for the forecast errors.

\subsection{Double Seasonal Holt-Winters}

The double seasonal Holt-Winters (DSHW) is an extension of the previous method adding an extra seasonal equation ($s^2$) \cite{Taylor2006}, that captures a second seasonal period. It is also added other smoothing and seasonal parameter $\omega$ and $\tau_2$ respectively. The new parameters have information about a bigger periodicity multiple of $\tau_1$. The additive DSHW is described in Equation \ref{DSHW} - \ref{DSHW1}.

\begin{eqnarray}\label{DSHW}
\hat{Y}_{t+h} & = & (\ell_t+h b_t)+s^1_{t-\tau_1+h}+ s^2_{t-\tau_2+h} \\
\ell_t &=& \alpha_1 (Y_t-(s^1_{t-\tau_1} +s^2_{t-\tau_2}))+  \\ \nonumber 
& & (1-\alpha)(\ell_{t-1}+b_{t-1}) \\
b_t &=& \alpha_2 (\ell_t-\ell_{t-1} ) + (1-\alpha_2)b_{t-1} \\
s_t^1 &=& \alpha_3(Y_t -(\ell_t +s^2_{t-\tau_2})) + (1-\alpha_3)+s^1_{t-\tau_1} \\
s_t^2 &=& \omega (Y_t-(\ell_t +  s_t^1)) + (1-\omega)+ s^2_{t-\tau_2} \label{DSHW1}
\end{eqnarray} %where $l_t$ and $b_t$ are the level and trend. $s^1$ and $s^2$ are the two indices and $\alpha_1,\alpha_2, \alpha_3$ and $\omega $ are the smoothing parameters and 

Where $Y_{t+h}$ is the $h$-th step ahed forecast from forecast origin $t$. The parameters are optimized by minimizing the sum of squared errors of one step-ahead residuals. In order to use the multiplicative version the Single and Double Seasonal Holt-Winters, they are obtained by the product of the seasonal components \cite{arima,SBM2007}. 





%\section{Chaotic Nonlinear time series analysis}



%Some kind of time series can present dynamics difficult to model with linear models. The nonlinearity in their behaviors can produce false residuals in their predictions with apparent normal random distribution making unreliable the use of linear methodologies. In nonlinear time series prediction specially in machine learning, there exists methodologies in the literature. like K-Nearest Neighbors (k-NN), Artificial Neural Networks (ANN) and Support Vector Machines (SVM) that are sample based learning algorithms that learn the relation between the input and the output of the time series.

%Although machine learning algorithms can deal reasonably well in practice with the nonlinearity, there exists complementary statistical theory oriented to the analysis of time series that also present chaotic behavior that cannot be analysed with correlation analysis due to its non-periodicity. For this reason other methodologies should be considered.



%Under the theory of  chaotic time series prediction, there are different methodologies for analyzing the characteristics of this kind of time series as, false neighbours to discover the embedded dimension, Mutual Information (MI) for determining the level of uncertainty of the information in time series. This section includes two basic algorithms used frequently to analyse the the nonlinearity of the time series constructing the attractor in the phase space.  Mutual Information Criteria and false nearest neighbors to determine the sampling rate and the number of measurements that should be used for k-nearest neighbors forecasting algorithm. 



%\subsection{Mutual Information}
%http://www.physics.emory.edu/faculty/weeks//research/tseries3.html
%http://chaos.utexas.edu/manuscripts/1064949034.pdf
%http://www.scholarpedia.org/article/Mutual_information

%http://www.bcu.gub.uy/Comunicaciones/Jornadas%20de%20Economa/iees03j3111112.pdf

%The Mutual Information method is an algorithm that determines the delay coordinates for plotting attractors in the phase space. An attractor is set of values toward which variables in a dynamical system tend to evolve.

%Mutual Information (MI) is an index that measure the organisation of the variables among them. It is an a-dimensional and in some literature manage the units of bits. When the bits value is low means reduction in the uncertainty and in contrary an increase of the uncertainty. When MI is zero means independency among the variables.


%For two discrete variables X and Y whose joint probability distribution is $P_{XY}(x,y)$ , the mutual information between them, denoted $I(X;Y)$ , is given by
%\begin{equation}
%	I(X;Y) = \sum_{y \in Y} \sum_{x \in X} log \left( \frac{p(x,y)}{p(x)p(y)}\right)
%\end{equation} where $p(x,y)$ is the joint probability distribution function of $X$ and $Y$, and $p(x)$ and $p(y)$ are the marginal probability distribution functions of $X$ and $Y$ respectively.



%\subsection{False Neighbours for embedded dimension estimation}


%The false nearest neighbours (FNN) algorithm is presented as a method for determining the proper regression vector for recreating the dynamics of nonlinear systems. This algorithm which was originally developed for the analysis of chaotic time series, is used to determine the proper regression vector for input -output system identification and inferential prediction using time series data. The FNN algorithm is presented, and the problem of analysing noise corrupted time series is discussed. The choice of adjustable parameters in the algorithm and the data requirements for the algorithm are also discussed. The application of the algorithm to the identification of an electrical leg stimulation experiment and an industrial pulp digester model is presented and the results are analysed.

%False neighbours is the alternative to the correlation methods in linear systems to compute the lags needed to make a forecast in nonlinear systems. The false neighbours analysis comes from the idea of that similar data points in the embedding space are unlikely to have similar evolution since they diverge. 
%\begin{equation}
%	X_{fnn}(r) = \frac{ \sum_{n=1}^{N-m-1} \theta \left( \frac{|\mathbf{s}_n^{(m+1)} - \mathbf{s}_{k(n)}^{m+1}|}{|\mathbf{s}_n^{(m)} - \mathbf{s}_{k(n)}^{m}|}   -r \right)  \theta \left(\frac{\sigma}{r} -|\mathbf{s}_n^{(m)} -\mathbf{s}_{k(n)}^{(m)}| \right)}{\sum_{n=1}^{N-m-1} \theta \left( \frac{\sigma}{r} - \mathbf{s}_n^{(m)} - \mathbf{s}_{k(n)}^ {m} \right)  }
%\end{equation} where $\mathbf{s}_{k(n)}^ {m}$ is the nearest neighbor to $\mathbf{s}_n$ in the $m$- dimensional space. 

\section{k-Nearest Neighbors Forecasting}

The k-Nearest Neighbors (kNN) is an nonparametric learning algorithm used in classification and forecasting. It takes decisions based on the experience contained in the training set (in the best case a subset of them). kNN assumes that the data is in a feature metric space. The considered data might be scalars, multidimensional vectors, labels or characters, etc.
kNN forecasting is used to solve the problem of estimating the next elements $Y_{t+1},\dots, Y_{t+h}$ given a set of examples of the time series. kNN is considered a lazy learning algorithm which given a sample of past information, finds in the history similar scenarios to imitate their dynamics and use them to construct a short term forecast. kNN only needs three parameters: the sampling rate $\tau$, the embedding dimension m, and the geometric size of the neighborhood defined in $\varepsilon$ (where the objects inside the radius $\mathds{\varepsilon}$ are considered part of the neighborhood). kNN forecasting requires a times series defined in Equation \ref{knots}



%The k-Nearest Neighbors (kNN) is an nonparametric learning algorithm used in classification and forecasting. It takes decisions based on the experience contained in the training set (in the best case a subset of them). kNN assumes that the data is in a feature metric space. The considered data might be scalars, multidimensional vectors, labels or characters.  kNN forecasting is used to solve the problem of estimating the next elements  given a set of examples of the time series. kNN is considered a lazy learning algorithm which given a sample of past information, find in the history similar scenarios to imitate their dynamics and use them to construct a short term forecast. kNN only needs three parameters: the sampling rate $\tau$, the embedded dimension $m$, and  the geometric size of the neighborhood defined in $\mathds{\epsilon}$ (where the objects inside the radius $\mathds{\epsilon}$ are considered part of the neighborhood). kNN forecasting requires a times series defined in Equation \ref{knots}


\begin{equation}\label{knots}
\mathbf{Y}_{full} = \{Y_1,\dots,Y_t,\dots,Y_n\}
\end{equation} where $Y_1$ is the first element of the time series, $Y_n$ is the latest value at time $n$ and $t$ is the $t-$th measurement. For this purpose, the time series is organised in subsequences named delay vectors of the form.

$$\mathbf{D}_t=\{Y_{t-(m+1)\tau},\dots,Y_{t}\} $$ where $m$ is the size of the delay vector $\mathbf{D}_t$, and $\tau$  is the sampling frequency over the time series where $t=\{(m-1)\tau+1, (m-1)\tau+2,\dots,n-1-h \}$. To get the nearest neighbors of $\mathbf{D}_t$, the parameter $\varepsilon$ is defined to get the delayed neighbor vectors $v_1,...,v_k$ respect with $\mathbf{D}_t$ satisfying Equation \ref{nb}

\begin{equation}\label{nb}
dist (	\mathbf{D}_{v_\kappa},\mathbf{D}_t	) \in \varepsilon
\end{equation} where $dist$ can be any distance. For magnitudes is common to use the Euclidean distance, but for comparing sequences of qualitative information, the Hamming distance is an adequate metric. For real numbers $dist$ can be any norm. The more used norms are infty and euclidean distance.

\subsection{Real Numbers Forecasting}

The forecasting for real numbers where the vector to forecast is a real number vector $\mathbf{D}_t \in \mathds{R}^m$ is performed by using Equation \ref{knnpred} with the vectors that satisfy the constraint expressed in Equation \ref{nb}. The forecasting value is the result of the weighted sum of the \textit{futures} $Y_{t+ h\Delta n}$ of the selected vectors $v_1,\dots,v_\kappa$ and it is defined in Equation \ref{knnpred}

\begin{equation}\label{knnpred}
	\hat{Y}_{t + h \Delta n} = \alpha_j \sum_{j=1}^\kappa Y_{t + h \Delta n}
\end{equation} where $h \Delta n$ is the prediction horizon at next instants $\{1,\dots,h\}$, $\{\alpha_j\}_{j=1}^\kappa$ are weights that add up to $1$ expressed by Equation \ref{weightsknn}:

\begin{equation}\label{weightsknn}
\sum_{j=1}^\kappa \alpha_j =1
\end{equation} There are two popular ways to assign the values to the weights $\alpha$:

\begin{enumerate}
\item Set all the weights equally to predict the average value. 

\begin{equation}
	\alpha_j = \frac{1}{\kappa}
\end{equation}

\item Set the weights inversely proportional to the closeness of the vectors $\mathbf{D}_{v_k}$ respect with $\mathbf{D}_t$ as described by Equation \ref{setpropw} and \ref{setpropw2}.

\begin{equation}\label{setpropw}
	\alpha_j = \frac{\beta_j}{\sum_{i=1}^\kappa \beta_i} 
\end{equation} where the partial weights $\{\beta_i,\dots, \beta_\kappa\}$ are assigned with Equation \ref{setpropw2}

	\begin{equation}\label{setpropw2}
	\beta_j = \frac{dist(\mathbf{D}_t,\mathbf{D}_{v_\kappa}) - dist(\mathbf{D}_t,\mathbf{D}_{v_j})}{dist(\mathbf{D}_t,\mathbf{D}_{v_\kappa}) - dist(\mathbf{D}_t,\mathbf{D}_{v_1})}
	\end{equation} where $\mathbf{D}_{v_1}$ is the closest  and $\mathbf{D}_{v_j}$ is the farthest vector. As a result the closer delayed vector will have more influence for the prediction.



\end{enumerate}


Besides of these methods for the weight assignment is possible to set the weights using other kind of kernels like gaussian, sigmoidal or another kind of nonlinear relation.


\subsection{Qualitative Forecasting}
\label{qualitativeForecasting}
When each element $Y_t$ of the time series take elements $Y_t \in \mathbf{A}$ from any alphabet $\mathbf{A}$, where $\mathbf{A}$ contains symbols used as labels, rather than scalar values, we call the time series qualitative. For example, classification labels for the identification of certain patterns associated with any class. Since these values are not related to any magnitude, the metric to measure the distance must considers only the qualitative differences of the elements of the delay vector $\mathbf{D}_t$. For this purpose, the Hamming distance is a suitable metric to measure the difference between two sequences of labels. The Hamming distance is shown in \ref{hamming}

\begin{equation}\label{hamming}
dist (\mathbf{x},\mathbf{y}) = \frac{\sum_{i=1}^{m}{hamming(x_i,y_i)}}{m}
\end{equation} where $\mathbf{x} = \{x_1,\dots,x_m\}$  and $\mathbf{y} = \{y_1,\dots,y_m\}$ are \textit{label} vectors where $\mathbf{x} , \mathbf{y} \in A^m$ are sequence labels of length $m$ and  the distance is evaluated as presented in Equation \ref{hamming2}

\begin{eqnarray}\label{hamming2}
  hamming (x,y) = \begin{cases}
        0 \text{ if $x=y$}
        \\
        1 \text{ otherwise}.
        \end{cases}
\end{eqnarray}

The estimation of the next $h$ labels is performed by means of the statistical mode $Mo$ (Equation \ref{estimatingModes})

\begin{equation}\label{estimatingModes}
	\hat{Y}_{t+h \Delta n } = Mo (\{Y_{t + h \Delta n}\}_{j=1}^{\kappa})
\end{equation} where $\hat{Y}_{t+h \Delta n}$ is the forecasted value at time t given by the mode of the set $\{Y_{t + h \Delta n}\}_{j=1}^{\kappa}$ produced by the nearest neighbors and $h \Delta n$ takes the prediction horizon values from $1$ to $h$. This scheme is also known as majority vote, studied deeply by Bon Boyer and J. Strother Moore \cite{Boyer1991}. The scheme also has important applications in multi-model classification systems (\cite{Nipkow2011}, \cite{SCM02} \cite{Fre95}).



%where $\hat{Y}_{t+h \Delta n}$ is the forecasted value at time $t$ given by the mode of the set $\{Y_{t + h \Delta n}\}_{j=1}^{\kappa}$ produced by the nearest neighbors and $h \Delta n$ takes the prediction horizon values from $1$ to $h$.




\subsection{Simple Nonlinear Filter}

A filter is a function that replaces a noisy measurement by a new value with less noise, and closer to the real value. In time series, filters are useful for removing noise in order to find a dynamic structure in a system. The Simple Nonlinear Filter presented in the book of Nonlinear time series analysis by H. Kantz \cite{Kantz2004}, is an algorithm for noise reduction of nonlinear time series. This algorithm is based on finding similar pat- terns of any section of the time series. Once similar patterns are found, the average of these components is computed and the studied point is replaced by a new filtered point.

The raw data might contain noisy information in the time series. The recent raw data input is processed in real time and the filter should pro- vide a new value with a reduced amplitude of the noise component. The noise filter tries to identify the data by decomposing it into two components: the signal structure and random fluctuations. This is presented as an additive superposition of the two components in Equation \ref{superposition}


\begin{equation}\label{superposition}
	Y_{t+1} =  s_{t+1}+ \eta_{t+1}
\end{equation} where $s_t$ is the noise-free signal and $\eta_t$ is random noise with null autocorrelations.

In order to recover the signal $s_t$ from the raw (and possibly noisy) signal $Y_{t}$, it is necessary the availability of a function $\hat{f}$ that processes the data 
$\{ Y_{t-m+1},\dots,Y_{t}\}$. The ideal noise filter must satisfies Equation \ref{noiseFilterRequirement}

\begin{equation}\label{noiseFilterRequirement}
	s_{t+1} - f(Y_{t-m+1},\dots,Y_{t}) = 0
\end{equation} which it is an equivalence approximated by replacing the $t$-th value by the middle $m/2$ average value of the nearest neighbors vectors. This function $\hat{f}$ is given by Equation \ref{filter2}

\begin{equation}\label{filter2}
	\hat{s}_{t_0-\lceil m/2 \rceil} =  \frac{1}{|U_\in ({Y}_{t_0})|} \sum_{Y_{t} \in U_\in (Y_{{t}_0})} x_{t-  \lceil m/2 \rceil }
\end{equation} where $\hat{s}_{(t)_0-\lceil m/2 \rceil)}$ is the estimated \textit{noiseless} value and $U_{\in (Y_{{t}_0})}$ is the set representing the neighbors close to $Y_{t_0}$.






\section{Radial Basis Function Artificial Neural Networks}


In machine learning, artificial neural networks (ANN) are computational models inspired in the biological neural networks applied to solve engineering problems as clustering, function modelling, regression and system identification for control \cite{Hoskins}, \cite{MurJoh97}. For the purpose of this thesis we are interested in the prediction capabilities of these kind of models.

A basic ANN is composed by an input layer of same size as the input vector, a hidden layer that weighs the result of applying a certain basis function to each element of the input vector, and the output vector is the weighted sum of the values from the hidden layer. Radial Basis Function Artificial Neural Networks (RBF-ANN) are a kind of artificial neural networks with the characteristic of using Radial Basis Functions in the neurons of its hidden layer \cite{Orr1996}.


The Radial Basis Functions (RBF) are monotonically decreasing or increasing real-valued functions that depend just on the distance between the origin (or center) $\mu$ and another other point $\mathbf{x}$. The center $\mu$, the distance $r$ and the radius or width $ \sigma $  the parameters of a RBF.

Therefore a RBF function $\psi(\mathbf{x}, \mu)$ computes an associated value with the distance $r = ||\mathbf{x} -\mu||^2$ . This value is the maximum when the distance, $r$, is the minimum, and the minimum value when $r$ tends to infinity. This property makes that these kind of functions produce higher response for the values in the neighborhood close to the centre.

Particularly, the RBF for the ANN considered in this work is the Gaussian Radial Basis that is one of the most used RBF functions for general purposes and it is given by Equation \ref{rbf}


\begin{equation}\label{rbf}
\psi_j(\mathbf{x}) = exp \left(-\frac{||\mathbf{x}-\mu_j||^2}{2 \sigma^2_j} \right)
\end{equation} where $\mathbf{x}$ is the input vector, $\mu$ is the center of the function, and $\sigma$ is the width of the exponential function. The mathematical model of the RBF-ANN can be expressed as follows

 %tha its architecture is composed just by three layers; the input, the hidden and output layer, and the weights are optimized using gradient based algorithms assuming that the fixed widths of the radial basis are given a priori for each neuron.
%A basic ANN is composed by an input layer of same size as the input vector, a hidden layer that weight the result of applying a certain basis function to each element of the input vector, and the output vector that is the weighted sum of the values from the hidden layer. Radial Basis Function Artificial Neural Networks (RBF-ANN) is a kind of artificial neural networks with the characteristic of using Radial Basis Functions in the neurons of its hidden layer \cite{Orr1996}.
%The mathematical model of the RBF-ANN can be expressed as follows%applying the radial basis function $\phi$ to each element $X$.

 %(instead of a sigmoidal like activation function).% shown un Figure \ref{RBF}.


\begin{equation}\label{ann}
f_k(\mathbf{x}) = \sum_{j=0}^M w_{kj}\psi_j(\mathbf{x})
\end{equation} where $w_{kj}$ is the weight of the $j$-th neuron and the $k$-th output, $M$ is the total number of hidden neurons, $f_k(\mathbf{x})$ is the $k$-th output layer of the neural network and $\psi$ is a radial basis function. The weights $\mathbf{w}$ in Equation \ref{ann} must be set to minimise Equation \ref{of}

\begin{equation}\label{of}
\underset{\mathbf{w}}{\text{arg min}} \sum_{i=1}^N (\mathbf{\mathbf{y}}_i-\hat{\mathbf{y}}_i)^2
\end{equation} where $\mathbf{w}$ is the set of weights that minimises Equation \ref{of} given the $\sigma$ widths and $\mu$ centers. $n$ is the total number of samples $\mathbf{x}$ of the training set, $\mathbf{\hat{y}_i}$ is the mapping produced by the artificial neural network in Equation \ref{ann} and ${\mathbf{y}_i}$ is the actual output associated to input $\mathbf{\mathbf{x}}_i$.



%The Radial Basis Functions satisfy the condition of being asymptotically decreasing $ \phi(r) \rightarrow 0$ as $r$ increases $|r| \rightarrow \infty$, or in quadratic functions where the function tends to infinite $\phi(r) \rightarrow \infty $ as $r$ goes to infinity $|r| \rightarrow \infty$. Particularly, the RBF-ANN considered in this work is the Gaussian Radial Basis that is one of the most used RBF functions for general purposes and it is given by Equation \ref{rbf}


%\noindent where $x$ is the input vector, $\mu$ is the center of the function, and $\sigma$ is the width of the exponential function.

%\section{K-Nearest Neighbours}


\section{Error Measurement Indicators}

%http://goo.gl/Hxu0mm

To measure the general performance of the error in forecasting models, different indicators are used in the literature. These indicators are useful metrics to compare the accuracy of forecasting models with the aim of avoiding the error cancelation produced by the accumulation of positive and negative errors. This is achieved removing the direction of the error either using the absolute value or squaring the error value. The $i$-th error is an scalar value represented by $e(i)$. Popular error indicators or deviations used to summarize the error in a scalar vector are:


\begin{itemize}
\item \textbf{Mean Absolute Error (MAE)}: Measures the average of the absolute errors of the error set with size $n$ of forecasts. It can be interpreted as the average of all the equally weighted error magnitudes. This is expressed by Equation \ref{SMAE}.

\begin{equation}\label{SMAE}
	MAE = \frac{1}{n} \sum_{1=1}^n | e(i) | 
\end{equation}

\item \textbf{Mean Squared Error (MSE):} It is a quadratic metric that computes the average of the individual square errors. This metric assigns higher weights to large magnitude errors with a quadratic proportion. It is expressed in Equation \ref{MSE}.

\begin{equation}\label{MSE}  
MSE = \frac{1}{n} \sum_{1=1}^n  e(i)^2 
\end{equation}

\item \textbf{Mean absolute percentage error (MAPE):} This accuracy metric normalises the errors respect to the process mean giving the error in terms of percentage. This characteristic is useful to compare different performances in different time series belonging to different order. Since the re-escalation is applied, it makes the indicator a-dimensional. The definition is found in Equation \ref{SMAPE}.

\begin{equation}\label{SMAPE}
MAPE = \frac{100}{n} \sum_{i=1}^{n} |\frac{e(i) }{\mu}| 
\end{equation}  where $\mu$ is the mean of the full set of values.

\item \textbf{Root Mean Square Error (RMSE):} It is a quadratic metric that measures square root of the average squared error. Since each error is squared, weights are assigned quadratically to the errors which they are then summed. The result of the sum is normalised when it is divided by the number of elements $n$. It is useful for comparing vectors of predicted and observed values. This is expressed by Equation \ref{SRMSE}.

\begin{equation}\label{SRMSE}
RMSE =\sqrt{    \frac{\sum_{i=1}^n   e(i)^2 }{n} } 
\end{equation}

\end{itemize} 

The average $RMSE_h$ is a reference index for comparing the performance of the different forecasting algorithms at different prediction horizon values $h$. It is defined by Equation \ref{MRMSE},

%Since in forecasting models often is required to know the performance predicting several steps ahead these indexes are generalised for this end. To measure the accuracy $h$ steps ahead of the forecasting model $n$ is substituted by $h$ in any error indicator formula. The reference index for comparing the performance of the different algorithms for different step aheads $h$, the average $RMSE_h$ is used. It is defined by Equation \ref{MRMSE}.

\begin{equation}\label{MRMSE}
	\overline{RMSE}_h = \frac{1}{n}\sum_{n'=1}^{n} \sqrt{    \frac{\sum_{i=n'}^{n'+h-1}   e(i)^2 }{h} } 
\end{equation} where $n'$ is an auxiliary index that is moved one step ahead and then fixed to move the index $i$, $h$ steps ahead. At each $n'$ is computed the root square of the $h$ next square errors $e(i)^2$ average. Finally the average of the independent vector errors is computed.





%where the individual error $e_h(i)$  where $h=1$ might be the difference of the real observed value and the estimated predicted value $\hat{Y}_i - Y_i $ and when $h \leq 1$ can be used the vector norm $|\hat{Y}-  Y|$. This error indicator is large as soon as $h$ increases. In some cases we want to have comparable errors independently of the prediction horizon size $h$. So for each error $e_k(i)$ in its simple form is proposed 

%\begin{equation}
%e_h(i) = \frac{   \sqrt{ \sum_{j=1}^{h} (\hat{y}_j- y_j )^2 }  }{h}
%\end{equation}



%\subsection{}

\section{Classification and Clustering}


This section addresses the k-means algorithm and the silhouette method used in data mining and machine learning for pattern recognition and identification. These methods are able to identify temporal behavior characteristics useful for local modelling which allow to build a multiple model able to capture and differentiate dynamical modes which present different statistical properties among them.

A clustering algorithm is a method that groups a set of vectors according to a specific similarity criterion (e.g, the euclidean distance, the infinity norm, or hamming distance). These algorithms work with the principle that vectors (or objects) may share characteristics in common so, the objects with similar characteristics are grouped in a class. In this way the clustering simplifies the description of the data diversity of a complex data set. 

\subsection{k-Means Clustering}
%http://stats.stackexchange.com/questions/10540/how-to-interpret-mean-of-silhouette-plot
The k-means algorithm is a data mining clustering method based on minimising the mean distance of each centroid to a set of selected data points according to their distances. The centroids represent different classes of data, and are used as prototypes. The k-means algorithm tries to minimise the objective function described as shown in Equation \ref{kmeans2}
% is the objective function to find those centroids $\mu$ that minimises the mean distance .


\begin{equation}\label{kmeans2}
\underset{ \{ \mu_{i} \}_1^k }{\text{arg min}} \sum_{i=1}^k \sum_{x\in \mathbf{x} }||x-\mu_{i}||^2
\end{equation} where $\mathbf{x}=\{x_1,\dots,x_n\}$ is the set of objects to be classified, $\mu_i$ is the centroid to be optimised, and $k$ is the number of clusters contained in the data set. Solving the k-means optimisation in Equation ({\ref{kmeans2}) is NP-hard but in practice it can be solved efficiently with the 2-step Lloyd's algorithm described by means of equations (\ref{kms1})  and (\ref{kms2}). When the centroids are initialised (random initialisation is an accepted strategy), the next step is to update the next position of the centroids $\mu_i$ using Equation:



\begin{equation}\label{kms1}
 \mathcal{C}_i = \{\mathbf{x}_n: ||\mathbf{x}_n-\mu_i|| \leq \forall ||x_n-\mu_l||\}
\end{equation} where $ \mathcal{C}_i$ is the set of objects belonging to centroid $\mu_i$, defined by the minimum distance given by $||\mathbf{x}_n-\mu_i||$ over all the centroids and the update is computed by Equation %Equation (\ref{kms2})

\begin{equation}\label{kms2}
 \mu_{i} = \frac{1}{|\mathcal{C}_i|} \sum_{x \in \mathcal{C}_i} x
\end{equation} where $\mu_{i}$ is the updated centroid generated by the arithmetic mean of the objects in $\mathcal{C}_i$. In order to select the most suitable number of classes $k$ that fit better the data, auxiliary methods to measure and validate the quality the data separation among classes might be used.
%\section{Nearest Neighbor Rule}
	
%K-nearest neighbors is a simple but powerful learning non parametric algorithms that do not require training. It is useful for clustering and classification  data. Despite of its simplicity it has interesting properties and guarantees that make KNN suitable for the implementation in some situations like the lower bounded error by the Bayes Error Rate. The Bayes error rate gives a statistical lower bound on the error achievable for a given classification problem and associated choice of features. \cite{Tumer2003}. KNN can be used for density estimation and classification.


%KNN is powerful for density estimation since it is non-parametric and it does not make any assumption of the distribution of the data (Guassian distribution ) KNN works analogously to the Parzen window in the sense that uses a kernel hypercube of volume $V$ with a center in $x$. where the density is estimated according to the $k$ elements inside, The more data density , the more elements inside the hypercube has.

%\section{Self-Organizing Maps}
%A self-organizing map (SOM) is a kind of artificial neural network trained in a unsupervised learning way to produce a discretized representation of the input space of set of samples. After the training, each neuron contains the characteristic shape or prototype for each class \cite{Rojas1996} \cite{Kohonen201352}.



%The $SOM$ is trained with a training set of samples $X =\{x_i,\dots,x_m\}$, The Best matched neuron with the input sample $x(t)$ is used as starting point to propagate the information given to the $SOM$ through of the neighbourhood neurons that are updated towards the input sample $x(t)$ by the next learning rule in Equation (\ref{lrule}):


%\begin{equation}\label{lrule}
%w_n(t+1) = w_n(t)+\eta(t)h_{bn}(t)[x(t)-w_n]
%\end{equation} where the component $\eta(t) h_{bn} < 1$ defines the speed that the neuron $w_n$ is updated towards $x(t)$. $\eta(t)$ is the time dependent exponential decreasing learning rate  defined by Equation (\ref{learningRate}). The Figure \ref{fig:learningR2} shows the graphically the how the learning rate is decreasing along the time. \begin{equation}\label{learningRate}. 
%\eta(t)=\exp(-\frac{t}{\tau_n})
%\end{equation} and $h_{bf}$ is the neighbourhood function that propagates the information of the BMU influencing to the neighbour neurons according to their distance defined by the Equation (\ref{hbn}). It is seen as graph which might adopt different configurations as shown in Figure \ref{fig:regularGrid} and \ref{fig:randomGraph}.

%\begin{equation}\label{hbn}
%h_{bn} (t) = \exp (-\frac{r(t)}{2 \cdot r ^2(t)})
%\end{equation} where $r(t)$ is a monotically decreasing function that shrinks the gaussian function $h_{bn}$ according to the Equation (\ref{rt}). This mechanism makes the learning process to reduce the influence among the neighbours in each time-step $t$. Graphically is observed in the Figure \ref{fig:ejemKvecinos1} where the level curves shows that the influence shrinks along the time. 

%\begin{figure}[!htb]
%	\centering
%	  \includegraphics[width=50mm]{figures/GridGrayTones.eps}
%	   \caption{Effect of the Equation \ref{hbn} in the neighbourhood along the time}\label{fig:ejemKvecinos1}
%\end{figure} 






%\begin{equation}\label{rt}
%r(t) = r_0 exp (-\frac{t}{\tau_r})
%\end{equation} where $\tau_r$ is a defined constant that controls the convergence speed. of $r(t)$ and $r_0$ is the initial gaussian width.




%\begin{itemize}
%    \item $w_n(t):$ The Neuron prototype.
%    \item $\eta(t)$: Learning rate decreasing over the time.
%    \item $h_{bn}$:the distance between the best matched neuron and $w_b$
%\end{itemize}




%\begin{figure}[!htb]
%	\centering
%	  \includegraphics[width=50mm]{figures/learningRate.eps}
%	   \caption{Learning rate at different values of $\tau_n$}\label{fig:learningR2}
%\end{figure} 


%\begin{figure}[!htb]
%	\centering
%	  \includegraphics[width=40mm]{figures/regularGrid.eps}
%	   \caption{Regular topological configuration of $h_{bf}$}\label{fig:regularGrid}
%\end{figure} 

%\begin{figure}[!htb]
%	\centering
%	  \includegraphics[width=50mm]{figures/randomgraph.eps}
%	   \caption{Irregular topological configuration of $h_{bf}$}\label{fig:randomGraph}

%\end{figure} 

%Bibliography 

\subsection{Selecting the Best Cluster Partition}



In order to find the best separation of classes after the execution of a clustering algorithm, an index that indicates how well the clusters are separated is used. Depending on the characteristics of the data the clustering algorithm for separating the data in groups with its number of clusters $k$. The parameter $k$ is chosen from a set of possible values $K_{test} =\{1,\dots,k,\dots, k_{max}\}$. Once the clustering algorithm is executed, an indicator of quality $\mathcal{Q}_k$ is associated with the clustered data in $k \in K_{test}$ clusters. The criteria depends on the kind of index, it might be the maximum value of the indicator as expressed by Equation (\ref{q2}). 


% 

\begin{eqnarray}\label{q2}
  \underset{k \in K_{test} }{\text{arg max}} \text{ } \mathcal{Q}_k
\end{eqnarray} Also the minimum value of the indicator as shown in Equation (\ref{q1}) 

\begin{eqnarray}\label{q1}
  \underset{k \in K_{test} }{\text{arg min}} \text{ } \mathcal{Q}_k
\end{eqnarray} or the $K_{test,k}$ that produces the biggest difference respecting $K_{test,k-1}$ as shown in Equation (\ref{q3})




\begin{eqnarray}\label{q3}
  \underset{k \in K_{test} }{\text{arg max}} \text{ } \mathcal{Q}_k - \mathcal{Q}_{k-1}
\end{eqnarray} where $\mathcal{Q}_k$ is defined by Equation (\ref{q4})

\begin{eqnarray}\label{q4}
 \mathcal{Q}_k = \mathcal{Q}(\mathcal{P},k)
\end{eqnarray} where $\mathcal{P}$ is the partitioned data, $k$ is the selected number of clusters in $\mathcal{P}$, and $\mathcal{Q}$ is the function that evaluates the index for $\mathcal{P}$ using any algorithm. In the literature are found a plentiful of quantitative indexes for evaluating the clustering quality. A good survey with a good number clustering criteria is found in \cite{http://goo.gl/WEjjt4}. 



%EXAMPLE HERE


\subsubsection{Silhouette Method}


In clustering analysis, intuitive ideas are adopted for the development of clustering algorithms and indicators for measuring the cluster quality. The concepts of cluster, mass, radius, density, cohesion, and separation are mixed and used for this proposal.


The silhouette method is an algorithm to compute a separability coefficient developed by Peter J. Rousseauw \cite{Rousseeuw1987} inspired by the human intuition of clustering and classification. The silhouette coefficient is reliable in compact and clearly separated clusters. The silhouette method requires the data set partitioned in $k-$ clusters and the set of proximities between all the objects.

To measure the silhouette coefficient we define $\mathrm{s}(i)$ as the dissimilarity distance of object $i$. $\mathrm{A}$ is the cluster to which object $i$ belongs. The average similarity of $i$ with the other $j$ objects in $\mathrm{A}$ is computed and expressed as Equation \ref{disim} and \ref{disim2}.


\begin{equation}\label{disim}
\mathrm{a}(i) = \frac{1}{ |\mathrm{A}|-1 }  \sum_{j \in \mathrm{A} \text{ and } i \neq j} dist(i,j)
\end{equation}



\begin{equation}\label{disim2}
	\mathrm{b}(i) =  \text{minimum } dist(i,\mathrm{B})
\end{equation} where $dist(i,\mathrm{B})$ is the set of all similarity measures between $i$ and objects $j \in \mathrm{B}$. $\mathrm{b}(i)$ is the minimum similarity measure of the nearest cluster $\mathrm{B}\neq \mathrm{A}$. With these definitions we are able to compute the individual silhouette coefficient for each object $i$ using Equation \ref{silhouette}.


\begin{equation}\label{silhouette}
 \mathrm{s}| (i) =  \frac{\mathrm{b}(i)-\mathrm{a}(i)}{max \{\mathrm{a}(i),\mathrm{b}(i)\} }
\end{equation}

Finally the average silhouette coefficient is given by Equation \ref{silhouette2}.


\begin{equation}\label{silhouette2}
\frac{1}{n}\sum_{i=1}^n \mathrm{s} (i)
\end{equation}

In many real cases, there exists the possibility of having not perfectly separated clusters having mixed clusters, in this case the output of Equation \ref{silhouette} might be negative. The meaning of this is that there is an intrusive object inside the set $\mathrm{A}$ belonging actually to another cluster. For that reason $\mathrm{b}(i)$ can be substituted by the mean of all the distances of the objects $i \in \mathrm{A}$ with the objects $j \in \mathrm{B}$.


\begin{equation}
	\mathrm{b}(i) =  \frac{1}{ |\mathrm{B}|-1 }  \sum_{j \in \mathrm{B} \text{ and } i \neq j} dist(i,j)
\end{equation} where $\mathrm{B}$ is the closest cluster to the object $i$ and $dist(i,\mathrm{B})$  is the set of dissimilarity distances between $i$ and the $j$ objects contained in $\mathrm{B}$.


\subsection{Feature Extraction}


Since the proposed Multi-Model Predictor addressed in Chapter 4 is based strongly on the feature extraction for the identification of patterns, this subsection includes a general guide introducing a general classification of different feature extraction algorithms that can be implemented for the identification and modelling of dynamical patterns.
The feature extraction is a kind of data processing to obtain meaningful characteristics from the raw data in order to improve the performance of algorithms oriented to system modelling and identification. This processing is useful to remove redundant data, select local features or enhance a signal. A classification of the feature extraction algorithms is shown next.

\begin{itemize}
\item \textbf{Standardisation and normalisation:} These algorithms translate to a common scale to compare and analyse data from different sources with different scales. In time series it is convenient to scale the data sequence to unique range in order to compare the performance of different forecasting methods applied to different time series with different range. These algorithms are studied widely and applied in machine learning and pattern recognition \cite{SKF08}, \cite{AH01}.
\item \textbf{Signal enhancement:} In the nature regularly the signal is noisy or weak to manipulate. This is because the signal to noise ratio is high. In order to reinforce the signal, it is possible to apply de- noising, smoothing, sharpening or de-trending filters. As example, differentiating is a simple de-trending operation that removes the trend from the time series in order to obtain a stable mean in time series. This is useful when is required to compare the qualitative characteristics of the time series. This algorithms are also found in instrumentation research \cite{ELW08}, \cite{Ras14}.

\item \textbf{Extraction of local features:} Methods for encoding the data and transform the knowledge in features. In time series this consists on finding the proper model for certain sections of the time series with specific characteristics. The local features are modelled by independent models selected following certain rules and conditions. A survey about time series clustering by using local features is found in \cite{War05}.

\item \textbf{Linear and nonlinear space embedding methods:} Since the Multi-Model Predictor framework is based on clustering and regression methods, they may have limitations dealing with high dimensional data. In order to deal with this fact, it is important to discard the irrelevant components that do not contribute with relevant information. Data discrimination is useful for two reasons: The number of optimisation variables might be reduced and the computational performance might be improved once the irrelevant data is not computationally analysed anymore. For example, old past in- formation of time series with negligible influence in the recent dynamics of the system is better to be discarded. Another example in nonlinear time series, is the use of mutual information and false neighbors algorithms \cite{Kantz2004} for detecting and discarding useless information for system modelling. 

%purpose analogously to the correlation and autocorrelation methods for linear times series analysis.% PCA (Principal component analysis) is used often in classical data analysis for data discrimination. 

\item \textbf{Nonlinear expansion:} When the data is too complex, is common to transform the data into a higher dimension. This idea is applied to nonlinear time series, where observing just one variable in a unidimensional space, is possible to reconstruct the trajectory in a higher dimensional space. The theory of machine learning classifiers as ANN and SVM provides the possibility of implementing a set of hidden neurons with nonlinear activation functions to transform or map the input data in a nonlinear space. For the Support Vector Machines the implementation of kernel tricks is useful also for transforming the dimension and shape of the original feature space of the data to another that improves the performance. Examples of applications exploring the nonlinear relationship of the natural data are found in \cite{CLX11} and \cite{She05}.
%[Figura de ejemplo series de tiempo]

\item \textbf{Feature discretisation:} For dealing with computational complexity, some algorithms require to discretise the continuous model or data. For example, for the implementation of a dynamical system for simulation in MPC for being solved by computers or micro controllers since the continuous characterisation of the model is intractable for computers. An important article about discretization of partial differential equations and discretization of continuous attributes as preprocessing step for machine learning are found in \cite{BWZ08} and \cite{CGB96}
\end{itemize}


Here is concluded the theory and the description of the algorithms involved to address the Multi-Model Predictor framework. The Multi-Model Predictor is addressed and developed in Chapter \ref{MMF}.

\chapter{Multi-Model Forecasting}\label{MMF}




Time series can present several pattern behaviours in regular time lapses activated by events generated by natural or social circumstances (e.g. social events, meteorological phenomena, regime of chemical reactions). This is, for example, the case of water or electricity consumption that presents a daily periodicity (the demands present a repetitive pattern every day varying regularly for week-ends and holidays), a weekly periodicity (the demand decreases during the week-ends) and seasonal changes (the demand changes according to the season because of weather, holidays, etc.).

Approaches have been proposed in the literature for forecasting time series that presents several patterns and seasonalities, see e.g., \cite {Alvisi2007} or \cite{Quevedo2010}, where a two-level model is used: a daily flow time series model is combined with a daily 10-minute distribution pattern. The identification of these patterns allows to design local models to forecast specific regimes. Recent related research on identifying behavior regimes in time series can be found in the literature. In \cite{Benmouiza2013}, the implementation of a global NAR neural network predictor is proposed to estimate the regimes associated with local NAR neural network predictor used for forecasting the hourly global solar radiation. In \cite{KumarP10}, an algorithm is proposed for clustering the data and training local predictive models for each set to generate a forecast based on the combination of the local models. In \cite{Alvarez07}, clustering is used to group the days with similar pattern with regard to the variation of working days and holidays.
In fact, time series forecasting presenting multiple patterns is related to the multi-modelling approach of dynamical systems \cite{MurJoh97} used for modelling nonlinear systems under
the assumption that they can be approximated by a finite number of interpolated linear invariant (LTI) models in different operating points (regimes). The identification of those multi-models also involves identifying the different regimes. Once identified, a linear model is estimated for each regime and these local models are combined on-line using weighted sum and a scheduling variable.


This Chapter describes three different implementations of the Multi-Model Forecasting Framework. The Section \ref{generalpredArch} describes the general predictor architecture used by them. The three algorithms are classified in two groups according to their similarity in their preparation and operational phases methodologies:

\begin{enumerate}
	\item Qualitative and Quantitative Multi-Model Predictor \textbf{QMMP}. Which includes:
	\begin{enumerate}
		\item Multi-Model Predictor Based on the Qualitative and Quantitative 				Decomposition of time series using SARIMA and kNN as qualitative estimator 			(\textbf{QMMP+kNN}).
		\item Multi-Model Predictor Based on the Qualitative and Quantitative decomposition of time series using SARIMA with a noise filter using a predefined calendar activity as qualitative estimator(\textbf{QMMP+NF}).
	\end{enumerate} These algorithms are based on the qualitative and quantitative decomposition which are inspired from the SARIMA pattern algorithm proposed by Quevedo in \cite{Quevedo2010} (\textbf{QMMP}).
	
	\item ANN Multi-Model Predictor 
	
	
	
	\begin{enumerate}
		\item Multi-Model Forecasting Using Radial Basis Function Neural Networks with an On-line Mode Recognition (\textbf{RBFMMP+OR}).
	\end{enumerate} Which consists in a framework for training machine learning Multi-Model predictors. 
\end{enumerate}


 The Multi-Model Forecasting \textbf{RBFMMP+OR} algorithm is described in Section \ref{mmfrbf}. A particular case is addressed using a collection of algorithms that allows the multi-model prediction, such as k-means for pattern classification, RBF-ANN for the Multi-Model Predictor, and a mode recognition mechanism based on the nearest neighbor rule.



In Section \ref{qqmm} are described the two Multi-Model Predictors based on the decomposition of the quantitative and qualitative decomposition of time series \textbf{QMMP}. The first configuration \textbf{QMMP+kNN} implements two parallel forecasters where the qualitative predictor assumes the lack of future information to activate the most suitable operation mode. For this implementation, the decisions are based completely on the history of the operation modes sequence. The second implementation, assumes the existence of an activity calendar with the definition of the sequence of working and resting days related to the forecasting operation modes. This scheme is extended with a nonlinear filter module for modelling and predicting the mismatches to improve the prediction accuracy. Both algorithms use a low order SARIMA model to forecast the quantitative (aggregated) component of the time series.




\section{General Predictor Architecture}\label{generalpredArch}

%%repetitivo
The proposed general architecture is designed, as we mentioned before, to be used as general modelling methodology applicable to time series and also to systems presenting multiple behaviours. The proposed framework provides a methodology for: 

 \begin{itemize}
   \item The preparation phase consisting of the off-line training of the \textbf{Multi-Model Predictor (MMP)} and
   \item Execution phase that is the on-line implementation of the \textbf{Multi-Model Predictor (MMP)}.
 \end{itemize}


The Multi-Model Predictor \textbf{MMP} has an architecture composed of interconnected modules involved in the mode recognition or prediction, and activation of the suitable model predictor. This architecture is described in Figure \ref{gs}. It has three main modules: \textbf{Feature Extraction}, \textbf{Mode Recognition} and the \textbf{Multi Forecasting Model}.

The function provided by the \textbf{Feature Extraction} module allows to transform the data to extract qualitative information from the time series \cite{guyon2013}. This is done by transforming the time series input into suitable qualitative representation extracting detrended unitary patterns for being used by the \textbf{Mode Recognition}, and into quantitative representation for being used by the Multi Forecasting Model.
The \textbf{Mode Recognition} module estimates the next pattern to be activated using the processed information given by the \textbf{Feature Extraction}, and the \textbf{Multi-Model} module contains a set of independent forecasting models associated with each mode.

% and that predicts the aggregated magnitudes distributed along the selected unitary pattern performing the $h$ step ahead forecast.

 \begin{figure}[h!]
				\centering
				\includegraphics[trim= 18mm 20mm 14mm 18mm, clip, width=80mm]{figures/generalFramework.pdf}
				\caption{General structure of the Multi-Model Predictor}
				\label{gs}
\end{figure}




%\section{Multi Model Forecasting Using Radial Basis Function Neural Networks with an On-line Mode Recognition (\textbf{MMP+MR})}
\section{Multi-Model Forecasting Using RBF-ANN with an On-line Mode Recognition}\label{mmfrbf}


%This paper proposes a general a Multiple-Model Predictor framework for forecasting time series that presents multiple patterns (behaviours). 

The proposed algorithm based on machine learning algorithms allows the off-line identification and training of the different dynamic behaviours presented in time series, and the on-line model recognition and activation according to the detected pattern. A particular implementation based on k-means and Radial Basis Functions (RBF) is proposed to exemplify a possible implementation oriented to the short term water demand forecast using demand data from the Barcelona drinking water network.


%The \textbf{MMP+MF}: First, in Section 2, the Multiple- Model Predictor framework for forecasting time series that presents multiple patterns (behaviours) is introduced. In Section 3, a particular implementation based on K-means and RBF neural networks is proposed. Section 4 describes the on-line mode recognition mode of the MMP approach. Section 5 presents the application of the MMP approach to short-term demand forecast in drinking water networks. Finally, Section 6 conclusions are drawn.


The Multi-Model Predictor \textbf{MMP} consists in a sequence of steps that involves the training of a machine learning model using classified patterns of the time series. This process is described by means of the block diagram of the Figure \ref{mmtp}. The raw data might be required to be pre-processed to enhance the mode and cluster identifiability of the patterns considering the \textbf{Feature Extraction} as an optional component.

 \begin{figure}[h!]
				\centering
				\includegraphics[width=110mm]{figures/mmprocess.eps}
				\caption{Multi-Model training process}
				\label{mmtp}
\end{figure}

The steps involved in the off-line \textbf{MMP} training are:

\begin{enumerate}
\item \textbf{Decomposition}: The training set is organised in samples $S$ of the time series given by the \textbf{Split} Module. If needed, the output of this module is transformed to another feature space by the means of \textbf{Feature Extraction} module responsible of mapping the vectors $S$ into vector $S'$ with Equation \ref{map2}

\begin{equation}\label{map2}
\mathds{\psi}:S \mapsto S'
\end{equation}

\item \textbf{Classification}: This step includes the \textbf{Pattern Classifier} module for classifying the split time series $S$ (if the feature spaces is transformed, then the classification is performed with the transformed vectors $S'$) extracted by the \textbf{Feature Extraction} module in order to generate labeled samples $LS$ that include the belonging class. The classified labeled samples $LS$ are formatted by the \textbf{Data Formatting} module that generates training sample vectors $TS$ that contain input-output training information with the associated class attached.

\item \textbf{Training:} In this step, a machine learning model is trained using the input-output samples $TS$.
\end{enumerate}



%A particular case based on k-means and \textbf{RBF-ANN} is proposed to exemplify a possible implementation. 

%To illustrate the use and performance of the proposed approach, an application of the short-term water demand forecast is presented using demand data from the Barcelona drinking water network.

%A particular implementation based on K-means and Radial Basis Functions (RBF) is proposed to exemplify a possible implementation oriented  to the short-term water demand forecast using demand data from the Barcelona drinking water network.


Since the proposed framework might be seen as an open modular model, there exists a large collection of machine learning and pattern recognition algorithms in the literature that can be used for the implementation of the proposed \textbf{MMP}. However only a particular implementation using standard and well known algorithms used in machine learning literature, is addressed with the objective of illustrating the improvement of the prediction accuracy over the traditional methodologies \cite{smola2008ml} - \cite{Wu2008}. Therefore the following algorithms are used:

\begin{itemize}
  \item k-means clustering algorithm for the \textbf{Pattern Classifier} module,
  \item RBF neural networks for the \textbf{Multi-Model} predictor module, and
  \item Discrete differentiation of time series for the \textbf{Feature Extraction} module.
\end{itemize}


%The \textbf{Feature Extraction} module is based on differentiating the time series.
  The implementation of the \textbf{Split Time Series}, \textbf{Data Formatting} and \textbf{Mode Recognition} modules are addressed in the next section as part of the training step.




\subsection{Discrete Derivative as a Feature Extraction Method}

The \textbf{Feature Extraction} module performs the task of transforming raw data into a set of useful features \cite{guyon2013}. The function of the feature extraction is to improve the classification performance of the data. The discrete derivative (or differentiation) of the time series is implemented as feature extraction method described by


\begin{equation}\label{derivative}
Y'_t = Y_t - Y_{t-1}
\end{equation} where $Y'_t$ is given by the difference of the data at time $t$ at lag $t-1$.

The effect of this transformation consists in removing the mean and trend taking the relative increments of the time series used for the mode identification \cite{Qi2008}. 	



\begin{centering}
%[DIBUJO AQUI]
\end{centering}


\subsection{On-line Mode Recognition for the Multi-Model Predictor Approach} \label{sec:recognition}


The proposed methodology for building the Multi-Model Predictor is designed to use machine learning regression methods that learn a model that associates the input with the output vectors from a subset of samples \cite{Dasgupta2011}. In order to build the Multi-Model Predictor, the specific algorithms for the \textbf{MMP} training and On-line Mode Recognition with the Multi-Model Predictor (\textbf{RBFMMP+OR}) implementation are described next.


%pattern identification, time series decomposition and multi-model training.


%\subsection{Decomposition(\textit{Rodrigo})}

\subsubsection{Preparation Phase}

In the preparation phase is performed the training of the \textbf{Multi-Model} predictor. This phase involves the \textbf{Decomposition}, \textbf{Identification} and \textbf{Training} steps performed off-line. The implementation of each step involved is described next.



\subsubsection{\textit{Decomposition:}} This step decomposes the time series for the identification of the different behavior patterns. The decomposition starts assuming the availability of an univariate time series described by the Equation \ref{ts}

%\begin{equation}\label{ts}
%\mathbf{Y}= \{Y_1,\dots, Y_N\}
%\end{equation} where 

\begin{equation}\label{ts}
\mathbf{Y}=\{Y_1,\dots ,Y_i,\dots, Y_n\}
\end{equation} where $Y_1$, $Y_i$ and $Y_n$ are the first, the $i$-th and last element of the time series respectively. The time series $\mathbf{Y}$ is decomposed in samples $\mathbf{S}$ (Equation \ref{sects})

\begin{equation}\label{sects}
\mathbf{S} = \{ S_1,\dots,S_i,\dots,S_{N'}  \}
\end{equation} where  $N'$ is the total number of samples defined by $\lfloor \frac{n-h-m+1}{\tau}\rfloor$ and $S_i$ is the sample obtained splitting the original time series $\mathbf{Y}$ according to Equation \ref{splitting}  % samples according to the split point that indicates the the number of subsamples belonging to a pattern, number of observations $o$, and prediction horizon $h$. Number of observations $o$ and and the prediction horizon $h$ define the structure of the RBF.

\begin{equation}\label{spliting}
S_i= \{Y_{(\tau \cdot i - m + 1)},\dots,Y_{\tau \cdot i },\dots ,Y_{ \tau \cdot i+h } \}
%\mathbf{S_i}= \{Y_{(p \cdot i - o)},\dots,Y_{(p\cdot i )},\dots ,Y_{\{p\cdot (i + 2 \cdot h - 1 ) \}} \}
\end{equation} where $m$ is the number of observations, $h$ is the desired prediction horizon, and $\tau$ is the period parameter that defines a split point that is applied every $\tau$ number of displacements. The $\tau$ displacements are performed by a sliding window of size $m+h$ inside each sample $S_i$ in order to construct the training samples \textit{TS}.  A graphical description of the data organisation used for the construction of the training set is shown in the Figure \ref{splitting}, where $S_1$ and $S_2$ are samples of size $m+h+\tau-1$ that allows $\tau$ displacements of the sliding window inside each sample.


\begin{figure}[h!]
				\centering
				\includegraphics[trim= 0mm 80mm 5mm 83mm, clip, width=70mm]{figures/splitTS2.pdf}
				\caption{Time Series processing data}
				\label{splitting}
\end{figure}

\subsubsection{\textit{Identification and training:}} The objective of the identification analysis is to detect the number of different modes in the samples $\mathbf{S}$. The samples $\mathbf{S}$ are classified into a suitable number of classes $k$ using a clustering algorithm to group them according to their similarity. Then a local forecast model is trained with classified data for each specific mode. In order to choose a number of clusters $k$, the classification is validated by maximizing the mean silhouette coefficient shown in Equation \ref{silhouette}. When the \textbf{Feature Extraction} module is implemented, the \textbf{Pattern Classifier} uses the samples $S'$ that are transformed in the same feature space to classify $S$. After the classification of $S$, labeled samples $LS_i =  \{ C_i,S_i \}$ are obtained where $C_i \in K$ are the possible labels of $C_i$ with $K=\{1,\dots,k\}$. Then, the class information is incorporated using extended input vectors that include the $m$ last observations concatenated with the codified class information. The procedure for generating training samples $\mathbf{TS}$ is implemented in the \textbf{Data Formatting} module. To generate the training samples $\mathbf{TS}$, a sliding window of size $m+h$ is used to obtain from the labeled samples $\mathbf{LS}$ the training samples $\mathbf{TS}=\{ \{ \mathbf{C}' , \mathbf{Input} \},  \mathbf{y} \}$ where $C'_i = \{\mathbb{I}_{C_i,1}, \dots ,\mathbb{I}_{C_i,k} \}$ is the codification of the class in $c_i$ using the $C_i-th$ row of the identity matrix $\mathbb{I}_{k \times k}$ as shown in:

\begin{equation}\label{idm}
	\begin{bmatrix}
       k_1 \rightarrow k'_1            \\[0.3em]
       \vdots \\[0.3em]
       k_{k} \rightarrow k'_{k} 
  	\end{bmatrix}
		=
	\begin{bmatrix}
       1 &  \dots & 0         \\[0.3em]

       \vdots & \ddots           & \vdots \\[0.3em]
       0           & \dots & 1
     \end{bmatrix}_{k,k}
     =
	\mathbb{I}_{k,k}
\end{equation}


The vector  $ Input_i = \{ S_{i,j}, \dots , S_{i,j+m-1} \} $ is a sequence of $m$ inputs and  $\mathbf{y}_i = \{S_{i,j+m},\dots, S_{i,j+m+h} \}$ is the output or target vector of size $h$ where $j = \{1,\dots,\tau\}$ is the index that moves the sliding window $\tau$ times composing the training set by extending the each input vector to $\mathbf{x_i} = \{C_i, Input_i \}$ obtaining a set of training samples denoted by $\mathbf{TS} = \{\mathbf{x}_i, \mathbf{y}_i \}_i^{N'}$.


\subsubsection{Training:} The machine learning method is trained using the training samples $\textbf{TS}$ to optimize the objective function

\begin{equation}\label{of2}
\underset{\mathbf{w \text{ }}}{\text{minimise}} \sum_{i=1}^{N'} (f(\mathbf{x}_i)-\mathbf{y}_i)^2
\end{equation} where $\mathbf{w}$ is the set of weights that minimises the sum of the squared errors between the estimated output by the neural network denoted by $f(\mathbf{x_i})$ and the real output vectors $\mathbf{y_i}$.

%\sigma \text{ } \mu
%\subsubsection{Operational Phase: Implementation of the Multi-Model Predictor Pattern Estimation }
\subsubsection{Operational Phase}

The operational phase consists in the implementation executing the Multi-Model Predictor.  After training the RBF-ANN, arises the challenge of designing a mechanism for the \textbf{Mode Recognition} module to estimate and discover the current operation mode. For this module a mode discovery based on the nearest neighbor rule using a variable queue of observations is proposed. The idea is based on the exploitation of the current and limited historical information to estimate the belonging class of the observed measurements using the Euclidean distance. The implementation consists in a queue $\mathcal{W'}$ of variable size that stores from $m$ to $m+\tau-1$ measurements shown in Equation \ref{stackZ}
%After detecting and defining the classes in the time series the problem of mode Discovery arises. The mode discovery is a mechanism that should be able to discover, detects, or predict the operation mode for prediction.


\begin{eqnarray}\label{stackZ}
 \begin{bmatrix}
       \mathcal{W}_{1} &   \\[0.3em]
       \mathcal{W}_{2} &   \\[0.3em]
       \vdots &  \\[0.3em]
        \mathcal{W}_{m+mod(t,\tau)}           &
     \end{bmatrix} = \begin{bmatrix}
       Y_{t-(m+mod(t,\tau))+1} &         \\[0.3em]
       Y_{t-(m+mod(t,\tau))+2} &         \\[0.3em]
       \vdots &  \\[0.3em]
       Y_t
     \end{bmatrix}
\end{eqnarray} where $\mathcal{W}_1$ is the first element of the queue that stores a delayed measurement $Y_{t-(m+mod(t,\tau))+1}$, and $\mathcal{W}_{m+mod(t,\tau)}$ stores the current measurement $Y_t$. Notice that $\mathcal{W}_1$ is realigned each period defined by $\tau$. In order to simplify the notation, we define $\{ \mathcal{W'} \}^{sup(t)}$ and the prototype vectors $\{ \mathbf{P}_k \}^{sup(t)}$

\begin{equation}\label{Z}
\{ \mathcal{W}' \}^{sup(t)} = \{\mathcal{W}_1, \dots, \mathcal{W}_{sup(t)} \}, \end{equation}

\begin{equation}\label{P}
\{ \mathbf{P}_k \}^{sup(t)} = \{P_1^k, \dots, P_{sup(t)}^k \},
\end{equation}  where  $sup(t)=m+mod(t,\tau)$. The estimation of the belonging class of the current observations in time $t$ stored in $\left. \mathcal{W'} \right|^{sup(t)} $ is performed using the nearest neighbor rule

 \begin{eqnarray}\label{euclid}
  \underset{k \in K }{\text{arg min}}  \left| \left| \{ \mathbf{P}_k \}^{sup(t)} - \{ \mathbf{Z} \}^{sup(t)}   \right|\right|
\end{eqnarray} where the class selected is the argument $k$ that minimises the distance between the prototype $\{ \mathbf{P_k} \}^{sup(t)} $ and the current measurement stored in $\{ \mathcal{W'}  \}^{sup(t)} $. %The euclidean distance to estimate the most similar class to the the observations


 %The implementation is described by the next Algorithm (\ref{stack}). It is important to mention that depending of the data, the use of any filtered data for performing the pattern estimation might reduce the false neighbors increasing the accuracy in the forecasting.


%\begin{algorithmic}[1]
%\STATE $stack = \{\}$
%\FOR  {$i=\{1,\dots,L-m\}$}
%\STATE $stack = \{stack, Y_i\}$
%\IF { $ i \ge  m $ }
%\IF { $ |stack| >  m \text{ and } (\text{mod}(size(stack)-m+1  ,m) = 0 )$}
%\STATE $ stack =
%This thesis has contributed with a general multi-model predictor framework defined by modules, and a methodology for training and implementing multi-model forecasting based on an on-line mode recognition that works with time series that present periodic patterns. The proposed approach shows the effectiveness using classified data for training a \textbf{Multi-Model} predictor together with a \textbf{Mode Recognition} that outperforms the traditional \textbf{RBF-ANN} implementation and other popular and well documented statistical methodologies as SARIMA and Double Seasonal Holt-Winters.



\section{Multi-Model Predictor Based on Qualitative and Quantitative Decomposition}\label{qqmm}

In this section two implementations of the Multi-Model Predictor are addressed exploiting the decomposition of the time series in its qualitative and quantitative components; a Multi-Model Predictor with qualitative forecasting assuming no knowledge about the future (E.g, the lack of a calendar activity), and another implementation assuming the calendar of activity that provides the forecasting modes but including a nonlinear filter for detecting mismatches in the qualitative pattern prediction. The architecture of the Multi-Model Predictor is described previously in Figure \ref{gs}. In both implementations are included:

\begin{itemize}
  \item  A seasonal ARIMA model that predicts the quantitative component using daily aggregated magnitudes.
  \item A mode estimator that can be implemented using different strategies depending on the knowledge data assumption. One assumption is that the sequence of the modes are not known a priori and the other when the operation modes are a priori known as a good approximation of the global behavior, for example, the annual activity calendar of working and resting days governing the behavior of the urban population. The modelling study of such information belongs to the global modelling.

  \item A k-means clustering algorithm for finding the patterns modes.
\end{itemize}



To perform the training and tuning parameters of the different algorithms involved, the raw time series described by Equation \ref{ts}, is decomposed in its qualitative and quantitative components. The quantitative time series is obtained by Equation \ref{quant}



\begin{equation}\label{quant}
Z_i=  \sum_{j=((i-1)\tau+1)}^{i \cdot \tau} Y_j
\end{equation} where $\tau$ indicates the seasonality while the qualitative (or normalised) time series is obtained by Equation \ref{qual}:


\begin{equation}\label{qual}
X_i=\frac{ \{   Y_{(  (i-1)\tau+1)},\dots, Y_{i*\tau}  \}    }{Z_i}
\end{equation} where $\mathbf{X} = \{X_1,\dots,X_i,\dots,X_{N}\}$ is the time series containing the adimensional qualitative unitary patterns. The qualitative patterns  are classified and associated with a class $K \in \{ 1, \dots , k \}$. The associated classes are contained in $\mathbf{C}$ defined by Equation \ref{kmeans}


%where in each time period $\tau$, its magnitude is removed obtaining the qualitative time series that is classified with the k-means classifier.


\begin{equation}\label{kmeans} 
	C_T = \underset{  i \in K }{\text{arg min}} ||X_T-P_{i}||
\end{equation} where $P_i$ is the distribution pattern or prototype, given by the mean of the objects contained in each class, and $k$ is the number of classes. Each object $C_T \in K$ contains a label from $K$. The new time series with the sequence of labels $C_T$ is described by Equation \ref{labels}.

\begin{equation}\label{labels}
	\mathbf{C} = \{C_1,\dots, C_{T},\dots,C_{N}\}
\end{equation} 


Time series prediction models for ${\mathbf{Z}}$ and $\mathbf{C}$ are modelled independently. On one hand, the estimation of the next value $\hat{{Z}}_{T+1} $ might be performed by a low order Seasonal ARIMA, and on the other hand, $\hat{{C}}_{T+1} $ can be estimated using two main general approaches based on the certainty assumption of the data. The first one is using the a priori knowledge of a human based calendar assuming that the patterns approximate the activity calendar of working and resting days. The second approach consists in using a general method for determining the next pattern automatically without any assumption. For this latter approach, methods based on the knowledge representation as kNN, Bayesian Networks, Markov Chains can be used.

The final forecasting is produced by the composition of the predicted qualitative and quantitative components $ \mathbb{Z}_T = \{ {\hat{Z}}_{T+1},\dots,{\hat{Z}}_{T+H}\}$ and   $\mathbb{P}_T = \{ {P}_{\hat{C}_{T+1}},\dots,{{P}}_{\hat{C}_{T+H}} \} $, shown in Equation (\ref{prediction}).

\begin{eqnarray}\label{prediction}
	{\hat{Y}}_{ (T\tau)+1}, \dots, {\hat{Y}}_{(T+1) \tau } & = & {\hat{Z}}_{T+1}  \cdot {{P}}_{\hat{C}_{T+1}}\\ \nonumber
	& \vdots & \\ \nonumber
	{\hat{Y}}_{(T+H-1)\tau +1 } , \dots,{\hat{Y}}_{(T+H) \tau } & = & {\hat{Z}}_{T+H}  \cdot {{P}}_{\hat{C}_{T+H}} \\ \nonumber
\end{eqnarray}where $\tau$ is the length of the period, $H$ and $T$ are the prediction horizon and current time respectively in a $\tau$ period basis.



%In order to address this problem multiple models are used. In the literature there are several works related to the decomposition of time series identifying different nominal behaviors to construct a hybrid methodology that considers the use of a piecewise modelling of time series to model the nonlinearity. The article of R. Murray-Smith in \cite{MurJoh97} addresses the multiple modelling approach.




\subsection{Multi-Model Predictor Based on the Qualitative and Quantitative Decomposition of Time Series using SARIMA and kNN}

The implementation of this Multi-Model Predictor with a Mode Prediction without any information about the future of the prediction mode, uses a qualitative forecasting model based on kNN. The scheme is described in Figure \ref{qqscheme}.
\begin{figure}[h!]
				\centering
				\includegraphics[trim= 0mm 16mm 15mm 13mm, clip, width=105mm]{figures/generalFrameworkMMFixed.pdf}
				\caption{Multi-model training process}
				\label{qqscheme}
\end{figure}

According to Figure \ref{qqscheme}, the observed data so far $\{Y_1,\dots,Y_{floor(t/\tau)\tau}\}$ is received and processed by the \textbf{Qualitative Feature Extraction} module delivering the extracted qualitative information to the \textbf{Mode Recognition} module. On the other hand, the subset of observations

\begin{equation}
Y_{floor[(t-\tau*\acute{\alpha} + 1)/ \tau] \tau}, \dots, Y_{floor(t/\tau) \tau},
\end{equation}

\noindent (where $\alpha$ constraints the last days to be considered) is processed by the \textbf{Quantitative Feature Extraction} module related directly with the seasonality value used by SARIMA forecasting model. The SARIMA forecasting model is included in the  \textbf{Multi-Model} module. The implementation is composed by the preparation and operational phases using data organised in training and validation sets respectively. The procedures of both phases are described next.

%The implementation of the algorithm is divided in preparation and operational phase 



%%%%

%\subsection{Multi-Model Predictor based on the Qualitative and Quantitative Decomposition using Knn}

\subsubsection{Preparation Phase}

For the preparation phase, the parameters of the qualitative kNN predictor and SARIMA model are tuned using the training data set. The parameters $\varepsilon$ and delays $m$ of the qualitative kNN forecasting are found in Equation \ref{hamming}, \ref{hamming2} and \ref{estimatingModes} in Subsection \ref{qualitativeForecasting}.

%The algorithm is found in subsection \ref{sssec:qualitativeForecasting}. The Seasonal ARIMA predictor should be tuned using the training data set.  $\epsilon$ and the delays $m$ for the KNN, and the parameters of the autoregressive (AR) and (MA) parts for the ARIMA model. 

The correlation analysis is required to identify the SARIMA model, and then optimise its parameters with any gradient based parameter estimation algorithm. Once the SARIMA model is obtained. The kNN parameters are adjusted executing exhaustive search bounding the search space for the parameters $\varepsilon$ and $m$ with the objective of minimising the $\overline{RMSE}_h$ errors as follows:

%\ref{modes}
%\begin{figure}[h!]
%				\centering
%				\includegraphics[width=40mm]{figures/modes.eps}
%				\caption{Multi-model Training Process}
%				\label{modes}
%\end{figure}


\begin{equation}\label{kNNobjectiveFunction}
\begin{aligned}
& \underset{m, \varepsilon}{\text{arg min}}
  \sum_{T=1}^{N}  || \hat{\mathbf{Y}}_{T \tau +1}  - \mathbf{Y}_{T \tau +1} || \\
%& \text{subject to} \\    & \epsilon, m &
\end{aligned}
\end{equation} where $\hat{\mathbf{Y}}_{T \tau+1}$ is the predicted value given by the composition of the qualitative and quantitative forecasting for the next days expressed by





%$$
% \sum_{N=1}^{N = Tr.Samples}|| \hat{\mathbf{Y}}_\mathbf{N}  - \mathbf{Y}_\mathbf{N} ||
%$$


\begin{equation}\label{predictedData}
\hat{\mathbf{Y}}_{T \tau +1} = \{  \hat{Z}_{T+1} P_{\hat{C}_{T+1}}, \dots, \hat{Z}_{T+h} P_{\hat{C}_{T+H}} \}
\end{equation} 

\noindent and  $\mathbf{Y}_{T \tau +1}$ is the real hourly information (in our application, water consumption) defined by


\begin{equation}\label{realData}
{\mathbf{Y}}_{T \tau+1} = \{Y_{ T\tau+1},\dots,Y_{T \tau  \cdot  H}  \}
\end{equation}

\noindent where $H$ is the $H$-th predicted vector with $\tau$ elements. The Figure \ref{predictionSample} shows how the prediction at the smallest basis (hours) is performed along the period (day), using a sliding window of size $h$ covering the prediction subsequence data from $(\tau (T-1)+1)$ to $(\tau T \cdot  h)$ in $\tau$ steps. 


\subsubsection{Operational Phase}


%    0.7881

In the operational phase, the Multi-Model Predictor architecture presented in the Figure \ref{qqscheme} is implemented. In real-time, the model collects the raw measurements
as input for transforming the data to allow the extraction and classification of the qualitative patterns performed by the \textbf{Qualitative Feature Extractions} and the aggregated data by the \textbf{Quantitative Feature Extraction} every $\tau$ steps. The sequence of labels $\mathbf{C}$ (Equation \ref{labels}) are processed by the \textbf{Mode Detection} Module to estimate with Hamming distance based kNN, the next pattern to be considered and used by the \textbf{Multi-Model} Module.


For the operational control of water distribution networks, it is necessary to have available 24 hour ahead predictions not only in each period $\tau$, but in an hourly basis. For expanding the capability of having the prediction information, kNN estimates 2 modes ahead, and SARIMA predicts two aggregated consumption days described as the Figure \ref{predictionSample}. % obtaining the prediction 
%$\hat{Y},\dots,\hat{Y}_{N+1}$
The prediction at any time $t$ is provided by a sliding  window over $\mathbf{\hat{Y}}_{t+1}$ shown in Figure \ref{predictionSample} with a desired prediction horizon $h$.
% over the prediction $\{ Y_{N} ,\dots, Y_{N+1} \}$.


\begin{figure}\
				\centering
				\includegraphics[trim= 30mm 90mm 24mm 20mm, clip, width=70mm]{figures/predSampleEditedCopy.pdf}
				\caption{Prediction sample}
				\label{predictionSample}
\end{figure}

%\subsection{Multi-Model Predictor based on the Qualitative and Quantitative Decomposition using $K$nn}


%\section{Multi-Modelling forecasting using Machine Learning and an on-line mode Recognition}

%\section{Conclusions}

\subsection{Multi-Model Predictor Based on the Qualitative and Quantitative Decomposition of Time Series using SARIMA and Noise Filter}


This is another implementation of the Multi-Model Predictor using a low order SARIMA and an human activity calendar based on the information given by a human expert with qualitative sequence definition of resting and working days.

The contribution of this algorithm is the introduction of a noise filtering module to address the problem of filtering the residuals or (mismatches) generated by the Multi-Model Forecasting (\textbf{MMP}) proposed by J. Blanch, J. Quevedo, and V. Puig in Short term Demand Forecasting for Operational Control \cite{quevedo2014} and generalised in the work of R. Lopez, Puig and Rodriguez \cite{lopez15}. The objective of filtering the residuals is to discover the structure that improves the accuracy of the 24 hours ahead water demand forecasting for operational control. The algorithm uses a simple nonlinear filter to separate the noise from the structure of the residuals. The extracted information is useful for correcting the prediction in the next time instant.


%Usually predictors, filters, and smoothers are used for state recovery. Sometimes this is performed assuming the existence of a dynamical model (e.g, the Kalman filter for the on-line estimation and reconstruction of internal or unobserved states of linear dynamical systems from the observable states \cite{Welch2006}) or as a pre-processing method to clean the data and identify easily the information of interest. Some methods related to the pre-processing approach use data transformations as filters as the Discrete Fourier transform and its inverse transform \cite{Meng2002}, wavelet based filtering \cite{Joo20153868} or filling the missing values according to a smooth function as a spline \cite{SUAREZ-FARINAS2004}. 

In linear systems, the residuals of a good model fit a normal distribution with zero mean and constant variance along time. Real time series, e.g., those extracted from nature, might be influenced by external dynamics, changing gradually the behavior and characteristics of the system. Under those conditions we need to readjust the parameters once the prediction residual structures or the increasing of the error prediction are noticed, or even worst, use a new model to fit the new behavior properly. For cases where the systems present \textit{sporadic} unmodelled dynamics, the mismatch must be processed and predicted to improve the accuracy or the forecast.



\subsubsection{Implementation}

The \textbf{Filter} module is an extension of the the \textbf{Multi-Model Predictor} architecture. We call it \textbf{Qualitative Multi-Model Predictor}  with \textbf{Noise Filter}  (\textbf{QMMP+NF}). The \textbf{Filter} module provides a noise filtering mechanism to improve the forecasting accuracy and it is included in Figure \ref{gsfilter}. 

%In order to clarify how the Filter is implemented in the  \textbf{MMP+NF}, 




\begin{figure*}[!ht]
  \centering
  \includegraphics[trim= 10mm 10mm 12mm 13mm, clip, width=110mm]{figures/generalFrameworkMMwithFilterBiggerFixed.pdf}
  \caption{Multi-Model Predictor with the Filter Module}
  \label{gsfilter}
\end{figure*} 

The \textbf{Multi-Model} module is seen as separated \textbf{Qualitative Multi-Model Forecaster} and \textbf{Quantitative Forecaster} modules. The implementation of the proposed methodology is divided also in the preparation and operational phase. In the preparation phase is performed the separation of the data in training, validation and test sets and the implementation of all the modules. In the operational phase is performed the prediction mechanism for having the forecasting available for each hour along the day. The description of the two phases is explained in the next subsections.

\subsubsection{Preparation Phase}

The description of the modules of the \textbf{QMMP+NF} are:




\begin{itemize}
\item \textbf{Feature Extraction:} It performs the decomposition of the time series in its qualitative and quantitative components $\mathbf{X}$ and $\mathbf{Z}$  by using the equations (\ref{quant}) and (\ref{qual}).
\item \textbf{Mode Recognition:} This module is based completely on the knowledge of the human activity. It is represented by the function $\hat{F}_{cal}$ that is the two class calendar function defined a priori according to the working and working and resting days, so the next mode is predefined by the calendar by Equation (\ref{calendar}). \begin{equation}\label{calendar}
\hat{C}_{T+1} = \hat{F}_{cal}(C_1,\dots,C_i,\dots,C_T)
\end{equation} where each element $C_i \in [1,...,k]$ contains the class label associated with the pattern mode ${P}$. The validation of the calendar activity presented in the time series is performed by k-means and the silhouette coefficient. 
%The validation of the calendar activity presented in the time series is performed by k-means and the silhuette coefficient.

\item \textbf{Qualitative Multi-Model Forecaster:} This module receives the next mode selected by the strategy implemented in the \textbf{Mode Recognition} module. It contains the different patterns associated with different classes.


\item \textbf{Quantitative Forecaster:} It includes a SARIMA model using the quantitative time series $\mathbf{Z}$. It is tuned with the training set and validated with the validation set.


\item \textbf{Filter:} This module is tuned once the previous modules are defined and implemented. The \textbf{Filter} collects the residual vectors $\mathbf{R}$ produced by the difference of predicted qualitative component ${{P}}_{\hat{C}_{T+1}}$ and the real qualitative component in $X_{T+1}$. The Equation (\ref{residuals}) produces the residuals. 

\begin{equation}\label{residuals}
R_{T+1} = {P}_{\hat{C}_{T+1}} - X_{T+1}
\end{equation}



\end{itemize}



Filtering is performed on current time $T$, $H$ steps ahead. It uses the residuals stored in $\mathbf{R} =\{R_1,\dots,R_T\}$. Once the predictions are available, the residual indexes associated with the modes $\{\hat{C}_{T+1},\dots,\hat{C}_{T+H} \}$  are collected in vector $V$ following the Equation (\ref{collection})

\begin{equation}\label{collection}
V = \{\forall i \in [1,T], C_i = \hat{C}_{T+1} \}
\end{equation} The most recent occurrence of the mode $\hat{C}_{T+1}$ in the past is taken using Equation (\ref{Recent}).

\begin{equation}\label{Recent}
	\mathbf{s} = \text{Max}(V)
\end{equation} where \text{Max} function gets the vector element with the maximum value and it is stored in $\mathbf{s}$. The last occurrence of the predicted mode $\hat{C}_{T+1}$ is used as reference for searching $\kappa$ similar residuals to $R_{\mathbf{s}}$. The residual $R_s$ is selected as the basis for searching and sorting its $\kappa$ similar residuals. In Equation (\ref{sortresiduals}) the vectors $R_{V_l}$ are sorted and then taken the $\kappa$ closets residuals to $R_{\mathbf{s}}$

\begin{equation}\label{sortresiduals}
	dist(R_s,R_{V_{l}}) < dist(R_s,R_{V_{l+1}})
\end{equation} where function $dist$ is the Euclidean distance function and $l=\{1,\dots,\kappa-1\}$ are the indexes related to the closets residuals. Once $V$ indexes are ordered, the filter for the next time step $T+1$ is computed by the Equation (\ref{filter}) (that is the simplification of Equation (\ref{filter2}) setting $m=1$).

\begin{equation}\label{filter}
	F_{T+1} = \frac{1}{k}\sum_{i=1}^{\kappa} R_{v_i}
\end{equation} The $F_{T+1}$ vector is used as predictor corrector and it is integrated in Equation (\ref{corrector}).

\begin{equation}\label{corrector}
	\{ {\hat{Y}}_{T \tau+i}\}_{i=1}^{H \tau}   =  \{ \mathbf{\hat{Z}}_{T+i} \cdot \mathbf{{P}}_{\hat{C}_{T+i}}+\alpha F_{T+i} \}_{i=1}^{H}
\end{equation} where $\alpha \leq 1 $ is a real constant number that weighs the amplitude of the filter signal. Notice that when $\alpha=0$ the filter has no effect, and when $\alpha=1$ the filter might over re-adjust the prediction producing poor accuracy forecast. To estimate a value for the $\alpha$ parameter, the \textbf{MMP+NF} is executed with different $\alpha$ and $\kappa$ values using the validation set and are selected those that minimize the prediction error.




\subsubsection{Operational Phase}



For the operational phase of water distribution networks, as it is mentioned before, it is necessary to have available 24 hour ahead predictions in an hourly basis for the operational control. Similarly to the previous algorithm, two days ahead are predicted and a window of size $\tau$ is slid over the prediction ${\hat{Y}_{T \tau +1}},\dots,  {\hat{Y}_{T\tau + H \tau}} $. This process is described by Figure \ref{predictionSample}. To measure the performance of this architecture in an hourly basis is obtained using the testing data set.



{\color{red}With this last algorithm is concluded this Chapter. Next, the evaluation performance is addressed in Chapter \ref{results2} implementing the proposed forecasting models to predict the drinking water demand}


\chapter{Results}\label{results2}


This chapter addresses the performance comparison in the short term of the proposed multi-model forecasting algorithms. In Section \ref{database} is described the database of water demand times series as well as their general characteristics. Section \ref{experiments} shows the experiments and validation for the proposed multi-model algorithms. %The time series database is composed by 105 yearly demand sequences generated by the drinking water network flowmeters of Barcelona. 


\section{Description of the database}\label{database}
To illustrate application with real data of the proposed forecasting models, a brief description of the time series origin is explained. The time series are selected from the database generated by the flowmeters of the water supply network (WSN) of Barcelona. The WSN of Barcelona is managed by AG-BAR company (\textit{Aguas de Barcelona}). AG-BAR supplies water to Barcelona and metropolitan area. The water delivered comes from different water resources: from the rivers Ter and Llobregat, the underground flows and wells that provide water through pumping stations. The total water resources provides a flow approximately of $7 m^3 / s$. A desalinisation plant is also part of the DWN located at the end of Llobregat river, it produces drinking water by treating the sea water. This plant of $60 hm^3/year$ capacity, is strategically convenient specially in case of drought periods. 


%The Figure \ref{map} describes the hydrologic map of Catalonia where the treatment plants are installed at the Llobregat, and Ter river.

% The geographical topology of Barcelona and metropolitan area is structured in pressure flows. This is due the difference in height between the highest point and the sea level that is about $500m$. The pressure is homogenised using intermediate tanks and pumping stations. The water supply system is sensitive to changes in network topology (ruptures), typical daily profiles, as well as major changes in water demand. Important variations of the water demand are noticed hourly. 

The water demand often presents a pattern repeated daily with variations during weekends, and also, sometimes between winter and summer.





%\begin{figure}[h!]
%				\centering
%				\includegraphics[width=50mm]{figures/fig1.pdf}
%				\caption{Location of Pumping stations}
%				\label{map}
%\end{figure}



The experiments are performed with measurements taken by flowmeters of sectors that present a typical behavior of water consumption of residential zones. Since the database is composed by raw time series directly acquired from the flowmeters, some time series present missing or irregular data due to technical issues such as poor transmission quality or malfunction. Due to this, the data should be validated or preprocessed before being used as correct data. The validation and reconstruction of raw data is a task that is out of the scope for this thesis, nevertheless it is possible to make a preliminary outlier analysis with well known statistical tools. We detect and count the outliers adopting the modified Thompson $tau$ technique \cite{dieck2007measurement}. Thompson $tau$ helps to decide whether to keep or discard suspected outliers in a sample of a single variable. The sequences considered for the analysis are those with zero or few outliers. The Figure \ref{allCompleteTimeSeries} and \ref{allCompleteTimeSeries2} show the plots of all the complete time series found in the database with the corresponding number of detected outliers. The sequences remarked black are considered for testing the proposed multi-model forecasting methods. The vertical axis is the normalised magnitude $Y$  and the horizontal axis represents the time $t$ in hours. 






\begin{figure}[h!]
				\centering
				\includegraphics[trim= 20mm 45mm 20mm 40mm, clip, width=110mm]{figures/databaseOutliers1LBN.pdf}
				\caption{Raw time series generated by different flowmeters during the year 2012}
				\label{allCompleteTimeSeries}
\end{figure}


\begin{figure}[h!]
				\centering
				\includegraphics[trim= 20mm 45mm 10mm 40mm, clip, width=100mm]{figures/databaseOutliers2LBN.pdf}
				\caption{Raw time series generated by different flowmeters during the year 2012}
				\label{allCompleteTimeSeries2}
\end{figure}


The time series are normalised transforming the original range values to the bounded domain [0, 1]. The flowmeters are enumerated to identify the sector that they cover with their corresponding detected outliers. We choose those time series 5, 11, 14, 20, 78 and 90 containing less than 70 outliers shown in Table \ref{thompson}, and one time series with an irregular data segment enumerated as 19. The sectors correspond to those with code in the original database p10007, p10015, p10017, 10026, p10095, p10109 and p10025 respectively.


\begin{table}[ht!]
\centering
\begin{tabular}{|c|r|}
\hline
Sector & \multicolumn{1}{c|}{Outliers} \\ \hline
5      & 4                             \\ \hline
11     & 8                             \\ \hline
14     & 1                             \\ \hline
19     & 453                           \\ \hline
20     & 66                            \\ \hline
78     & 1                             \\ \hline
90     & 10                            \\ \hline
\end{tabular}
\caption{Number of outliers detected by the modified Thompson $tau$ technique with a significance of $\alpha=0.01$}
\label{thompson}
\end{table}

\newpage


The characteristic trend of the selected time series is shown in the plot of Figure  \ref{trend01} and \ref{trend02}. 



\begin{figure}[h!]
				\centering
				\includegraphics[trim= 15mm 0mm 0mm 0mm,clip,width=100mm]{figures/trend01BN.pdf}
				\caption{Linear regression for illustrating the trend of the different selected time series}
				\label{trend01}
\end{figure}


\begin{figure}[h!]
				\centering
				\includegraphics[trim= 15mm 80mm 0mm 80mm,clip,width=50mm]{figures/trend02BN.pdf}
				\caption{Linear regression for illustrating the trend of the different selected time series}
				\label{trend02}
\end{figure}


The fitted green plain line describes the trend of each series with the slope and intercept components proper of the linear regression. The values of the series of interest are presented in Table \ref{trend}.
\begin{table}[]
\centering
\begin{tabular}{|c|r|r|}
\hline
Sector & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Slope\\ ($1 \times 10^{-5}$)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Intercept\\ (Mean)\end{tabular}} \\ \hline
5      & 0.2214                                                                                           & 0.3351                                                                          \\ \hline
11     & -0.4062                                                                                          & 0.3998                                                                          \\ \hline
14     & 0.1380                                                                                           & 0.2066                                                                          \\ \hline
19     & -0.6750                                                                                          & 0.3054                                                                          \\ \hline
20     & 0.008                                                                                            & 0.4340                                                                          \\ \hline
78     & -0.2145                                                                                          & 0.3900                                                                          \\ \hline
90     & 0.3389                                                                                           & 0.3900                                                                          \\ \hline
\end{tabular}
\caption{Trend described by the slope and intercept components of linear regression}
\label{trend}

\end{table}

A light trend near to zero is noticed in the series, nevertheless it is not possible to assume trend is constant in the long term as soon as we do not have more information about the next years. The trend of the Sector 19 is severely affected for an irregular period that makes to have a possibly false sense of the trend.



Regarding the seasonality, the Figure \ref{autocorr01} and \ref{autocorr02} shows the strongest autocorrelation at lag 24 for all the time series, which confirms the water demand cycles consumption are repeated in daily basis. Once the database is described, the next Subsection presents the results of the different algorithms developed.




\begin{figure}[h!]
				\centering
				\includegraphics[width=100mm]{figures/autocorr01.eps}
				\caption{Autocorrelation plots of the different sectors}
				\label{autocorr01}
\end{figure}


\begin{figure}[h!]
				\centering
				\includegraphics[width=50mm]{figures/autocorr2.eps}
				\caption{Autocorrelation plot of the different sectors}
				\label{autocorr02}
\end{figure}





\section{Experiments}\label{experiments}

%The algorithm 1 and 2 and the algorithm 3 based in the temporal classification and in the on-line recognition of the prediction model. The first algorithm is a generalised version of SARIMA pattern.  The activity calendar is substituted by a mechanism based in kNN that learns from the historic data pattern modes without requiring information given by a human expert. 





%The second algorithm keep the calendar information for the daily pattern estimation but includes a nonlinear filter for dealing with pattern mismatches presented in SARIMA Pattern. 


This subsection presents the performance presented by the different Multi-Model Predictor algorithms: the quantitative and qualitative Multi-Model predictors  (\textbf{QMMP+NF}, \textbf{QMMP+kNN}) and the ANN Multi-Model Predictor \textbf{(RBFMMP+OR)}. These algorithms are tested and compared with the selected time series described previously.  



\subsection{Validation and Performance Comparison of \textbf{QMMP} Algorithms}

%\subsubsection{Validation of the Multi-Model Predictor Based on the Qualitative and Quantitative Decomposition of time series using SARIMA and kNN}

The performance of \textbf{QMMP+kNN} and \textbf{QMMP+NF} depends on the selected prediction strategy. As we mentioned before, a SARIMA pattern is selected to produce the quantitative forecast and an activity calendar or a kNN to produce the qualitative forecasting. To select the qualitative prediction strategy, a two fold cross validation is performed over a the Training-Validation set from the full mode sequence. This analysis helps to understand the dynamics that follows the mode sequences. The organisation of the sets are described in Figure \ref{validation}.


\begin{figure}[h!]
				\centering
				\includegraphics[width=70mm]{figures/validation.pdf}
				\caption{Organisation of the training-validation and test sets}
				\label{validation}
\end{figure}


The training-validation is divided in datasets $\mathrm{A}$ and $\mathrm{B}$ taken from the $70\%$ of the total data. For this task the time series $\mathbf{Y}_{full}$, are decomposed in its qualitative and qualitative components $\mathbf{Z}$ and $\mathbf{X}$ by using the Equation \ref{quant} and \ref{qual} respectively. The parameter $\tau$ is set to $24$ according to the seasonality observed in the correlation plots of Figure \ref{autocorr01} and \ref{autocorr02}, the qualitative time series $\mathbf{X}$ is classified using k-means. The number of classes is selected according to the maximum silhouette coefficient criteria. Different number of classes are tested with $K_{test} = \{1,\dots,10\}$. Their silhouette values are plot in the Figure \ref{silhouetteVals}, where all the time series considered matches with a number of classes $k=2$.
 
 
\begin{figure}[h!]
				\centering
				\includegraphics[trim= 0mm 80mm 0mm 85mm,clip,width=80mm]{figures/silhuetteValues2.pdf}
				\caption{Mean silhouette coefficient values for different values of k in k-means and different time series of water demand.}
				\label{silhouetteVals}
\end{figure}






The centroids of each class obtained by k-means represent the different nominal consumption patterns and are used as unitary mode prototypes. The classification labels $\mathbf{C}$ are represented in Figure \ref{allModes}, where each monochromatic square represents a pattern mode label. 



 \begin{figure}[!ht]
 				\centering
				\includegraphics[trim = 11mm 70mm 0mm 70mm, clip,width=100mm]{figures/6modesLLL.pdf}
				\caption{The two pattern modes of different sectors. The week starts on Sunday.}
				\label{allModes}
\end{figure}




%In order to choose the best strategy for the qualitative mode prediction, the training-validation set is used.
We consider the calendar $\hat{F}_{cal}$ (Equation (\ref{calendar}) ) and kNN qualitative mode predictor (Section \ref{qualitativeForecasting}) to be chosen. The mode strategy that describes and predicts better the mode sequences is selected.

%The performance of the qualitative prediction is validated using two iteration cross validation comparing the prediction errors given by the Equation \ref{qualitativeParametersOpt}.

The cross validation estimates the predictability of the pattern mode of both strategies. The strategy is selected according to the minimisation of the average pattern prediction errors criteria. The two iteration cross validation is performed in this way: the first iteration takes the subset $\mathrm{B}$ as the training set and the set $\mathrm{B}$ is the validation set, and for the second iteration, $\mathrm{B}$ is used as training set and $\mathrm{B}$ as the validation set. After finishing the two iterations, the errors obtained are averaged. The results are shown in the Table \ref{crossValidation}. 


\begin{table}[htbp]

\begin{center}
\begin{footnotesize}
\begin{tabular}{|r|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{} & \multicolumn{2}{c|}{Iter 1} & \multicolumn{2}{c|}{Iter 2} & Mean & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{suggestion} \\ \hline
\multicolumn{1}{|c|}{Sectors} & kNN & Calendar & kNN & Calendar & kNN & Calendar &  \\ \hline
5 & 5 & 5 & 13 & 0 & 9 & 2.5 & calendar \\ \hline
11 & 20 & 4 & 15 & 7 & 17.5 & 5.5 & calendar \\ \hline
14 & 5 & 5 & 11 & 17 & 8 & 11 & kNN \\ \hline
19 & 3 & 5 & 14 & 1 & 8.5 & 3 & calendar \\ \hline
20 & 4 & 1 & 15 & 2 & 9.5 & 1.5 & calendar \\ \hline
78 & 4 & 3 & 7 & 2 & 5.5 & 2.5 & calendar \\ \hline
90 & 7 & 5 & 18 & 2 & 12.5 & 3.5 & calendar \\ \hline
\end{tabular}
\end{footnotesize}
\end{center}
\caption{Two iteration cross validation for kNN and Calendar mode estimation}
\label{crossValidation}
\end{table}
In both iterations the parameters $m$ and $\varepsilon$ defining the length and neighborhood of the qualitative delay vectors of kNN are optimised using the Equation \ref{qualitativeParametersOpt} setting the prediction horizon one day ahead $H=1$ (prediction of one pattern mode ahead).





%but for shorter prediction horizons as the case of 5, 6 and 12 RBF-ANN is better. This is a debatable comparison since different RBF-ANN architectures are specifically trained to do predictions at different horizons. 




\begin{equation}\label{qualitativeParametersOpt}
\begin{aligned}
& \underset{m, \varepsilon}{\text{arg min}}
  \sum_{T=1}^{N}  dist( \hat{\mathbf{C}}_{T+H}  , \mathbf{C}_{T+H}) \\
%& \text{subject to} \\    & \epsilon, m &
\end{aligned}
\end{equation} where $dist$ is the Hamming distance since $\mathbf{C}$ is a sequences of labels and the prediction is performed by using the Equation \ref{estimatingModes}. Table \ref{knnparameters} includes the parameters that optimize the prediction with kNN for each iteration. The distance of the neighborhood and size of the delay vector for kNN mode estimator are those that minimises the objective function expressed by Equation (\ref{kNNobjectiveFunction}) subject to $ \varepsilon = \{0.01,0.02,\dots,0.1\}$  and $m=\{2,\dots,10\}$.


\begin{table}[htbp]
\begin{center}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{1}{|l|}{} & Iter 1 & Iter 2 \\ \hline
\multicolumn{1}{|l|}{Sector} & m & m \\ \hline
14 & 10 & 5 \\ \hline
90 &  8 & 6 \\ \hline
78 & 8  & 9 \\ \hline
20 & 10 & 9 \\ \hline
11 &  6 & 8  \\ \hline
5 &  10 & 9 \\ \hline
19 &  8 & 9 \\ \hline
\end{tabular}
\end{center}
\caption{$m$ parameters that optimize the mode prediction with kNN. $\varepsilon=0.01$ was chosen for all time series}
\label{knnparameters}
\end{table}

This test suggest the use of the calendar for all time series, with the exception of the Sector 14 that is more suitable to be predicted using kNN. %Next, a performance comparison of the algorithms 1 and 2 is shown.

%\ref{QQMMP}




%The Table ?? shows that the multi-model approach outperforms SARIMA Pattern, Holt-Winters and RBF-ANN for $h$ set to $24$, 

\subsubsection{Performance of \textbf{QMMP + kNN} and  \textbf{QMMP+NF}}

For the algorithm \textbf{QMMP + kNN} the data set is also divided in two sets. The training and testing set with the $70\%$ and $30\%$ from the total data respectively. With the training set is performed the preparation phase consisting in the decomposition, pattern mode estimation, SARIMA and kNN parameter tuning.

For the algorithm \textbf{QMMP+NF} the data set is divided in 3 sets. The preparation phase of is similar than \textbf{QMMP + kNN}. With the difference that the half of the testing set is used for tuning the noise filter. The organisation of the different data sets can also be observed in Figure \ref{trainingTestingScheme}.


%validationFilterMM01.pdf


 \begin{figure}[!ht]
 				\centering
				\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]{figures/validationFilterMM01.pdf}
				\caption{Training and testing data organisation}
				\label{trainingTestingScheme}
\end{figure}





The parameters $m$, $\varepsilon$ and SARIMA structures of the two algorithms are shown in Table \ref{MMPParams} which the search space for the kNN qualitative forecaster are defined by $m=\{1,\dots,20\}$ and $\varepsilon=\{0.01,0.02,\dots,1.00\}$. For the noise filter parameters the search space is defined by $ \alpha=\{0,0.1,$ $0.2,\dots, 1 \} $ and $\kappa=\{1,2,\dots,40\}$. All those values are tested using the validation set and selected those that minimize the mean of all the independent $RMSE_h$ described in Equation \ref{MinMSE}.

%\begin{equation}\label{MinMSE}
%	\underset{  \alpha, \kappa }{\text{arg min}} ||\hat{Y}_{T}-Y_{T}||
%\end{equation} where $\hat{Y}_{T+i}= \mathbf{\hat{Y}}_{\lfloor t/\tau \rfloor+i} $ is computed using Equation (\ref{corrector}).

\begin{equation}\label{MinMSE}
	\underset{  \alpha, \kappa }{\text{arg min } } \overline{RMSE_h}
\end{equation} %where $\hat{Y}_{T+i}= \mathbf{\hat{Y}}_{\lfloor t/\tau \rfloor+i} $ is computed using Equation (\ref{corrector}).


%For the Daily Seasonally SARIMA pattern, a model with structure (0, 1, 1) is selected with Model Seasonally Integrated and Seasonal M A(7).




\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{ 6}{c|}{ SARIMA Models} & kNN & \multicolumn{ 2}{c|}{Noise filter} \\ \hline
Sectors & 	p 	& 	d	& 	q 	& 	P 	& D & Q & $m$ 	& $\alpha$  & $\kappa$ \\ \hline
5  		& [2,4]& 	1 	& [1]	& 	[0]	& 0 & [7]	& 9 	& 0.85 & 70   \\ \hline
11 		& [2,4]& 	1 	& [1]	& 	[0]	& 0 & [7]	& 14 	&  0.65& 70   \\ \hline
19 		& [2,4]& 	1 	& [1]	&	[0]	& 0 & [7] 	& 13	&  0.55& 42   \\ \hline
20 		& [2,4]& 	1	& [1]	& 	[0]	& 0 & [7] 	&	13 	&  0.8&  70  \\ \hline
78 		& [1,2,3,19]	& 1	& [0]	& 	[0]	& 0 & [7] 	& 10	&  0.6&  70  \\ \hline
90 		& [2,4]& 	1 	& [1] 	& 	[0]	& 0 &  [7]	& 8 	&  0.7&  70  \\ \hline
14 	& [0] 	&  	1 	& [2, 7] & 	[0] 	& 0 & [7]	&  7 	& 0.6  & 70   \\ \hline
\end{tabular}
\end{center}
\caption{Parameters for the Noise Filter, kNN Qualitative Forecaster and SARIMA models for each time series found.}
\label{MMPParams}
\end{table}


%Model for sector90.txt, sector5.txt sector 20 sector 19 11
%model = arima('D',1, 'MALags',[1] ,'Seasonality',7,'SMALags',[7],'ARLags',[2 4]);

%model for sector5.txt, sector 78
%model = arima('D',1, 'MALags',[1] ,'Seasonality',7,'SMALags',[7],'ARLags',[1 2 3 19]);
%[EstMdl,EstParamCov] = estimate(model,Y(1:183)');

%model = arima('D',1, 'MALags',[1] ,'Seasonality',7,'SMALags',[7 12],'ARLags',[1 2 3 8 12]);
%[EstMdl,EstParamCov] = estimate(model,Y(1:255)');
%clean10017
%model = arima('D',1, 'MALags',[2 7] ,'Seasonality',7,'SMALags',[7]);







The two algorithms are compared with well known forecasting models such as, Double Seasonal Holt-Winters (DSHW) and RBF Neural Networks (RBF-ANN).. %The performance is compared using the Average Mean Square Error defined by Equation \ref{MRMSE}.



% This Multi-Model Predictor can be seen as the generalized version of the SARIMA pattern proposed by Puig. It is designed to forecast qualitative patterns without assumption about any calendar. It is useful for applying to the time series where can be decomposed in qualitative an quantitative time series and there is no clear pattern about the qualitative component as the case of the activity calendar used in the SARIMA pattern.


%The holidays and weekdays patterns are determined classifying unitary patterns of the training set and taking the corresponding centroids as unitary patterns associated with an activity calendar to perform the daily prediction.

For the Holt-Winters, two parameters associated with two periods are fixed. Since Holt-Winters manages hourly data, period $\tau_1$ and period $\tau_2$ parameters are set to 24 and 168 for the daily and weekly period.
For the RBF-ANN, different structures are implemented with 192 neurons in the hidden layer, fixed widths with $\sigma = 1$, and input layer of size 24 and the output layer of size 24 ($h=24$). 

Finally to measure the performance of the algorithms is used the $\overline{RMSE}_{24}$ (Equation \ref{MRMSE}). The $\overline{RMSE}_h$ produced by the \textbf{QMMP}, \textbf{QMMP} \textbf{+NF}, \textbf{QMMP}\textbf{+kNN}, \textbf{RBF-ANN}  and \textbf{DSHW} using the test set for $h=24$ steps ahead, $\alpha$ and $\kappa$ best values for the \textbf{QMMP+NF} are reported in Table \ref{performance}.

%\begin{table}[htbp]
%\centering
%\begin{tabular}{|r|r|l|l|r|r|}
%\hline
%\multicolumn{1}{|l|}{Sectors} & \multicolumn{1}{c|}{\textbf{MMP}} & \multicolumn{3}{c|}{\textbf{MMP+NF}} & \multicolumn{1}{l|}{\textbf{RBF-ANN}} \\ \hline
%\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{$\overline{MSE}_{24}$} & \multicolumn{1}{c|}{$\alpha$} & \multicolumn{1}{c|}{$\kappa$} & \multicolumn{1}{l|}{$\overline{MSE}_{24}$} & \multicolumn{1}{l|}{$\overline{MSE}_{24}$} \\ \hline
%78 & 0.0404 & 0.6 & 70 & \textbf{0.0359} & 0.0572 \\ \hline
%90 & 0.0632 & 0.7 & 70 & \textbf{0.0469} & 0.0627 \\ \hline
%11 & 0.0492 & 0.65 & 70 & \textbf{0.0470} & 0.0649 \\ \hline
%20 & 0.0494 & 0.8 & 70 & \textbf{0.0473} & 0.0578 \\ \hline
%5 & 0.0406 &  0.85 & 70  & \textbf{0.0400} & 0.0563 \\ \hline
%19 & 0.0248 & 0.55 & 42 & \textbf{0.0239} & 0.0333 \\ \hline
%10017 & 0.0723 & 0.6 & 70 & 0.0727 & \textbf{0.0558} \\ \hline
%\end{tabular}
%\caption{$\overline{MSE}_{24}$ performance comparison of the different algorithms}
%\label{performance}
%\end{table}



\begin{table}[htbp]
\begin{center}
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|l|}{Sec} & \multicolumn{1}{l|}{\textbf{QMMP}} & \multicolumn{1}{l|}{\textbf{QMMP+NF}} & \textbf{QMMP+kNN} & \multicolumn{1}{l|}{\textbf{RBF-ANN}} & \multicolumn{1}{l|}{\textbf{DSHW}}  \\ \hline
5 & 0.0436 & \textbf{0.04} & 0.0494  & 0.0563 & 0.1009   \\ \hline
11 & 0.0492 & \textbf{0.047} & 0.051749 & 0.0649 & 0.0755   \\ \hline
19 & 0.0248 & \textbf{0.0239} & 0.029168 & 0.0333 & 0.0313   \\ \hline
20 & 0.0494 & \textbf{0.0473} & 0.050302 & 0.0578 & 0.0555   \\ \hline
78 & 0.0404 & \textbf{0.0359} & 0.041641 & 0.0572 & 0.0512   \\ \hline
90 & 0.0632 & \textbf{0.0469} & 0.0640  & 0.0627 &  0.0609   \\ \hline
14 & 0.0723 & 0.0727 & \textbf{0.0418} & 0.0558 &   0.0592  \\ \hline
\end{tabular}
\end{center}
\caption{$\overline{RMSE}_{24}$  performance comparison of the different algorithms}
\label{performance}
\end{table}


%\begin{table}[htbp]
%\begin{center}
%\begin{tabular}{|r|r|r|r|r|r|}
%\hline
%\multicolumn{1}{|l|}{Sectors} & \multicolumn{1}{l|}{\textbf{MMP}} & \multicolumn{1}{l|}{\textbf{QMMP+NF}} & \textbf{QMMP+kNN} & \multicolumn{1}{l|}{\textbf{RBF-ANN}} & \multicolumn{1}{l|}{\textbf{DSHW}}  \\ \hline
%5 & 0.0436 & 0.04 & 0.0494  & 0.0563 & 0.100916   \\ \hline
%11 & 0.0492 & 0.047 & 0.051749 & 0.0649 & 0.075592   \\ \hline
%19 & 0.0248 & 0.0239 & 0.029168 & 0.0333 & 0.03130634   \\ \hline
%20 & 0.0494 & 0.0473 & 0.050302 & 0.0578 & 0.05559293   \\ \hline
%78 & 0.0404 & 0.0359 & 0.041641 & 0.0572 & 0.05129   \\ \hline
%90 & 0.0632 & 0.0469 & 0.0640  & 0.0627 & 0.06093128   \\ \hline
%17 & 0.0723 & 0.0727 & 0.0418 & 0.0558 & \multicolumn{1}{l|}{}   \\ \hline
%\end{tabular}
%\end{center}
%\caption{$\overline{RMSE}_{24}$  performance comparison of the different algorithms}
%\label{performance}
%\end{table}


In order to compare graphically the performance regarding the \textbf{QM} \textbf{MP}\textbf{+NF}, the figures \ref{fig:stacked:1} - \ref{fig:stacked:14} illustrate the improvement that the filter produces: the first column of plots shows the prediction errors $RMSE_{24}$ at each time $t$ in hours along the testing set comparing the performance of the Noise Filter (\textbf{QMMP+NF}) (red line) with the original Qualitative and Quantitative forecasting (\textbf{QMMP}) (Black dotted line) with the pattern mode calendar. For the second column, a prediction sample 48 hours ahead is plot comparing the actual (black line), with the normal (\textbf{QMMP}) (blue dotted line) and filtered predictions (\textbf{QMMP+NF}) (red line). It is observed that the noise filter reduces in general the prediction error since the red line is the most of the time lower than the black line representing the prediction with SARIMA pattern without filtering.
%The Table \ref{performance} shows a performance improvement of the proposed \textbf{MMP+NF} algorithm minimizing the $\overline{MSE}_{24}$ error respecting \textbf{MMP} and \textbf{RBF-ANN} for all sectors except the sector 10017. Taking the average error of each forecasting method for all tested sectors, the proposed methodology improved the accuracy $\% 19.5$ respecting the \textbf{RBF-ANN}, and $\%12.39$ respecting the \textbf{MMP}.





%\begin{tabular}{cc}
%  \num\putindeepbox[7pt]{\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]{figures/validationFilterMM01.pdf}}
%    & \num\putindeepbox[7pt]{\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]{figures/validationFilterMM01.pdf}} \\
%  \num\putindeepbox[7pt]{\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]{figures/validationFilterMM01.pdf}}
%    & \num\putindeepbox[7pt]{\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]{figures/validationFilterMM01.pdf}} \\
%\end{tabular}





\begin{figure}[htbp]
	\centering
	%%----start of first figure----
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s5-p1007MD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 5}
		\label{fig:stacked:1}
	\end{minipage}%
 	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s5-p1007LMD.pdf}
		\\[00pt]
		\caption{Prediction sample of 2 days ahead of Sector 5}
		\label{fig:stacked:2}
	\end{minipage}%
	\\[00pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s11-p10015LMD}
		\\[00pt]
		\caption{Error along the testing set of Sector 11}
		\label{fig:stacked:3}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s11-p10015LMD.pdf}
		\\[00pt]
		\caption{Prediction sample of 2 days ahead of Sector 11}
		\label{fig:stacked:4}
	\end{minipage}%
	\\[00pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s19-p10025LMD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 19}
		\label{fig:stacked:5}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s19-p10025LMD.pdf}
		\\[00pt]
		\caption{Prediction sample of 2 days ahead of Sector 19}
		\label{fig:stacked:6}
	\end{minipage}%
\end{figure}



\begin{figure}[htbp]
	\centering
	%%----start of first figure----
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s20-p10026LMD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 20}
		\label{fig:stacked:7}
	\end{minipage}%
 	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s20-p10026LMD.pdf}
		\\[00pt]
		\caption{Prediction sample of 2 days ahead of Sector 20}
		\label{fig:stacked:8}
	\end{minipage}%
	\\[00pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s78-p1098LMD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 78}

		\label{fig:stacked:9}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s78-p10098LMD.pdf}

		\caption{Prediction sample of 2 days ahead of Sector 78}
		\label{fig:stacked:10}
	\end{minipage}%
	\\[00pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s90-p1019LMD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 90}
		\label{fig:stacked:11}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s90-p10019LMD.pdf}
		\\[00pt]
		\caption{Prediction sample of 2 days ahead of Sector 90}
		\label{fig:stacked:12}
	\end{minipage}%
\end{figure}




\begin{figure}
\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 28mm 75mm 00mm 65mm,clip,width=62mm]{figures/horizon-s13-p1017mMD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 14}
		\label{fig:stacked:13}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 30mm 75mm 00mm 65mm,clip,width=62mm]{figures/s14-p10017MD.pdf}
		\\[00pt]
		\caption{Error along the testing set of Sector 14}
		\label{fig:stacked:14}
	\end{minipage}%
\end{figure}




\subsubsection{Discussion}

 The proposed architecture can be generalised for those time series that present clear patterns in regular time lapses and can be validated as we did in Chapter 6.1. The SARIMA pattern is a simple and effective method that fit in the proposed Multi-Model \textbf{MMP} architecture. It works pretty good for the short term water demand forecasting. The general architecture of this methodology is very flexible, although we explored some configurations, therefore more work exploring suitable algorithms for decomposing the time series in other simpler forms and predicting them should be done.  The architecture allows to substitute and plug different forecasting models for the qualitative and quantitative components. For example, the quantitative series is possible to be modelled with \textbf{DSHW}, \textbf{ANN} among others. Also the qualitative component is not limited to be modelled either with the calendar or  kNN.  As an example, the structure organisation of the multi-model allows to implement straightforward a mix of the \textbf{MMP+kNN} and \textbf{MMP+NF} approaches for increasing the prediction accuracy.


\subsubsection{Conclusions}

This subsection has shown a validation strategy based on the two iteration cross validation for choosing a qualitative mode predictor for proposed Multi-Model Predictor based on the decomposition of qualitative and quantitative components. Also two algorithms are compared against RBF-ANN and Holt-Winters. kNN mode predictor is shown as an alternative of pattern prediction when the calendar based pattern prediction performs poor. The algorithms and models can be chosen a priory observing the performance presented in the testing set. It is introduced a filter that distinguishes the noise from the structure of the past residuals to improve the prediction accuracy. It is demonstrated in practice that discovering structure from the past errors may improve the performance in general. The implementation of the filter processing may produce more flexible and adaptable forecasting models that fit the gradual appearance of unobserved and unmodelled dynamics that may occur in real systems.

% The implementation was tested with real time series generated by the flowmeters of several residential sectors with typical behavior of the Drinking Water Network of Barcelona city.  The algorithm were tested predicting 24 hours ahead for control purposes. 


As future work the authors suggest to explore the performance of the kNN mode predictor plus the noise filter for improving the pattern prediction. Also is suggested the exploration of the other kind of forecasters and filters and the study. 




\subsection{Validation and performance of the Multi-Model Forecasting Using RBF Artificial Neural Networks with an On-line Mode Recognition}\label{valandper}

In this subsection, another Multi-Model Predictor implementation, the \textbf{RBFMMP+OR}, is applied to the water demand time series described previously in Subsection \ref{database}.

%For exemplifying the shape of the curve of the water demand of the sector 17, a sample of the first 350 hours of water demand is shown in Figure\ref{sampleWaterDemand}. 


%\begin{centering}
%\begin{figure}[h!]
%				\centering
%				\includegraphics[width=60mm]{figures/sampleTS.eps}
%				\caption{Sample of the water demand time series}
%				\label{sampleWaterDemand}
%\end{figure}
%\end{centering}

The \textbf{RBFMMP+OR} is implemented with the algorithms described in the Section \ref{mmfrbf} setting the parameters as follows: for the \textbf{Split Time Series} and \textbf{Mode Recognition} modules, the split point is set to $\tau=12$ and the mode recognition implements the nearest neighbor rule. For the \textbf{Feature Extraction} module implements the discrete derivative or differentiation of the time series. The \textbf{Pattern Classifier}  implements k-Means. The number of clusters $k$ is defined by the maximisation of the average silhouette coefficient. For the \textbf{Multi-Model} module predictor, the RBF-ANN structure has input layer of size $k+24$, the hidden layer has $192$ neurons with width $\sigma=1$ and an output layer size $24$ where each output neuron is the prediction for each time step the next 24 hours. 



%is implemented with fixed widths $\sigma=1$ with $192$ hidden neurons, $k+24$ input layer and and output layer of $h=24$ for performing the forecasting 24 hours ahead. 


 %structures varying the number of inputs and outputs according to the desired horizons $h=\{1,2,3,4,5,6,12,24\}$ are shown in the Table \ref{dailyresults}.  The number of neurons is fixed to $M=192$ for all cases and for the \textbf{Feature Extraction} module is selected the discrete derivative of the time series.

%(Equation (\ref{derivative})), for the \textbf{Pattern Classifier} the k-means algorithm is implemented (equations (\ref{kmeans})-(\ref{silhouette})),



 %Radial basis function neural network is used for regression selecting a width parameter $\sigma=1$ and $192$ neurons in the hidden layer. The discrete derivative is adopted as feature extraction method used for the pattern detection module and the real time mode discovery algorithm is used.

%The parameters for formatting the data are set as follows: the split point is set $\tau=24$, number of past measurements taken into account for all the horizons is set $m=24$.

The data is divided in two sets; $70\%$ for training and tuning the different parameters of the models, and the $30 \%$ for validation. The Figure \ref{silhouetteVals2} shows the average silhouette values obtained for different sectors and different number of clusters $k$. The \textbf{Classifier} module uses k-Means algorithm. 



\begin{figure}[h!]
				\centering
				\includegraphics[trim = 11mm 70mm 10mm 80mm, clip,width=80mm]{figures/rbfannmodes.pdf}
				\caption{Mean silhouette coefficient values for different values of k in k-means and different time series of water demand.}
				\label{silhouetteVals2}
\end{figure}

The results including the number of clusters selected are indicated in Table \ref{silhouetteVals2} where the $\overline{RMSE}_h$ given by the Equation \ref{MRMSE} is used for measuring the accuracy of the forecasting in the validation set. The proposed approach \textbf{RBFMMP+OR} is compared with:

\begin{itemize}
  \item A Seasonal ARIMA Pattern (SARIMA) (\textbf{QMMP}) based on the work of \cite{Quevedo2010} with a SARIMA model structure is defined previously in the Table \ref{MMPParams}. 
  
  %$ARIMA(0,1,1) \times$ with  Model Seasonally Integrated with Seasonal $MA(7)$.
  \item A Double Seasonal Holt-Winters (DSHW) \cite{Kalekar2004} with  period $\tau_1$ and period $\tau_2$ set to 24 and 168 for the daily and weekly period respectively.
  \item A standard RBF Artificial Neural Network \textbf{RBF-ANN} taking $m+k$ inputs and $h$ outputs, with $m=24$.
\end{itemize}

The Table \ref{sectorResults} presents the results of this comparison.  The columns of the \textbf{RBFMMP+OR} contains the modes identified from the Figure \ref{silhouetteVals2}.




\begin{table}[h]
\centering
\label{sectorResults}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Sect.  &	\textbf{QMMP}	&  \textbf{DSHW}  &\begin{tabular}[c]{@{}c@{}}     $k$  \\ modes  \end{tabular} &\begin{tabular}[c]{@{}c@{}}     \textbf{RBFMMP} \\  \textbf{+}  \\ \textbf{OR} \end{tabular}  & \begin{tabular}[c]{@{}c@{}} \textbf{RBF-ANN }\\ $24+k$ inputs\\ $h$ outputs   \end{tabular}  \\ \hline
5       &	\textbf{0.0453}	&	0.1009168	& 5	& \textit{0.0522}& 	0.0526 \\ \hline
11       &	\textbf{0.051719}	& 0.07225436*		& 7	&0.0658	& \textit{0.0606}\\ \hline
19       &	\textbf{0.027823}	&	0.03130634	& 2	& \textit{0.0336} & 0.037	\\ \hline
20       &	\textbf{0.045474} 	& 0.05559293		& 7	& \textit{0.0543} & 0.0557	\\ \hline
78       &	\textbf{0.040847}	&	0.0512956	& 7	&0.0546	& \textit{0.0527}\\ \hline
90       & \textbf{0.059176} 		&  0.0609 	& 8	& 0.0624 & \textit{0.0599}	\\ \hline
14      &	0.0651	&	0.05646	&	6 & \textbf{0.0433} & 0.0513	\\ \hline
Avg.      &	\textbf{0.0479}	&	0.0612	& 	6 & 	0.0523 & 0.0528 \\ \hline
\end{tabular}
\caption{$\overline{RMSE}_{24}$ indicator for each time series produced by \textbf{QMMP},  \textbf{DSHW}, \textbf{RBFMMP+OR} and \textbf{RBF-ANN}}
\label{sectorResults}
\end{table}

%\begin{tabular}{|c|c|c|}
%\hline
%Sectors  &  \begin{tabular}[c]{@{}c@{}} \textbf{RBF-ANN }\\ $24+k$ inputs,\\ $h$ outputs   \end{tabular}  &  \begin{tabular}[c]{@{}c@{}}RBF-ANN\\ 24 inputs,\\ $h$ outputs   %\end{tabular}\\ \hline

%5       &	 0.0526		&	0.0543		\\ \hline
%11      &	 \textit{0.0606}		&   0.0623		\\ \hline
%19       &	 0.037		& 	0.034		\\ \hline
%20       &	0.0557		&   0.00557		\\ \hline
%78       & 	\textit{0.0527}		&   0.0553		\\ \hline
%90       &	\textit{0.0599}		&  	0.139		\\ \hline
%14      &	0.0513		& 	0.0511		\\ \hline
%Average      &		0.0528	& 	0.05736		\\ \hline
%\end{tabular}
%\caption{$\overline{RMSE}_{24}$ indicator for each time series}
%\label{sectorResults}
%\end{table}


Considering the average performance of each algorithm for all the series, the Table shows that \textbf{QMMP} is the algorithm with the best average performance with an average error of $0.0479$. The second best is the proposed algorithm \textbf{RBFMMP+OR} with an average performance of $0.0523$, the third best algorithm is the \textbf{RBF-ANN} with an average performance of $0.0528$. The \textbf{DSHW} is the algorithm with the worst performance with an average performance of $0.0612$. Comparing with percentages,  \textbf{QMMP}, \textbf{RBFMMP+OR},   and \textbf{RBF-ANN} are $\%21$, $\%13.7$ and  $\%14.5$ better than \textbf{DSHW} performance. The asterisk written in the performance box of the  \textbf{DSHW} with regard to Sector 11 means that it displays an unstable behavior for a set of values of the time series, so in order to have a consistent error measurement, the unstable section was omitted. This kind of situations reinforce the need of exploring filtering and reconstructing the time series. In figures \ref{fig:stackedB:1}-\ref{fig:stackedB:7} are plot the prediction samples produced by the \textbf{RBFMMP+OR} 24 hours ahead during the day 298. The red dotted line is the prediction and the solid black line is the real water demand during that period.







\begin{figure}[htbp]
	\centering
	%%----start of first figure----
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/05L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 5}
		\label{fig:stackedB:1}
	\end{minipage}%
 	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/11L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 11}
		\label{fig:stackedB:2}
	\end{minipage}%
	\\[20pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/17L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 17}
		\label{fig:stackedB:3}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/19L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 19}
		\label{fig:stackedB:4}
	\end{minipage}%
	\\[20pt]
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/20L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 20}
		\label{fig:stackedB:5}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/78L.pdf}
		\\[00pt]
		\caption{Prediction sample Sector 78}
		\label{fig:stackedB:6}
	\end{minipage}%
		\\[00pt]
\end{figure}




\begin{figure}[h!]
				\centering
				\includegraphics[trim = 10mm 80mm 10mm 85mm,clip,width=56mm]{figures/RBFMMP/90L.pdf}
				\\[00pt]
				\caption{Prediction sample Sector 90}
				\label{fig:stackedB:7}
\end{figure}


Another experiment is presented analysing the performance of \textbf{RBF} \textbf{MMP} \textbf{+} \textbf{OR} at different prediction horizon values $h$ with the same algorithms presented in Table \ref{sectorResults}.  \textbf{RBFMMP} \textbf{+} \textbf{OR}  shows better performance than the traditional \textbf{RBF-ANN}, specially for the series of Sector 14. The experiment consists in testing different \textbf{RBF} \textbf{MMP} \textbf{+OR} and  \textbf{RBF-ANN}  architectures, such that the structures have an input layer of size 24 plus the mode input size $k$, 192 neurons in the hidden layer, and  variable outputs size according to the desired horizons $h=\{1,2,3,4,5,6,12,24\}$. 
The mean silhouette values for each prediction horizon $h$ is shown in the Figure \ref{fcs01} and the results including the selection of $k$ are in Table \ref{dailyresults}. 



\begin{figure}[h!]
				\centering
				\includegraphics[width=90mm]{figures/kplot.eps}
				\caption{Mean silhouette value for varying values of $k$ and variable prediction Horizon $h$}
				\label{fcs01}
\end{figure}


 


%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|c|c|}
%\hline
%Horizon $h$ & \begin{tabular}[c]{@{}c@{}} \textbf{QMMP} \\   \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{DSHW}\\  \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{RBFMMP+OR}\\   \\ (KMEANS - RBF-ANN) \\ $24+k$ inputs, $h$ outpus \end{tabular}   \\ \hline
%1       &	0.0422	&	0.04513	&	0.0173	\\ \hline
%2       &	0.0456	&	0.04729	&	0.0217	\\ \hline
%3       &	0.0483	&	0.04885	&	0.0244	\\ \hline
%4       &	0.0506	&	0.05016	&	0.026	\\ \hline
%5       &	0.0528	&	0.05131	&	0.0283	\\ \hline
%6       &	0.0551	&   0.05234	&	0.283	\\ \hline
%12      &	0.0651	&	0.05646	&	0.0345	\\ \hline
%24      &	0.0587	&	0.05922	&	0.0433	\\ \hline
%\end{tabular}

%\begin{tabular}{|c|c|c|}
%\hline
%Horizon $h$  &  \begin{tabular}[c]{@{}c@{}} \textbf{RBF-ANN }\\ $24+k$ inputs,\\ $h$ outputs   \end{tabular}  &  \begin{tabular}[c]{@{}c@{}}RBF-ANN\\ 24 inputs,\\ $h$ outputs   \end{tabular}\\ \hline

%1       &	 0.024		&	0.025		\\ \hline
%2      	&	 0.027		&   0.0287		\\ \hline
%3       &	0.0307		& 	0.0314		\\ \hline
%4       &	0.0333		&   0.034		\\ \hline
%5       & 	0.0350		&   0.0361		\\ \hline
%6       &	0.0366		&  	0.0373		\\ \hline
%12      &	0.0425		& 	0.0436		\\ \hline
%24     	&	0.0511		&  	0.0528		\\ \hline


%\end{tabular}
%\caption{$\overline{RMSE}_h$ indicator for each method for the hourly time series of the sector 10013.}
%\label{dailyresults}
%\end{table}







%the   better in general than the other prediction methodologies, however, the sector 17 is predicted in a better way by the proposed \textbf{RBFMMP+OR}. The order of the prediction performance from the best to the worst methodology according to the number of time series solved better is\textbf{QMMP}, solving 6 out of 7.  was the second better algorithm solving better 4 time series better compared with \textbf{DSHW}.  \textbf{RBF-ANN} solved 4 time series better than \textbf{DSHW}

%was the best predictor methodology, then ,  and 

%\begin{itemize}
%\item Explained variance: $EV = 1 - \frac{Var(e)_k}{Var(x_k) }$. That indicates the non modelled data. $EV = 1$ means that it captures the the whole process data.
%\item Mean Absolute Errors: Measures the deviation absolute errors means the units of the process .
%\item Mean squares error: $MSE =  \frac{1}{n} \sum_{i=1}^n e_k(i)^2$
%\item Mean Absolute Percentage Error: $MAPE=\frac{100}{n} \sum_{i=1}^{n} |\frac{e_k(i)}{\mu}|$
%\end{itemize}



%\begin{figure}[h!]
%				\centering
%				\includegraphics[width=90mm]{figures/kplot.eps}
%				\caption{Mean silhuette value for varying values of $k$ and prediction Horizon $h$}
%				\label{fcs01}
%\end{figure}


%\includegraphics[trim = 11mm 5mm 0mm 5mm, clip,width=100mm]




\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 \begin{tabular}[c]{@{}c@{}} Horizon\\ $h$   \end{tabular}& \begin{tabular}[c]{@{}c@{}} \textbf{QMMP} \\   \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{DSHW}\\  \end{tabular}& \begin{tabular}[c]{@{}c@{}} \textbf{$k$}\\ 
\end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{RBFMMP+OR}\\  k-Means  \\ $24+k$ inputs \\ $h$ outputs \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{RBF} \\ \textbf{-ANN}\\ $24+k$ inputs\\ $h$ outputs   \end{tabular}  \\ \hline
1       &	0.0422	&	0.04513	& 5	&\textbf{0.0173}	&	0.0240\\ \hline
2       &	0.0456	&	0.04729	& 5	&\textbf{0.0217}	&	0.0270\\ \hline
3       &	0.0483	&	0.04885	& 5	&\textbf{0.0244}	&	0.0307\\ \hline
4       &	0.0506	&	0.05016	& 5	&\textbf{0.0260}	&	0.0333\\ \hline
5       &	0.0528	&	0.05131	& 5	&\textbf{0.0283}	& 	0.0350\\ \hline
6       &	0.0551	&   0.05234	& 5	&\textbf{0.2830}	&	0.0366\\ \hline
12      &	0.0651	&	0.05646	& 6	&\textbf{0.0345}	&	0.0425\\ \hline
24      &	0.0587	&	0.05922	& 6	&\textbf{0.0433}	&	0.0511 \\ \hline
\end{tabular}

%\begin{tabular}{|c|c|c|}
%\hline
%Horizon $h$  &  \begin{tabular}[c]{@{}c@{}} \textbf{RBF-ANN }\\ $24+k$ inputs,\\ $h$ outputs   \end{tabular}  &  \begin{tabular}[c]{@{}c@{}}RBF-ANN\\ 24 inputs,\\ $h$ outputs   \end{tabular}\\ \hline

%1       &	0.0240		&	0.0250		\\ \hline
%2      	&	0.0270		&   0.0287		\\ \hline
%3       &	0.0307		& 	0.0314		\\ \hline
%4       &	0.0333		&   0.0340		\\ \hline
%5       & 	0.0350		&   0.0361		\\ \hline
%6       &	0.0366		&  	0.0373		\\ \hline
%12      &	0.0425		& 	0.0436		\\ \hline
%24     	&	0.0511		&  	0.0528		\\ \hline


%\end{tabular}
\caption{$\overline{RMSE}_h$ indicator for each method for the hourly time series of the Sector 14.}
\label{dailyresults}
\end{table}


%function [ e ] = RMSE(A,B)
%    e=sqrt(sum((A - B).^2)/length(A));
%end


The Table \ref{dailyresults}, shows that \textbf{RBFMMP+OR} is better than \textbf{RBF-ANN} at different prediction horizons. \textbf{QMMP} and \textbf{DSHW} performs quite similar but not better than the propsed framework. The good accuracy at shorter prediction horizons, is because special architectures are trained to predict at specific horizon avoiding the accumulation of forecasting errors, unlike forecasting models such as DSHW or ARIMA that are not able to produce a forecast horizon longer than 1 without feedback. 


%this accumulative error effect is presented in forecasting models not able to produce a forecast horizon longer than 1 without feedback such as \textbf{DSHW} and ARIMA models. %It is consistent and better for shorter prediction horizons compared with \textbf{DSHW} and \textbf{QMMP}. 
%But the most noticeable is that Multi-Model Predictor approach produces better results than the standard (single model) implementation of \textbf{RBF-ANN}. 

%This shows an evidence that the proposed approach for local prediction modelling improves the general performance.



\subsection{Discussion}

This approach basically proposes a methodology to train and implement a Multi-Model Predictor with classified temporal data using RBF-ANN for time series with defined periodicity. This approach by introducing the concept of regimes or operation forecasting modes requires a mode recognition mechanism to identify the current mode on-line  . One of the main drawbacks is the selection of a good value for the split point. A short split point value leads to have short prototypes and short overlaps between consecutive segments producing a big number of classes. A big split point leads to have long prototypes maybe difficult to recognise when the number of observations are short producing poor mode estimations and consequently poor predictions (Remember that the split point defines the minimum amount of observations stored in the queue for estimating the current mode). Although it is not the best model for minimising the error for all the time series studied in this thesis, it performs better than the traditional RBF-ANN. The author believes that the use of more sophisticated algorithms for clustering, feature extraction and prediction might improve the performance even more. As future work, other implementations should be studied extending the experimentation with more algorithms such as feature extraction and, dimensionality reduction algorithms. We can conclude the model is suitable for stationary time series presenting defined seasonal patterns along the time.


%\subsection{Conclusions}


 %The proposed approach offers a multi-model framework based in Machine learning methods such as RBF-ANN.  The approach can be useful for time series with certain characteristics since is observed that is able to predict time series that the others models not. 
 
 




\chapter{Conclusions}

The main contribution of this thesis is the introduction of general Multi-Model Predictor (\textbf{MMP}) architecture. Different implementations where proposed following the qualitative and quantitative decomposition of the data and a Multi-Model forecasting based on Machine learning methodologies with the implementation of an On-Line Mode Recognition. 



\section{General Conclusions}

Time series modelling is related to the black-box modelling of dynamical systems when the model that produces such dynamics is completely unknown. For addressing this problem, different tools from statistic and computer science are used nowadays to describe the data sequence using general models.

In statistics are found the parametric models like moving average, auto-regressive, exponential smoothing models that assumes certain regularity in the dynamic composition of the data, and in computer science nonparametric models that do not need prove any assumption about the data to used like Support Vector Machines, multilayer perceptron and Artificial Neural Networks which due to their characteristics are used as universal function approximators.

Time series extracted from the nature might present complex behavior difficult to be modelled just with one single linear model. In order to obtain a more accurate approximation, is assumed that the behavior can be approximated by interpolating several Linear Time Invariant (LTI) models, which each one is used to model scenarios or regimes locally of the dynamical system.  This fact is related to the multiple modelling approach of nonlinear dynamical systems.

Based on this idea, this thesis addressed the Multiple-Modelling problem by defining a general framework which may adopts a collection of statistical and machine learning algorithms for producing as accurate predictions as possible of the system by identifying and modelling the local regimes.

The work of this thesis has some advantages over the proposed methodologies mentioned in Chapter 2, the proposed multi-modelling approach does not depend completely of the structured periodicity of the data, overall when the global model (presented in the sequence of modes or regimes) is based on nonparametric forecasting methodologies which learns from the past information to predict the next mode. These algorithms are based on the knowledge acquired by learning from the historical behavior of the data from predefined activity modes governed by a calendar. In other words, these prediction algorithms are based on the \textit{common sense} that come from the predefined calendar of the human activity, to the analysis of the past information to learn and predict the behavior of the next time step as shown with the qualitative kNN predictor.


\section{Particular Conclusions}

The particular conclusions related to the algorithms developed in this thesis are:

\begin{itemize}
	\item \textbf{Multi-Model Predictor Based on the Qualitative and Quantitative Decomposition using SARIMA and kNN \textbf(MMP+kNN):}  This algorithm was able to present better performance in the water demand time series of a sector that does not follow the dynamic of the predefined activity calendar at all. The implementation outperforms the SARIMA pattern algorithm, RBF-ANN, and Holt-Winters for this case.
	
	\item \textbf{Multi-Model Predictor Based on the Qualitative and Quantitative Decomposition of the Time Series including a Nonlinear Filter (\textbf{MMP+NF})}: This Multi-Model implementation includes a nonlinear filter. The nonlinear filter is used with water demand time series which their qualitative behavior approximate the human calendar activity of working and resting days. The nonlinear filter was able to find new nonlinear structure of the qualitative behavior of the time series. The integration of this mechanism provides of interesting advantages over other algorithms in real situations when the dynamics of the system is changing gradually along the time, with traditional approaches, the new data should be analysed in order to decide if the current model is still valid or not.  As soon as is detected a dynamic change, it might be required to construct a new forecasting model. The proposed model is able to detect the mismatches to correct the next prediction pattern. This algorithm is more robust to the model mismatches in the sense that presents adaptability as soon as the new information provided by the pattern mismatches is collected and processed.
	
	\item \textbf{Multi-Model Forecasting Using Radial Basis Function Neural Networks with an On-Line Mode Recognition \textbf{(RBFANN+OR)}}: This implementation proposes at the same time a general framework for constructing a Multi-Model Predictor based completely in machine learning algorithms. The Disadvantage of this approach is that sometimes produces false positives regarding the forecasting mode.
\end{itemize}

The three methodologies based on the proposed general Multi-Model Predictor framework has some disadvantage that should be solved. The mode prediction may produce false positives. This is aggravated when the assumption of the certainty of the data is not correct at all. For example, the prediction fails when the strategy is to trust in the pattern calendar activity when in reality the prediction mode is governed by other kind of rules. In this situation the prediction will not be robust enough. In order to solve this problem, is possible to be selective by associating certain algorithms or prediction schemes to specific time series. This can be performed off-line executing a cross validation methodology as we did in Chapter 5.1. If the prediction scheme is required to be adaptive taking decisions in an on-line fashion, methodologies for exploiting in real time the information collected so far should be considered for the prediction of the next mode for the global modelling.


\section{Future Work}

As future work the author proposes to follow the research line of global modelling based on modes of time series that can be implemented in the Multi-Model Predictor architecture. The prediction accuracy of the next mode depends on the knowledge assumption of the data (It might be a stochastic Markov process, deterministic rule based prediction, as previously mentioned in Chapter \ref{TSFSI}). A correct general assumption of the sequential data provides robustness to the prediction. The author believes that the integration of different (possibly correlated) information sources as temperature, humidity and pollution levels as auxiliary information for the qualitative forecasters can be integrated and exploited for improving the global mode prediction accuracy. Bayes based tools such as Bayesian Networks allow the integration of all these diverse information because they are able to capture the probabilistic dependency of random variables given a set of observations \cite{han2011}. These kind of models are capable to predict the operation modes using the Bayesian inference; in other words, it is possible to estimate the posterior probability of unknown variables given a set of observations from diverse sources considered as random variables. These variables might be associated with the operation modes or regimes. 

Another research direction, is the generalisation of the qualitative and quantitative method decomposition (Chapter \ref{qqmm}) generating several qualitative and quantitative time series associated with specific subperiod of water demand pattern. 

The proposed decomposition is useful when certain segments (or subperiods) of a period, obeys to different behavior. For example, in the work of \cite{Bakker2013}, M. Bakker et al. detected in the central and Southern part of Netherlands, an increment of the normal water demand from late afternoon to evening in summer days. This increment observed in these specific hours of the day is called, variant of season water demand pattern. This pattern is produced by people sprinkling their gardens, therefore the proposed forecasting method works differently for this part of the day.

Therefore the proposal is to extend the decomposition algorithm splitting the size period $\tau$ in subperiods of size $\tau'$. The selected subperiod size $\tau'$ is selected satisfying Equation \ref{tausub}. 

\begin{equation}\label{tausub}
\{ \exists \tau' , mod(\tau,\tau')=0 \text{ and } \tau'\leq \tau\}
\end{equation}

where if $\tau$ is a multiple of $\tau'$ then $mod(\tau,\tau')=0$ and then, $\tau'/\tau$ number of qualitative and quantitative different time series are obtained. For example: if $\tau=24$ (Assuming the time series show seasonality each 24 hours) and $\tau'=\tau/2$ then is possible to obtain a collection of 2 qualitative and 2 quantitative time series associated with specific period segments $\{1,\dots,\tau'\}$ and $\{\tau'+1,\dots,\tau\}$. The generalisation of this kind of decomposition (shown in Equation \ref{quant} and \ref{qual}) is defined by the Equation \ref{qngeneral} and \ref{qlgeneral}:


\begin{equation}\label{qngeneral}
X_T^{a} = \sum_{i=(T-1)\tau +(a-1)\tau'+1}^{(T-1)\tau + T\tau'} Y_i
\end{equation}


\begin{equation}\label{qlgeneral}
Z_T^{a} = \frac{\{Y\}_{i=(T-1) \tau +(a-1)\tau'+1}^{(T-1)\tau + a\tau'} }{X_T^{a}}
\end{equation}  where $a=\{1,\dots,\frac{\tau}{\tau'}\}$ indicates the subsection part of size $\tau'$, and $d$ the $d-$th subsection $a$. Such transformation produces $\frac{2 \tau}{\tau'}$ simpler time series, of which a number of $\frac{\tau}{\tau'}$ are quantitative and same number are qualitative with subpatterns of size $\tau'$. The sequence of elements associated with each subsection $a$ for the qualitative time series are set in $\mathbf{X^a}$ expressed by Equation \ref{quantts}.  

\begin{equation}\label{quantts}
\mathbf{X^a} = \{X^a_1,\dots,X^a_T,\dots,X^a_{\frac{n}{\tau}}\}
\end{equation}

And the set of qualitative time series is set to $\mathbf{Z^a}$ expressed in Equation \ref{qualtts}

\begin{equation}\label{qualtts}
\mathbf{Z^a} = \{Z^a_1,\dots,Z^a_T,\dots,Z^a_{\frac{n}{\tau}}\}
\end{equation}

where $Z^a_1,X^a_1$ are the first element,  $Z^a_d,X^a_d$ the current segments at time $d$ and  $Z^a_{\frac{n}{\tau'}},X^a_{\frac{n}{\tau'}}$ the last element of the series. For each qualitative $\mathbf{X^a} $ and quantitative $\mathbf{Z^a} $ time series is built a local forecasting model for each sequence, $f^{ql}_a$ and $f^{qn}_a$ respectively as part of bigger models $\mathbf{F^{ql}(X_d)}$ and $\mathbf{F^{qn}(Z_d)}$ described in Equation \ref{newmultimodel} and \ref{newmultimodel2}. 



\begin{eqnarray}\label{newmultimodel}
  \mathbf{F^{ql}(X_T)} = \begin{cases}
        f_1^{ql}(X_{T}^1,\dots,X_{T-\alpha}^1) & \text{ }\text{ }  \text{ if $mod ( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'})= 1 $} 
        \\
         f_2^{ql}(X_{T}^2,\dots,X_{T-\alpha}^2) &\text{ }\text{ }   \text{ if $mod ( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'}) = 2 $} \\
		&\vdots \\
       f_a^{ql} (X_{T}^a,\dots,X_{T-\alpha}^a) &\text{ } \text{ }   \text{if $mod ( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'}) = a $}
        \end{cases}
\end{eqnarray} 





\begin{eqnarray}\label{newmultimodel2}
  \mathbf{F^{qn}(Z_d)} = \begin{cases}
        f_1^{qn}(Z_{d}^1,\dots,Z_{d-\alpha}^1) & \text{ }\text{ }  \text{ if $mod ( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'}) = 1 $} 
        \\
         f_2^{qn}(Z_{T}^2,\dots,Z_{T-\alpha}^2) &\text{ }\text{ }   \text{ if $mod( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'}) = 2 $} \\
		& \vdots \\
       f_a^{qn} (Z_{T}^a,\dots,z_{T-\alpha}^a) &\text{ } \text{ } \text{ if $mod( \lfloor \frac{\tau'(t+1)} {\tau}  \rfloor , \frac{\tau}{\tau'}) = a $}
        \end{cases}
\end{eqnarray}  where $\alpha$ limits the previous days taken into account for producing the prediction. These models belong to a Multi-Model Predictor which the composition of both produces a short term forecast of $\tau'$ steps ahead associated with each segment $a$. The prediction produced is described by Equation \ref{generalpred}. 


\begin{equation}\label{generalpred}
\mathbf{F^{qn}(Z_d)} * \mathbf{F^{ql}(X_T)} = Y_{(T-1)\tau + (a-1)\tau'+1},\dots, Y_{(T-1)\tau + (a)\tau'}
\end{equation} where $\mathbf{F^{qn}(Z_T)} * \mathbf{F^{ql}(X_T)}$ is the composition of the qualitative and quantitative predictions.


Figure \ref{generalDecomp} shows an example where $\tau=24$ and $\tau'=\tau/4$ producing $4$ subsegments of size $\tau'$ composing the full cycle. The past data is limited setting $\alpha=3$ representing the days in the past considered (or cycles). The segments $a=\{1,2,3,4\}$, works as model selector associated with each time series and forecasters introduced in equations \ref{tausub} - \ref{newmultimodel2} to the generalised multi-model decomposition. The pointy lines are the respective forecasts produced by generalised the Multi-Model Predictor (Equation \ref{generalpred}) based on the qualitative and quantitative decomposition of the series.







 \begin{figure}[h!]
				\centering
				\includegraphics[width=85mm]{figures/generalizedDecomposition.pdf}
				\caption{Segments of the generalised decomposition}
				\label{generalDecomp}
\end{figure}

With this last figure is finished this Chapter and thesis, concluding that the framework proposed has still a lot of experimentation to do which can be applied to other purposes as for example, the modelling of the electricity demand.





%The periodic time series can be decomposed at most in $\tau$ time series, where each time series is modeled with a forecasting model. The objective of performing this is to avoid the accumulation of the forecasting error in the short term prediction horizon $h$. In this way the error is not accumulated and better forecasting predictions would be produced in the short term. ahead  We keep the daily prediction approach but in this case instead of using the aggregated information of each period  $\tau$, we construct $\tau$ different time series $TS_h$ from the original. Each time series $TS_h$ collects data every $\tau$ steps from $\mathbf{Y}$. To construct the new set of time series $\{TS_1,\dots,TS_\tau \}$ where each time series is defined by the next Equation

%\begin{equation}
%TS_h = \{y_h,\dots,y_{h+\tau},\dots,y_{h+ 2 \tau},y_{N}\}
%\end{equation} where $h$ takes values from $1$ to $\tau$. For each time series $TS_h$ an associated forecaster $f_h$ is tuned or trained. The selection of the predictor is performed by the module $mod$ function. The decomposed time series is shown in Figure \ref{taudecomp}

%\begin{equation}
%  y_{(N+h)}=f_{h}(TS_h)
%\end{equation} where $h =  mod(N,\tau)+1$. The number of $\tau$ independent model forecasters can be used for predicting one value $h$ time steps ahead. 

%\begin{figure}[h!]
%\centering
%\includegraphics[scale=0.3]{figures/independentTimeSeries.pdf}
%\caption{Periodic Time series decomposition in 24 sub series. Each one represents the water consumption specifically of an hour in the day }
%\label{taudecomp}
%\end{figure}







%\chapter{Failure Detection of drinking water networks}

\appendix

%\include{appendix/A/appA}
\include{appendix/B/appB}
%\include{appendix/C/appC}

{\small
	\bibliographystyle{alpha}
%	\renewcommand{\bibname}{References} 
	\bibliography{backmatter/bibliographyA}
}

%ACRONYMS
%Multi-Model Predictor MMP
%Multi-Model Module 

\makecopyright

\end{document}
